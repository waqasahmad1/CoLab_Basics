{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finalCyclic_1to7&7to1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waqasahmad1/CoLab_Basics/blob/master/finalCyclic_1to7%267to1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssy1Rgd08GqG",
        "colab_type": "code",
        "outputId": "99db1bd0-8c13-4a56-83b8-022257cee5a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!sudo pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-tfrklhgn\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-tfrklhgn\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.16.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z0ttdruf/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyYwLOuS0K8t",
        "colab_type": "code",
        "outputId": "49127d1c-7e00-4b51-8ac2-b5262ff3190e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.datasets import mnist\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import glob\n",
        "import keras\n",
        "#from data_loader import DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class CycleGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        (x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "        is_y_train_1 = y_train == 1 \n",
        "        self.x = x_train[is_y_train_1]\n",
        "        \n",
        "        \n",
        "        \n",
        "        is_y_train_7 = y_train == 7 \n",
        "        self.y = x_train[is_y_train_7]\n",
        "        self.x = self.x[:self.y.shape[0]]\n",
        "        \n",
        "        self.x = self.x.reshape(self.x.shape[0],28,28,1)\n",
        "        \n",
        "        self.y = self.y.reshape(self.y.shape[0],28,28,1)\n",
        "        self.x = self.x/127.5-1\n",
        "        self.y = self.y/127.5-1\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #self.horses, self.zebras = self.fun()\n",
        "        \n",
        "        \n",
        "\n",
        "        # Configure data loader\n",
        "        #self.dataset_name = 'apple2orange'\n",
        "        #self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
        "                                       #img_res=(self.img_rows, self.img_cols))\n",
        "\n",
        "\n",
        "        # Calculate output shape of D (PatchGAN)\n",
        "        patch = int(self.img_rows / 2**2)\n",
        "        self.disc_patch = (patch, patch, 1)\n",
        "\n",
        "        # Number of filters in the first layer of G and D\n",
        "        self.gf = 32\n",
        "        self.df = 64\n",
        "\n",
        "        # Loss weights\n",
        "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
        "        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminators\n",
        "        self.d_A = self.build_discriminator()\n",
        "        self.d_B = self.build_discriminator()\n",
        "        self.d_A.compile(loss='mse',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "        self.d_B.compile(loss='mse',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        #-------------------------\n",
        "        # Construct Computational\n",
        "        #   Graph of Generators\n",
        "        #-------------------------\n",
        "\n",
        "        # Build the generators\n",
        "        self.g_AB = self.build_generator()\n",
        "        self.g_BA = self.build_generator()\n",
        "\n",
        "        # Input images from both domains\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape)\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB(img_A)\n",
        "        fake_A = self.g_BA(img_B)\n",
        "        # Translate images back to original domain\n",
        "        reconstr_A = self.g_BA(fake_B)\n",
        "        reconstr_B = self.g_AB(fake_A)\n",
        "        # Identity mapping of images\n",
        "        img_A_id = self.g_BA(img_A)\n",
        "        img_B_id = self.g_AB(img_B)\n",
        "\n",
        "        # For the combined model we will only train the generators\n",
        "        self.d_A.trainable = False\n",
        "        self.d_B.trainable = False\n",
        "\n",
        "        # Discriminators determines validity of translated images\n",
        "        valid_A = self.d_A(fake_A)\n",
        "        valid_B = self.d_B(fake_B)\n",
        "\n",
        "        # Combined model trains generators to fool discriminators\n",
        "        self.combined = Model(inputs=[img_A, img_B],\n",
        "                              outputs=[ valid_A, valid_B,\n",
        "                                        reconstr_A, reconstr_B,\n",
        "                                        img_A_id, img_B_id ])\n",
        "        self.combined.compile(loss=['mse', 'mse',\n",
        "                                    'mae', 'mae',\n",
        "                                    'mae', 'mae'],\n",
        "                            loss_weights=[  1, 1,\n",
        "                                            self.lambda_cycle, self.lambda_cycle,\n",
        "                                            self.lambda_id, self.lambda_id ],\n",
        "                            optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "        \"\"\"U-Net Generator\"\"\"\n",
        "\n",
        "        def conv2d(layer_input, filters, f_size=4):\n",
        "            \"\"\"Layers used during downsampling\"\"\"\n",
        "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "            d = LeakyReLU(alpha=0.2)(d)\n",
        "            d = InstanceNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "            \"\"\"Layers used during upsampling\"\"\"\n",
        "            u = UpSampling2D(size=2)(layer_input)\n",
        "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "            if dropout_rate:\n",
        "                u = Dropout(dropout_rate)(u)\n",
        "            u = InstanceNormalization()(u)\n",
        "            u = Concatenate()([u, skip_input])\n",
        "            return u\n",
        "\n",
        "        # Image input\n",
        "        d0 = Input(shape=self.img_shape) #28,28,1\n",
        "\n",
        "        # Downsampling\n",
        "        d1 = conv2d(d0, self.gf) \n",
        "        #print(d1.shape) # 14,14,32\n",
        "        d2 = conv2d(d1, self.gf*2)\n",
        "        #print(d2.shape) # 7,7,64\n",
        "        \n",
        "        \n",
        "\n",
        "        # Upsampling\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        u1 = deconv2d(d2, d1, self.gf) #14,14,64\n",
        "        #print(u1.shape)\n",
        "        \n",
        "\n",
        "        u2 = UpSampling2D(size=2)(u1) #28,28,64\n",
        "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u2)\n",
        "        #print(\"out\") #28,28,1\n",
        "        #print(output_img.shape)\n",
        "        a = Model(d0, output_img)\n",
        "        \n",
        "\n",
        "        return a\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
        "            \"\"\"Discriminator layer\"\"\"\n",
        "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "            d = LeakyReLU(alpha=0.2)(d)\n",
        "            if normalization:\n",
        "                d = InstanceNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "\n",
        "        d1 = d_layer(img, self.df, normalization=False) # 14,14,64\n",
        "        \n",
        "        d2 = d_layer(d1, self.df*2) # 7,7,128\n",
        "        \n",
        "        \n",
        "       \n",
        "\n",
        "        validity = Conv2D(1, kernel_size=2, strides=1, padding='same')(d2)\n",
        "        b = Model(img,validity)\n",
        "    \n",
        "\n",
        "        return b\n",
        "\n",
        "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "        batch_size = 128\n",
        "        # Adversarial loss ground truths\n",
        "        valid = np.ones((batch_size,) + self.disc_patch)\n",
        "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
        "        imgs_len = self.x.shape[0]\n",
        "        print(valid.shape)\n",
        "        \n",
        "        \n",
        "        count = 0\n",
        "        for epoch in range(epochs):\n",
        "            #for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
        "                        #for imgs_A, imgs_B in enumerate(self.fun()):  \n",
        "            for i in range(imgs_len//batch_size):\n",
        "                imgs_A = self.x[i*batch_size:(i+1)*batch_size]\n",
        "                imgs_B = self.y[i*batch_size:(i+1)*batch_size]\n",
        "                \n",
        "                #print(imgs_A.shape)\n",
        "                #print(imgs_B.shape)\n",
        "                # ----------------------\n",
        "                #  Train Discriminators\n",
        "                # ----------------------\n",
        "\n",
        "                # Translate images to opposite domain\n",
        "                fake_B = self.g_AB.predict(imgs_A)\n",
        "                fake_A = self.g_BA.predict(imgs_B)\n",
        "\n",
        "                # Train the discriminators (original images = real / translated = Fake)\n",
        "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
        "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
        "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "\n",
        "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
        "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
        "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "\n",
        "                # Total disciminator loss\n",
        "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "\n",
        "\n",
        "                # ------------------\n",
        "                #  Train Generators\n",
        "                # ------------------\n",
        "\n",
        "                # Train the generators\n",
        "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
        "                                                        [valid, valid,\n",
        "                                                        imgs_A, imgs_B,\n",
        "                                                        imgs_A, imgs_B])\n",
        "\n",
        "                elapsed_time = datetime.datetime.now() - start_time\n",
        "\n",
        "                # Plot the progress\n",
        "                print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
        "                                                                        % ( epoch, epochs,\n",
        "                                                                            i, imgs_len//batch_size,\n",
        "                                                                            d_loss[0], 100*d_loss[1],\n",
        "                                                                            g_loss[0],\n",
        "                                                                            np.mean(g_loss[1:3]),\n",
        "                                                                            np.mean(g_loss[3:5]),\n",
        "                                                                            np.mean(g_loss[5:6]),\n",
        "                                                                            elapsed_time))\n",
        "              \n",
        "            count = count + 1\n",
        "            if(count == 10):\n",
        "              self.sample_images(epoch,i)\n",
        "              count = 0\n",
        "                            \n",
        "            # If at save interval => save generated image samples\n",
        "        \n",
        "        \n",
        "\n",
        "    def sample_images(self, epoch, batch_i):\n",
        "        #os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
        "        r, c = 2, 3\n",
        "\n",
        "        #imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
        "        #imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
        "        imgs_A = self.x[-1:]\n",
        "        imgs_B = self.y[-1:]\n",
        "        # Demo (for GIF)\n",
        "        #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
        "        #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB.predict(imgs_A)\n",
        "        fake_A = self.g_BA.predict(imgs_B)\n",
        "        # Translate back to original domain\n",
        "        reconstr_A = self.g_BA.predict(fake_B)\n",
        "        reconstr_B = self.g_AB.predict(fake_A)\n",
        "\n",
        "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Original', 'Translated', 'Reconstructed']\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt].reshape(28,28))\n",
        "                axs[i, j].set_title(titles[j])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        #fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        \n",
        "        \n",
        "    def fun(self):\n",
        "  \n",
        "      horses_train_set = [] #horses training array\n",
        "      zebras_train_set = [] #zebras training array\n",
        "\n",
        "\n",
        "      \n",
        "      imagesNameA = []\n",
        "      imagesNameB = []\n",
        "      for trainA in glob.glob(\"drive/My Drive/horse2zebra/trainA/*.jpg\"):\n",
        "        imagesNameA.append((trainA.split(\"/\")[4].split(\".\")[0]))\n",
        "        imagesNameA.sort()\n",
        "      #plt.imshow(imagesName[0]+\".jpg\")\n",
        "      for name in imagesNameA:\n",
        "        t_A = \"drive/My Drive/horse2zebra/trainA\"+\"/\"+ str(name)+ \".jpg\"\n",
        "        img_train_A = np.asarray(keras.preprocessing.image.load_img(t_A))\n",
        "        #resized = cv2.resize(img_train_A,dim, interpolation = cv2.INTER_CUBIC)\n",
        "        horses_train_set.append(img_train_A)\n",
        "                                  \n",
        "      for trainB in glob.glob(\"drive/My Drive/horse2zebra/trainB/*.jpg\"):\n",
        "        imagesNameB.append((trainB.split(\"/\")[4].split(\".\")[0]))\n",
        "        imagesNameB.sort()\n",
        "      #plt.imshow(imagesName[0]+\".jpg\")\n",
        "      for name in imagesNameB:\n",
        "        t_B = \"drive/My Drive/horse2zebra/trainB\"+\"/\"+ str(name)+ \".jpg\"\n",
        "        img_train_B = np.asarray(keras.preprocessing.image.load_img(t_B))\n",
        "        #resized = cv2.resize(img_train_A,dim, interpolation = cv2.INTER_CUBIC)\n",
        "        zebras_train_set.append(img_train_B)                          \n",
        "                                 \n",
        "                                 \n",
        "      horses = np.array(horses_train_set)/127.5-1\n",
        "      zebras = np.array(zebras_train_set)/127.5-1\n",
        "      \n",
        "                                 \n",
        "                               \n",
        "      \n",
        "      return (horses, zebras)\n",
        "            \n",
        "\n",
        "    def save_Model(self):\n",
        "          self.g_AB.save('DomainA2B.model')\n",
        "          self.g_BA.save('DomainB2A.model')\n",
        "          self.combined.save('CycleGANCombined.model')\n",
        "\n",
        "\n",
        "      \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "gan = CycleGAN()\n",
        "#gan.save_Model()\n",
        "gan.train(epochs=200, batch_size=1, sample_interval=200)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 7, 7, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0/200] [Batch 0/48] [D loss: 1.748793, acc:  31%] [G loss: 22.687321, adv: 1.505175, recon: 0.903715, id: 0.877570] time: 0:00:11.142451 \n",
            "[Epoch 0/200] [Batch 1/48] [D loss: 1.094748, acc:  33%] [G loss: 15.249275, adv: 2.158004, recon: 0.504468, id: 0.450686] time: 0:00:11.279665 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0/200] [Batch 2/48] [D loss: 1.455487, acc:  25%] [G loss: 8.318899, adv: 1.237105, recon: 0.269771, id: 0.216944] time: 0:00:11.402938 \n",
            "[Epoch 0/200] [Batch 3/48] [D loss: 0.731769, acc:  28%] [G loss: 6.868380, adv: 0.916764, recon: 0.232839, id: 0.164163] time: 0:00:11.512933 \n",
            "[Epoch 0/200] [Batch 4/48] [D loss: 0.549519, acc:  32%] [G loss: 6.829153, adv: 0.943779, recon: 0.228836, id: 0.154057] time: 0:00:11.627030 \n",
            "[Epoch 0/200] [Batch 5/48] [D loss: 0.444524, acc:  37%] [G loss: 6.545757, adv: 0.946353, recon: 0.216091, id: 0.139149] time: 0:00:11.741531 \n",
            "[Epoch 0/200] [Batch 6/48] [D loss: 0.467325, acc:  31%] [G loss: 6.046931, adv: 0.921320, recon: 0.194951, id: 0.130698] time: 0:00:11.853636 \n",
            "[Epoch 0/200] [Batch 7/48] [D loss: 0.408721, acc:  38%] [G loss: 5.996154, adv: 0.955479, recon: 0.190010, id: 0.115982] time: 0:00:11.967265 \n",
            "[Epoch 0/200] [Batch 8/48] [D loss: 0.465336, acc:  34%] [G loss: 5.180283, adv: 0.835142, recon: 0.162765, id: 0.104177] time: 0:00:12.078962 \n",
            "[Epoch 0/200] [Batch 9/48] [D loss: 0.465338, acc:  33%] [G loss: 4.668204, adv: 0.844021, recon: 0.137116, id: 0.100366] time: 0:00:12.189187 \n",
            "[Epoch 0/200] [Batch 10/48] [D loss: 0.501480, acc:  29%] [G loss: 4.476295, adv: 0.741778, recon: 0.137923, id: 0.098977] time: 0:00:12.303697 \n",
            "[Epoch 0/200] [Batch 11/48] [D loss: 0.459664, acc:  33%] [G loss: 4.216150, adv: 0.746228, recon: 0.125745, id: 0.080705] time: 0:00:12.414742 \n",
            "[Epoch 0/200] [Batch 12/48] [D loss: 0.495615, acc:  26%] [G loss: 4.069810, adv: 0.731754, recon: 0.120066, id: 0.080811] time: 0:00:12.523935 \n",
            "[Epoch 0/200] [Batch 13/48] [D loss: 0.435945, acc:  31%] [G loss: 3.911388, adv: 0.727075, recon: 0.113252, id: 0.073970] time: 0:00:12.639929 \n",
            "[Epoch 0/200] [Batch 14/48] [D loss: 0.425322, acc:  31%] [G loss: 3.800394, adv: 0.710339, recon: 0.109738, id: 0.068560] time: 0:00:12.754617 \n",
            "[Epoch 0/200] [Batch 15/48] [D loss: 0.420435, acc:  31%] [G loss: 3.689784, adv: 0.701336, recon: 0.105211, id: 0.069777] time: 0:00:12.863123 \n",
            "[Epoch 0/200] [Batch 16/48] [D loss: 0.402794, acc:  32%] [G loss: 3.628334, adv: 0.696156, recon: 0.103028, id: 0.065353] time: 0:00:12.973786 \n",
            "[Epoch 0/200] [Batch 17/48] [D loss: 0.400335, acc:  32%] [G loss: 3.529623, adv: 0.690014, recon: 0.098688, id: 0.070919] time: 0:00:13.086697 \n",
            "[Epoch 0/200] [Batch 18/48] [D loss: 0.369464, acc:  36%] [G loss: 3.471199, adv: 0.706360, recon: 0.094486, id: 0.064027] time: 0:00:13.197463 \n",
            "[Epoch 0/200] [Batch 19/48] [D loss: 0.389639, acc:  33%] [G loss: 3.344162, adv: 0.677412, recon: 0.090983, id: 0.066301] time: 0:00:13.313838 \n",
            "[Epoch 0/200] [Batch 20/48] [D loss: 0.375939, acc:  33%] [G loss: 3.238247, adv: 0.685790, recon: 0.085176, id: 0.064323] time: 0:00:13.424631 \n",
            "[Epoch 0/200] [Batch 21/48] [D loss: 0.400578, acc:  30%] [G loss: 3.161211, adv: 0.661520, recon: 0.083835, id: 0.061602] time: 0:00:13.538149 \n",
            "[Epoch 0/200] [Batch 22/48] [D loss: 0.368147, acc:  34%] [G loss: 3.086286, adv: 0.663050, recon: 0.080015, id: 0.059875] time: 0:00:13.657625 \n",
            "[Epoch 0/200] [Batch 23/48] [D loss: 0.364960, acc:  34%] [G loss: 3.023093, adv: 0.664492, recon: 0.076953, id: 0.060821] time: 0:00:13.770121 \n",
            "[Epoch 0/200] [Batch 24/48] [D loss: 0.356500, acc:  34%] [G loss: 3.010205, adv: 0.667863, recon: 0.076115, id: 0.060898] time: 0:00:13.880210 \n",
            "[Epoch 0/200] [Batch 25/48] [D loss: 0.376228, acc:  31%] [G loss: 2.933021, adv: 0.645924, recon: 0.074449, id: 0.058728] time: 0:00:13.990966 \n",
            "[Epoch 0/200] [Batch 26/48] [D loss: 0.358703, acc:  34%] [G loss: 2.872139, adv: 0.652145, recon: 0.071104, id: 0.057297] time: 0:00:14.104492 \n",
            "[Epoch 0/200] [Batch 27/48] [D loss: 0.351969, acc:  35%] [G loss: 2.847951, adv: 0.645462, recon: 0.070528, id: 0.059027] time: 0:00:14.215379 \n",
            "[Epoch 0/200] [Batch 28/48] [D loss: 0.341529, acc:  36%] [G loss: 2.762751, adv: 0.644712, recon: 0.066638, id: 0.055269] time: 0:00:14.325713 \n",
            "[Epoch 0/200] [Batch 29/48] [D loss: 0.341302, acc:  36%] [G loss: 2.795888, adv: 0.637496, recon: 0.068846, id: 0.054571] time: 0:00:14.441596 \n",
            "[Epoch 0/200] [Batch 30/48] [D loss: 0.330576, acc:  37%] [G loss: 2.729137, adv: 0.644903, recon: 0.064878, id: 0.056181] time: 0:00:14.552132 \n",
            "[Epoch 0/200] [Batch 31/48] [D loss: 0.331268, acc:  38%] [G loss: 2.693091, adv: 0.644232, recon: 0.063382, id: 0.052032] time: 0:00:14.667223 \n",
            "[Epoch 0/200] [Batch 32/48] [D loss: 0.321076, acc:  40%] [G loss: 2.707538, adv: 0.637435, recon: 0.064704, id: 0.055608] time: 0:00:14.777000 \n",
            "[Epoch 0/200] [Batch 33/48] [D loss: 0.349200, acc:  35%] [G loss: 2.665951, adv: 0.612863, recon: 0.065093, id: 0.052877] time: 0:00:14.889002 \n",
            "[Epoch 0/200] [Batch 34/48] [D loss: 0.321248, acc:  38%] [G loss: 2.605667, adv: 0.628059, recon: 0.060812, id: 0.049803] time: 0:00:15.001473 \n",
            "[Epoch 0/200] [Batch 35/48] [D loss: 0.333855, acc:  37%] [G loss: 2.526322, adv: 0.608445, recon: 0.059106, id: 0.049148] time: 0:00:15.114776 \n",
            "[Epoch 0/200] [Batch 36/48] [D loss: 0.343067, acc:  36%] [G loss: 2.576578, adv: 0.608415, recon: 0.061322, id: 0.051019] time: 0:00:15.226540 \n",
            "[Epoch 0/200] [Batch 37/48] [D loss: 0.328875, acc:  37%] [G loss: 2.561312, adv: 0.609146, recon: 0.060651, id: 0.050716] time: 0:00:15.341402 \n",
            "[Epoch 0/200] [Batch 38/48] [D loss: 0.330548, acc:  36%] [G loss: 2.441597, adv: 0.601374, recon: 0.055766, id: 0.046673] time: 0:00:15.455450 \n",
            "[Epoch 0/200] [Batch 39/48] [D loss: 0.319829, acc:  41%] [G loss: 2.491219, adv: 0.610774, recon: 0.057260, id: 0.047365] time: 0:00:15.568748 \n",
            "[Epoch 0/200] [Batch 40/48] [D loss: 0.330870, acc:  38%] [G loss: 2.460215, adv: 0.594542, recon: 0.057188, id: 0.049676] time: 0:00:15.686808 \n",
            "[Epoch 0/200] [Batch 41/48] [D loss: 0.303151, acc:  40%] [G loss: 2.423941, adv: 0.613701, recon: 0.053718, id: 0.046824] time: 0:00:15.796937 \n",
            "[Epoch 0/200] [Batch 42/48] [D loss: 0.326360, acc:  37%] [G loss: 2.342332, adv: 0.577371, recon: 0.053388, id: 0.048016] time: 0:00:15.909222 \n",
            "[Epoch 0/200] [Batch 43/48] [D loss: 0.307026, acc:  41%] [G loss: 2.464807, adv: 0.609346, recon: 0.056105, id: 0.046690] time: 0:00:16.020182 \n",
            "[Epoch 0/200] [Batch 44/48] [D loss: 0.317625, acc:  38%] [G loss: 2.340294, adv: 0.582530, recon: 0.052697, id: 0.047891] time: 0:00:16.134389 \n",
            "[Epoch 0/200] [Batch 45/48] [D loss: 0.314498, acc:  39%] [G loss: 2.382768, adv: 0.590536, recon: 0.054078, id: 0.046701] time: 0:00:16.245515 \n",
            "[Epoch 0/200] [Batch 46/48] [D loss: 0.322437, acc:  38%] [G loss: 2.344670, adv: 0.578103, recon: 0.053445, id: 0.045235] time: 0:00:16.356849 \n",
            "[Epoch 0/200] [Batch 47/48] [D loss: 0.322102, acc:  37%] [G loss: 2.302315, adv: 0.571200, recon: 0.052368, id: 0.042271] time: 0:00:16.468249 \n",
            "[Epoch 1/200] [Batch 0/48] [D loss: 0.314945, acc:  39%] [G loss: 2.342994, adv: 0.583062, recon: 0.052954, id: 0.044826] time: 0:00:16.584009 \n",
            "[Epoch 1/200] [Batch 1/48] [D loss: 0.309471, acc:  41%] [G loss: 2.274502, adv: 0.572117, recon: 0.050668, id: 0.044293] time: 0:00:16.699670 \n",
            "[Epoch 1/200] [Batch 2/48] [D loss: 0.310427, acc:  40%] [G loss: 2.274985, adv: 0.570251, recon: 0.050960, id: 0.042159] time: 0:00:16.810861 \n",
            "[Epoch 1/200] [Batch 3/48] [D loss: 0.308787, acc:  42%] [G loss: 2.297508, adv: 0.579524, recon: 0.051244, id: 0.041003] time: 0:00:16.922823 \n",
            "[Epoch 1/200] [Batch 4/48] [D loss: 0.294588, acc:  44%] [G loss: 2.309573, adv: 0.579209, recon: 0.051602, id: 0.043910] time: 0:00:17.036710 \n",
            "[Epoch 1/200] [Batch 5/48] [D loss: 0.307819, acc:  41%] [G loss: 2.241243, adv: 0.565248, recon: 0.049811, id: 0.041007] time: 0:00:17.148686 \n",
            "[Epoch 1/200] [Batch 6/48] [D loss: 0.332063, acc:  36%] [G loss: 2.257584, adv: 0.534255, recon: 0.053448, id: 0.045116] time: 0:00:17.259081 \n",
            "[Epoch 1/200] [Batch 7/48] [D loss: 0.314145, acc:  38%] [G loss: 2.236347, adv: 0.551489, recon: 0.050869, id: 0.042408] time: 0:00:17.369010 \n",
            "[Epoch 1/200] [Batch 8/48] [D loss: 0.294743, acc:  41%] [G loss: 2.184244, adv: 0.553609, recon: 0.048073, id: 0.041844] time: 0:00:17.478585 \n",
            "[Epoch 1/200] [Batch 9/48] [D loss: 0.308199, acc:  40%] [G loss: 2.178935, adv: 0.542131, recon: 0.049156, id: 0.039972] time: 0:00:17.590390 \n",
            "[Epoch 1/200] [Batch 10/48] [D loss: 0.297552, acc:  42%] [G loss: 2.228993, adv: 0.562067, recon: 0.049453, id: 0.041141] time: 0:00:17.702034 \n",
            "[Epoch 1/200] [Batch 11/48] [D loss: 0.303948, acc:  42%] [G loss: 2.208141, adv: 0.546240, recon: 0.050016, id: 0.040762] time: 0:00:17.816267 \n",
            "[Epoch 1/200] [Batch 12/48] [D loss: 0.327497, acc:  36%] [G loss: 2.191679, adv: 0.521959, recon: 0.051268, id: 0.044532] time: 0:00:17.928454 \n",
            "[Epoch 1/200] [Batch 13/48] [D loss: 0.308160, acc:  38%] [G loss: 2.221982, adv: 0.538353, recon: 0.051422, id: 0.043537] time: 0:00:18.039964 \n",
            "[Epoch 1/200] [Batch 14/48] [D loss: 0.287072, acc:  42%] [G loss: 2.159104, adv: 0.547515, recon: 0.047518, id: 0.039416] time: 0:00:18.150664 \n",
            "[Epoch 1/200] [Batch 15/48] [D loss: 0.311351, acc:  40%] [G loss: 2.136391, adv: 0.524706, recon: 0.048451, id: 0.042912] time: 0:00:18.262942 \n",
            "[Epoch 1/200] [Batch 16/48] [D loss: 0.277242, acc:  46%] [G loss: 2.110575, adv: 0.551326, recon: 0.044667, id: 0.038767] time: 0:00:18.376209 \n",
            "[Epoch 1/200] [Batch 17/48] [D loss: 0.294376, acc:  42%] [G loss: 2.118073, adv: 0.532195, recon: 0.046968, id: 0.043385] time: 0:00:18.486439 \n",
            "[Epoch 1/200] [Batch 18/48] [D loss: 0.281852, acc:  45%] [G loss: 2.167457, adv: 0.556642, recon: 0.046951, id: 0.039931] time: 0:00:18.597342 \n",
            "[Epoch 1/200] [Batch 19/48] [D loss: 0.285744, acc:  44%] [G loss: 2.136463, adv: 0.533220, recon: 0.047713, id: 0.042808] time: 0:00:18.710718 \n",
            "[Epoch 1/200] [Batch 20/48] [D loss: 0.299638, acc:  41%] [G loss: 2.060240, adv: 0.524902, recon: 0.044670, id: 0.042455] time: 0:00:18.823815 \n",
            "[Epoch 1/200] [Batch 21/48] [D loss: 0.322252, acc:  35%] [G loss: 2.101774, adv: 0.506941, recon: 0.048420, id: 0.043069] time: 0:00:18.936161 \n",
            "[Epoch 1/200] [Batch 22/48] [D loss: 0.283717, acc:  43%] [G loss: 2.083586, adv: 0.535392, recon: 0.044836, id: 0.039235] time: 0:00:19.052635 \n",
            "[Epoch 1/200] [Batch 23/48] [D loss: 0.289242, acc:  42%] [G loss: 2.100964, adv: 0.526373, recon: 0.046511, id: 0.041163] time: 0:00:19.163162 \n",
            "[Epoch 1/200] [Batch 24/48] [D loss: 0.279080, acc:  43%] [G loss: 2.055963, adv: 0.527536, recon: 0.044248, id: 0.040131] time: 0:00:19.273001 \n",
            "[Epoch 1/200] [Batch 25/48] [D loss: 0.305558, acc:  38%] [G loss: 2.062362, adv: 0.510130, recon: 0.046244, id: 0.040645] time: 0:00:19.381985 \n",
            "[Epoch 1/200] [Batch 26/48] [D loss: 0.300625, acc:  40%] [G loss: 2.056899, adv: 0.521332, recon: 0.044862, id: 0.040848] time: 0:00:19.498491 \n",
            "[Epoch 1/200] [Batch 27/48] [D loss: 0.289933, acc:  43%] [G loss: 2.080768, adv: 0.518546, recon: 0.046478, id: 0.041807] time: 0:00:19.611264 \n",
            "[Epoch 1/200] [Batch 28/48] [D loss: 0.282034, acc:  44%] [G loss: 2.118653, adv: 0.524036, recon: 0.047647, id: 0.038885] time: 0:00:19.724561 \n",
            "[Epoch 1/200] [Batch 29/48] [D loss: 0.285104, acc:  44%] [G loss: 2.230209, adv: 0.518329, recon: 0.053985, id: 0.041538] time: 0:00:19.838401 \n",
            "[Epoch 1/200] [Batch 30/48] [D loss: 0.275030, acc:  46%] [G loss: 2.213960, adv: 0.530627, recon: 0.051663, id: 0.040186] time: 0:00:19.952409 \n",
            "[Epoch 1/200] [Batch 31/48] [D loss: 0.287458, acc:  43%] [G loss: 2.068918, adv: 0.512889, recon: 0.046647, id: 0.039339] time: 0:00:20.062726 \n",
            "[Epoch 1/200] [Batch 32/48] [D loss: 0.263030, acc:  48%] [G loss: 2.066502, adv: 0.538657, recon: 0.043802, id: 0.040036] time: 0:00:20.177746 \n",
            "[Epoch 1/200] [Batch 33/48] [D loss: 0.293288, acc:  42%] [G loss: 2.045882, adv: 0.506341, recon: 0.045744, id: 0.038961] time: 0:00:20.288611 \n",
            "[Epoch 1/200] [Batch 34/48] [D loss: 0.271112, acc:  46%] [G loss: 2.017100, adv: 0.526702, recon: 0.042595, id: 0.037764] time: 0:00:20.401965 \n",
            "[Epoch 1/200] [Batch 35/48] [D loss: 0.280752, acc:  43%] [G loss: 1.986398, adv: 0.513514, recon: 0.042368, id: 0.037459] time: 0:00:20.511702 \n",
            "[Epoch 1/200] [Batch 36/48] [D loss: 0.298662, acc:  42%] [G loss: 2.013597, adv: 0.502099, recon: 0.044785, id: 0.039454] time: 0:00:20.623069 \n",
            "[Epoch 1/200] [Batch 37/48] [D loss: 0.280083, acc:  44%] [G loss: 2.017572, adv: 0.514503, recon: 0.043793, id: 0.038660] time: 0:00:20.735441 \n",
            "[Epoch 1/200] [Batch 38/48] [D loss: 0.284061, acc:  42%] [G loss: 1.954274, adv: 0.504443, recon: 0.041862, id: 0.036608] time: 0:00:20.851387 \n",
            "[Epoch 1/200] [Batch 39/48] [D loss: 0.279266, acc:  45%] [G loss: 1.997510, adv: 0.515811, recon: 0.042608, id: 0.037332] time: 0:00:20.962811 \n",
            "[Epoch 1/200] [Batch 40/48] [D loss: 0.288685, acc:  43%] [G loss: 1.978470, adv: 0.503835, recon: 0.042926, id: 0.038929] time: 0:00:21.074241 \n",
            "[Epoch 1/200] [Batch 41/48] [D loss: 0.266664, acc:  46%] [G loss: 1.946197, adv: 0.517757, recon: 0.040047, id: 0.035349] time: 0:00:21.187224 \n",
            "[Epoch 1/200] [Batch 42/48] [D loss: 0.279704, acc:  43%] [G loss: 1.919946, adv: 0.500470, recon: 0.040510, id: 0.037571] time: 0:00:21.302109 \n",
            "[Epoch 1/200] [Batch 43/48] [D loss: 0.271292, acc:  45%] [G loss: 1.993551, adv: 0.515868, recon: 0.042294, id: 0.038279] time: 0:00:21.412947 \n",
            "[Epoch 1/200] [Batch 44/48] [D loss: 0.274052, acc:  46%] [G loss: 1.947070, adv: 0.510632, recon: 0.040782, id: 0.038049] time: 0:00:21.526214 \n",
            "[Epoch 1/200] [Batch 45/48] [D loss: 0.277939, acc:  44%] [G loss: 1.963087, adv: 0.501412, recon: 0.042468, id: 0.037695] time: 0:00:21.640104 \n",
            "[Epoch 1/200] [Batch 46/48] [D loss: 0.278097, acc:  44%] [G loss: 1.977597, adv: 0.506957, recon: 0.042468, id: 0.036925] time: 0:00:21.757606 \n",
            "[Epoch 1/200] [Batch 47/48] [D loss: 0.280809, acc:  42%] [G loss: 1.975028, adv: 0.494889, recon: 0.043923, id: 0.036398] time: 0:00:21.870896 \n",
            "[Epoch 2/200] [Batch 0/48] [D loss: 0.279564, acc:  44%] [G loss: 2.073340, adv: 0.507564, recon: 0.046979, id: 0.038353] time: 0:00:21.984216 \n",
            "[Epoch 2/200] [Batch 1/48] [D loss: 0.270655, acc:  45%] [G loss: 1.991693, adv: 0.502030, recon: 0.044021, id: 0.038801] time: 0:00:22.096653 \n",
            "[Epoch 2/200] [Batch 2/48] [D loss: 0.272778, acc:  44%] [G loss: 1.953105, adv: 0.500236, recon: 0.042072, id: 0.035747] time: 0:00:22.208748 \n",
            "[Epoch 2/200] [Batch 3/48] [D loss: 0.273922, acc:  45%] [G loss: 1.950552, adv: 0.508567, recon: 0.041388, id: 0.035130] time: 0:00:22.323526 \n",
            "[Epoch 2/200] [Batch 4/48] [D loss: 0.264598, acc:  48%] [G loss: 1.968664, adv: 0.511682, recon: 0.041589, id: 0.036953] time: 0:00:22.434501 \n",
            "[Epoch 2/200] [Batch 5/48] [D loss: 0.274030, acc:  45%] [G loss: 1.923656, adv: 0.499869, recon: 0.040797, id: 0.034355] time: 0:00:22.545932 \n",
            "[Epoch 2/200] [Batch 6/48] [D loss: 0.295218, acc:  39%] [G loss: 1.941221, adv: 0.472941, recon: 0.044016, id: 0.039787] time: 0:00:22.657120 \n",
            "[Epoch 2/200] [Batch 7/48] [D loss: 0.280446, acc:  42%] [G loss: 1.924812, adv: 0.492748, recon: 0.041370, id: 0.036341] time: 0:00:22.766692 \n",
            "[Epoch 2/200] [Batch 8/48] [D loss: 0.265918, acc:  46%] [G loss: 1.904918, adv: 0.495722, recon: 0.040255, id: 0.036114] time: 0:00:22.886582 \n",
            "[Epoch 2/200] [Batch 9/48] [D loss: 0.286970, acc:  40%] [G loss: 1.881050, adv: 0.474000, recon: 0.041176, id: 0.034224] time: 0:00:23.001334 \n",
            "[Epoch 2/200] [Batch 10/48] [D loss: 0.275258, acc:  44%] [G loss: 1.941075, adv: 0.498233, recon: 0.041714, id: 0.035738] time: 0:00:23.112556 \n",
            "[Epoch 2/200] [Batch 11/48] [D loss: 0.281199, acc:  44%] [G loss: 1.924329, adv: 0.485306, recon: 0.042127, id: 0.035408] time: 0:00:23.226183 \n",
            "[Epoch 2/200] [Batch 12/48] [D loss: 0.306497, acc:  36%] [G loss: 1.920918, adv: 0.456733, recon: 0.044468, id: 0.039955] time: 0:00:23.339092 \n",
            "[Epoch 2/200] [Batch 13/48] [D loss: 0.280069, acc:  43%] [G loss: 1.966617, adv: 0.482796, recon: 0.044647, id: 0.038284] time: 0:00:23.453900 \n",
            "[Epoch 2/200] [Batch 14/48] [D loss: 0.262828, acc:  45%] [G loss: 2.016555, adv: 0.497113, recon: 0.045621, id: 0.034374] time: 0:00:23.563958 \n",
            "[Epoch 2/200] [Batch 15/48] [D loss: 0.282560, acc:  43%] [G loss: 1.974496, adv: 0.477439, recon: 0.045485, id: 0.039959] time: 0:00:23.675572 \n",
            "[Epoch 2/200] [Batch 16/48] [D loss: 0.265423, acc:  46%] [G loss: 1.894681, adv: 0.486895, recon: 0.040634, id: 0.032906] time: 0:00:23.790339 \n",
            "[Epoch 2/200] [Batch 17/48] [D loss: 0.264452, acc:  47%] [G loss: 1.882843, adv: 0.495487, recon: 0.039370, id: 0.037918] time: 0:00:23.912634 \n",
            "[Epoch 2/200] [Batch 18/48] [D loss: 0.266437, acc:  46%] [G loss: 1.890481, adv: 0.489411, recon: 0.040247, id: 0.034638] time: 0:00:24.024296 \n",
            "[Epoch 2/200] [Batch 19/48] [D loss: 0.266873, acc:  47%] [G loss: 1.899629, adv: 0.486365, recon: 0.040802, id: 0.037071] time: 0:00:24.135037 \n",
            "[Epoch 2/200] [Batch 20/48] [D loss: 0.285533, acc:  41%] [G loss: 1.840891, adv: 0.462328, recon: 0.040375, id: 0.039644] time: 0:00:24.246581 \n",
            "[Epoch 2/200] [Batch 21/48] [D loss: 0.307197, acc:  36%] [G loss: 1.918388, adv: 0.450843, recon: 0.045038, id: 0.037951] time: 0:00:24.358707 \n",
            "[Epoch 2/200] [Batch 22/48] [D loss: 0.261044, acc:  48%] [G loss: 1.915121, adv: 0.498349, recon: 0.040712, id: 0.035053] time: 0:00:24.470687 \n",
            "[Epoch 2/200] [Batch 23/48] [D loss: 0.268903, acc:  45%] [G loss: 1.898744, adv: 0.479426, recon: 0.041561, id: 0.034545] time: 0:00:24.585914 \n",
            "[Epoch 2/200] [Batch 24/48] [D loss: 0.260717, acc:  45%] [G loss: 1.832254, adv: 0.477355, recon: 0.038520, id: 0.034653] time: 0:00:24.700459 \n",
            "[Epoch 2/200] [Batch 25/48] [D loss: 0.285130, acc:  40%] [G loss: 1.858237, adv: 0.461758, recon: 0.041271, id: 0.035990] time: 0:00:24.813019 \n",
            "[Epoch 2/200] [Batch 26/48] [D loss: 0.286351, acc:  41%] [G loss: 1.848937, adv: 0.468506, recon: 0.040077, id: 0.036210] time: 0:00:24.928248 \n",
            "[Epoch 2/200] [Batch 27/48] [D loss: 0.274719, acc:  45%] [G loss: 1.835859, adv: 0.473541, recon: 0.039096, id: 0.037070] time: 0:00:25.042510 \n",
            "[Epoch 2/200] [Batch 28/48] [D loss: 0.272983, acc:  44%] [G loss: 1.855591, adv: 0.469833, recon: 0.040450, id: 0.034259] time: 0:00:25.152570 \n",
            "[Epoch 2/200] [Batch 29/48] [D loss: 0.266246, acc:  46%] [G loss: 1.880822, adv: 0.479096, recon: 0.040758, id: 0.036074] time: 0:00:25.266550 \n",
            "[Epoch 2/200] [Batch 30/48] [D loss: 0.274161, acc:  43%] [G loss: 1.937751, adv: 0.464224, recon: 0.044992, id: 0.033912] time: 0:00:25.376486 \n",
            "[Epoch 2/200] [Batch 31/48] [D loss: 0.265730, acc:  46%] [G loss: 2.039657, adv: 0.483822, recon: 0.048599, id: 0.036356] time: 0:00:25.492139 \n",
            "[Epoch 2/200] [Batch 32/48] [D loss: 0.242711, acc:  51%] [G loss: 1.932950, adv: 0.505342, recon: 0.041029, id: 0.035261] time: 0:00:25.604551 \n",
            "[Epoch 2/200] [Batch 33/48] [D loss: 0.262567, acc:  48%] [G loss: 1.825645, adv: 0.476716, recon: 0.038448, id: 0.033994] time: 0:00:25.718904 \n",
            "[Epoch 2/200] [Batch 34/48] [D loss: 0.247469, acc:  51%] [G loss: 1.837945, adv: 0.494376, recon: 0.037394, id: 0.033151] time: 0:00:25.832051 \n",
            "[Epoch 2/200] [Batch 35/48] [D loss: 0.264894, acc:  45%] [G loss: 1.814034, adv: 0.472440, recon: 0.038182, id: 0.033098] time: 0:00:25.949604 \n",
            "[Epoch 2/200] [Batch 36/48] [D loss: 0.284263, acc:  43%] [G loss: 1.845829, adv: 0.462756, recon: 0.040564, id: 0.035822] time: 0:00:26.061574 \n",
            "[Epoch 2/200] [Batch 37/48] [D loss: 0.271886, acc:  44%] [G loss: 1.859646, adv: 0.469874, recon: 0.040593, id: 0.035848] time: 0:00:26.171780 \n",
            "[Epoch 2/200] [Batch 38/48] [D loss: 0.278084, acc:  41%] [G loss: 1.789446, adv: 0.458337, recon: 0.038436, id: 0.032663] time: 0:00:26.285840 \n",
            "[Epoch 2/200] [Batch 39/48] [D loss: 0.271283, acc:  45%] [G loss: 1.830748, adv: 0.469981, recon: 0.039223, id: 0.034327] time: 0:00:26.398025 \n",
            "[Epoch 2/200] [Batch 40/48] [D loss: 0.279715, acc:  42%] [G loss: 1.800225, adv: 0.462345, recon: 0.038485, id: 0.035029] time: 0:00:26.511191 \n",
            "[Epoch 2/200] [Batch 41/48] [D loss: 0.261168, acc:  45%] [G loss: 1.777539, adv: 0.468061, recon: 0.036995, id: 0.032412] time: 0:00:26.624885 \n",
            "[Epoch 2/200] [Batch 42/48] [D loss: 0.270637, acc:  43%] [G loss: 1.768332, adv: 0.460809, recon: 0.037254, id: 0.033424] time: 0:00:26.737581 \n",
            "[Epoch 2/200] [Batch 43/48] [D loss: 0.264585, acc:  45%] [G loss: 1.830731, adv: 0.469319, recon: 0.039236, id: 0.034934] time: 0:00:26.849951 \n",
            "[Epoch 2/200] [Batch 44/48] [D loss: 0.264868, acc:  46%] [G loss: 1.792967, adv: 0.473143, recon: 0.037223, id: 0.033995] time: 0:00:26.969071 \n",
            "[Epoch 2/200] [Batch 45/48] [D loss: 0.268842, acc:  43%] [G loss: 1.804124, adv: 0.458653, recon: 0.039094, id: 0.033378] time: 0:00:27.080345 \n",
            "[Epoch 2/200] [Batch 46/48] [D loss: 0.269146, acc:  44%] [G loss: 1.799700, adv: 0.466717, recon: 0.038128, id: 0.034443] time: 0:00:27.193559 \n",
            "[Epoch 2/200] [Batch 47/48] [D loss: 0.271497, acc:  42%] [G loss: 1.781190, adv: 0.457303, recon: 0.038289, id: 0.030250] time: 0:00:27.307542 \n",
            "[Epoch 3/200] [Batch 0/48] [D loss: 0.269970, acc:  45%] [G loss: 1.839900, adv: 0.466190, recon: 0.040203, id: 0.036474] time: 0:00:27.419457 \n",
            "[Epoch 3/200] [Batch 1/48] [D loss: 0.269009, acc:  43%] [G loss: 1.857536, adv: 0.456911, recon: 0.041948, id: 0.032972] time: 0:00:27.532152 \n",
            "[Epoch 3/200] [Batch 2/48] [D loss: 0.262492, acc:  46%] [G loss: 1.871822, adv: 0.469025, recon: 0.041747, id: 0.034291] time: 0:00:27.642713 \n",
            "[Epoch 3/200] [Batch 3/48] [D loss: 0.267745, acc:  45%] [G loss: 1.834478, adv: 0.468517, recon: 0.039810, id: 0.030987] time: 0:00:27.754078 \n",
            "[Epoch 3/200] [Batch 4/48] [D loss: 0.257368, acc:  49%] [G loss: 1.824042, adv: 0.473997, recon: 0.038586, id: 0.033484] time: 0:00:27.869347 \n",
            "[Epoch 3/200] [Batch 5/48] [D loss: 0.271552, acc:  44%] [G loss: 1.795218, adv: 0.458669, recon: 0.038852, id: 0.031445] time: 0:00:27.988038 \n",
            "[Epoch 3/200] [Batch 6/48] [D loss: 0.285689, acc:  40%] [G loss: 1.785148, adv: 0.440078, recon: 0.039930, id: 0.035847] time: 0:00:28.103473 \n",
            "[Epoch 3/200] [Batch 7/48] [D loss: 0.271756, acc:  43%] [G loss: 1.759554, adv: 0.454897, recon: 0.037398, id: 0.033447] time: 0:00:28.215001 \n",
            "[Epoch 3/200] [Batch 8/48] [D loss: 0.257268, acc:  46%] [G loss: 1.762976, adv: 0.463809, recon: 0.036682, id: 0.032073] time: 0:00:28.326204 \n",
            "[Epoch 3/200] [Batch 9/48] [D loss: 0.272870, acc:  43%] [G loss: 1.733528, adv: 0.447288, recon: 0.037000, id: 0.031394] time: 0:00:28.441452 \n",
            "[Epoch 3/200] [Batch 10/48] [D loss: 0.269009, acc:  44%] [G loss: 1.799953, adv: 0.457139, recon: 0.039151, id: 0.031633] time: 0:00:28.553867 \n",
            "[Epoch 3/200] [Batch 11/48] [D loss: 0.262556, acc:  48%] [G loss: 1.797172, adv: 0.464771, recon: 0.038378, id: 0.032835] time: 0:00:28.667810 \n",
            "[Epoch 3/200] [Batch 12/48] [D loss: 0.289247, acc:  39%] [G loss: 1.818317, adv: 0.431640, recon: 0.042249, id: 0.035477] time: 0:00:28.778551 \n",
            "[Epoch 3/200] [Batch 13/48] [D loss: 0.263670, acc:  46%] [G loss: 1.862395, adv: 0.461007, recon: 0.041998, id: 0.035371] time: 0:00:28.894115 \n",
            "[Epoch 3/200] [Batch 14/48] [D loss: 0.246577, acc:  49%] [G loss: 1.835655, adv: 0.476416, recon: 0.039194, id: 0.030555] time: 0:00:29.012935 \n",
            "[Epoch 3/200] [Batch 15/48] [D loss: 0.273003, acc:  44%] [G loss: 1.752963, adv: 0.446457, recon: 0.037831, id: 0.035016] time: 0:00:29.126542 \n",
            "[Epoch 3/200] [Batch 16/48] [D loss: 0.253112, acc:  50%] [G loss: 1.736397, adv: 0.466273, recon: 0.035297, id: 0.029734] time: 0:00:29.239980 \n",
            "[Epoch 3/200] [Batch 17/48] [D loss: 0.264077, acc:  46%] [G loss: 1.766797, adv: 0.457203, recon: 0.037548, id: 0.033297] time: 0:00:29.352315 \n",
            "[Epoch 3/200] [Batch 18/48] [D loss: 0.254057, acc:  50%] [G loss: 1.803336, adv: 0.470215, recon: 0.038287, id: 0.033106] time: 0:00:29.463179 \n",
            "[Epoch 3/200] [Batch 19/48] [D loss: 0.258118, acc:  49%] [G loss: 1.807467, adv: 0.458993, recon: 0.039377, id: 0.032328] time: 0:00:29.573749 \n",
            "[Epoch 3/200] [Batch 20/48] [D loss: 0.267610, acc:  47%] [G loss: 1.714067, adv: 0.445375, recon: 0.036263, id: 0.035558] time: 0:00:29.688759 \n",
            "[Epoch 3/200] [Batch 21/48] [D loss: 0.290678, acc:  39%] [G loss: 1.761616, adv: 0.429414, recon: 0.039843, id: 0.034671] time: 0:00:29.804380 \n",
            "[Epoch 3/200] [Batch 22/48] [D loss: 0.254584, acc:  49%] [G loss: 1.758018, adv: 0.470341, recon: 0.035918, id: 0.031340] time: 0:00:29.916473 \n",
            "[Epoch 3/200] [Batch 23/48] [D loss: 0.264935, acc:  46%] [G loss: 1.775648, adv: 0.453309, recon: 0.038330, id: 0.032420] time: 0:00:30.036964 \n",
            "[Epoch 3/200] [Batch 24/48] [D loss: 0.258315, acc:  45%] [G loss: 1.730135, adv: 0.449826, recon: 0.036483, id: 0.031406] time: 0:00:30.151205 \n",
            "[Epoch 3/200] [Batch 25/48] [D loss: 0.274523, acc:  42%] [G loss: 1.723060, adv: 0.438785, recon: 0.037274, id: 0.033224] time: 0:00:30.265719 \n",
            "[Epoch 3/200] [Batch 26/48] [D loss: 0.281467, acc:  41%] [G loss: 1.761176, adv: 0.432776, recon: 0.039536, id: 0.033189] time: 0:00:30.377316 \n",
            "[Epoch 3/200] [Batch 27/48] [D loss: 0.260416, acc:  49%] [G loss: 1.760236, adv: 0.462868, recon: 0.036798, id: 0.034611] time: 0:00:30.494016 \n",
            "[Epoch 3/200] [Batch 28/48] [D loss: 0.268953, acc:  44%] [G loss: 1.779998, adv: 0.439511, recon: 0.040033, id: 0.030929] time: 0:00:30.608616 \n",
            "[Epoch 3/200] [Batch 29/48] [D loss: 0.254754, acc:  49%] [G loss: 1.801652, adv: 0.464012, recon: 0.038658, id: 0.033510] time: 0:00:30.721241 \n",
            "[Epoch 3/200] [Batch 30/48] [D loss: 0.266582, acc:  45%] [G loss: 1.782991, adv: 0.441492, recon: 0.039965, id: 0.031354] time: 0:00:30.833743 \n",
            "[Epoch 3/200] [Batch 31/48] [D loss: 0.262305, acc:  47%] [G loss: 1.749696, adv: 0.452579, recon: 0.037362, id: 0.032463] time: 0:00:30.946679 \n",
            "[Epoch 3/200] [Batch 32/48] [D loss: 0.256014, acc:  48%] [G loss: 1.762519, adv: 0.452338, recon: 0.037830, id: 0.033061] time: 0:00:31.064309 \n",
            "[Epoch 3/200] [Batch 33/48] [D loss: 0.274690, acc:  44%] [G loss: 1.734415, adv: 0.437194, recon: 0.037815, id: 0.033332] time: 0:00:31.176628 \n",
            "[Epoch 3/200] [Batch 34/48] [D loss: 0.261729, acc:  46%] [G loss: 1.725701, adv: 0.451360, recon: 0.036180, id: 0.031363] time: 0:00:31.288403 \n",
            "[Epoch 3/200] [Batch 35/48] [D loss: 0.274063, acc:  42%] [G loss: 1.715979, adv: 0.432472, recon: 0.037527, id: 0.031329] time: 0:00:31.403095 \n",
            "[Epoch 3/200] [Batch 36/48] [D loss: 0.275928, acc:  45%] [G loss: 1.740427, adv: 0.441812, recon: 0.037891, id: 0.033536] time: 0:00:31.518191 \n",
            "[Epoch 3/200] [Batch 37/48] [D loss: 0.266967, acc:  44%] [G loss: 1.757777, adv: 0.440377, recon: 0.038832, id: 0.031821] time: 0:00:31.638921 \n",
            "[Epoch 3/200] [Batch 38/48] [D loss: 0.265590, acc:  45%] [G loss: 1.696724, adv: 0.442039, recon: 0.035895, id: 0.030581] time: 0:00:31.751490 \n",
            "[Epoch 3/200] [Batch 39/48] [D loss: 0.270374, acc:  44%] [G loss: 1.750333, adv: 0.437079, recon: 0.038818, id: 0.030948] time: 0:00:31.862064 \n",
            "[Epoch 3/200] [Batch 40/48] [D loss: 0.271836, acc:  46%] [G loss: 1.718004, adv: 0.446284, recon: 0.036411, id: 0.032998] time: 0:00:31.974918 \n",
            "[Epoch 3/200] [Batch 41/48] [D loss: 0.262572, acc:  45%] [G loss: 1.693224, adv: 0.437979, recon: 0.036172, id: 0.028564] time: 0:00:32.091690 \n",
            "[Epoch 3/200] [Batch 42/48] [D loss: 0.264841, acc:  44%] [G loss: 1.669536, adv: 0.440491, recon: 0.034747, id: 0.031217] time: 0:00:32.207964 \n",
            "[Epoch 3/200] [Batch 43/48] [D loss: 0.263689, acc:  45%] [G loss: 1.735616, adv: 0.439307, recon: 0.037827, id: 0.032094] time: 0:00:32.322730 \n",
            "[Epoch 3/200] [Batch 44/48] [D loss: 0.259973, acc:  47%] [G loss: 1.695592, adv: 0.450787, recon: 0.034936, id: 0.031823] time: 0:00:32.435519 \n",
            "[Epoch 3/200] [Batch 45/48] [D loss: 0.265579, acc:  44%] [G loss: 1.710801, adv: 0.436600, recon: 0.036943, id: 0.031340] time: 0:00:32.549146 \n",
            "[Epoch 3/200] [Batch 46/48] [D loss: 0.268459, acc:  44%] [G loss: 1.692299, adv: 0.438316, recon: 0.035907, id: 0.031735] time: 0:00:32.659988 \n",
            "[Epoch 3/200] [Batch 47/48] [D loss: 0.266391, acc:  43%] [G loss: 1.666261, adv: 0.433795, recon: 0.035285, id: 0.028307] time: 0:00:32.772049 \n",
            "[Epoch 4/200] [Batch 0/48] [D loss: 0.270624, acc:  44%] [G loss: 1.697940, adv: 0.431977, recon: 0.036762, id: 0.033278] time: 0:00:32.883013 \n",
            "[Epoch 4/200] [Batch 1/48] [D loss: 0.261234, acc:  45%] [G loss: 1.680056, adv: 0.437750, recon: 0.035424, id: 0.031544] time: 0:00:32.994494 \n",
            "[Epoch 4/200] [Batch 2/48] [D loss: 0.267039, acc:  44%] [G loss: 1.665935, adv: 0.431919, recon: 0.035339, id: 0.031082] time: 0:00:33.113919 \n",
            "[Epoch 4/200] [Batch 3/48] [D loss: 0.269018, acc:  44%] [G loss: 1.697916, adv: 0.434944, recon: 0.036603, id: 0.029651] time: 0:00:33.227207 \n",
            "[Epoch 4/200] [Batch 4/48] [D loss: 0.257179, acc:  49%] [G loss: 1.732264, adv: 0.446400, recon: 0.037010, id: 0.032543] time: 0:00:33.342984 \n",
            "[Epoch 4/200] [Batch 5/48] [D loss: 0.271035, acc:  43%] [G loss: 1.724058, adv: 0.431093, recon: 0.038283, id: 0.029394] time: 0:00:33.454972 \n",
            "[Epoch 4/200] [Batch 6/48] [D loss: 0.274881, acc:  43%] [G loss: 1.732913, adv: 0.426765, recon: 0.038981, id: 0.035836] time: 0:00:33.565481 \n",
            "[Epoch 4/200] [Batch 7/48] [D loss: 0.270054, acc:  42%] [G loss: 1.719663, adv: 0.423501, recon: 0.038697, id: 0.031712] time: 0:00:33.679240 \n",
            "[Epoch 4/200] [Batch 8/48] [D loss: 0.246014, acc:  49%] [G loss: 1.693084, adv: 0.451193, recon: 0.034785, id: 0.031228] time: 0:00:33.791033 \n",
            "[Epoch 4/200] [Batch 9/48] [D loss: 0.270988, acc:  42%] [G loss: 1.646950, adv: 0.420250, recon: 0.035584, id: 0.029234] time: 0:00:33.904593 \n",
            "[Epoch 4/200] [Batch 10/48] [D loss: 0.260371, acc:  46%] [G loss: 1.689054, adv: 0.439845, recon: 0.035708, id: 0.029935] time: 0:00:34.021026 \n",
            "[Epoch 4/200] [Batch 11/48] [D loss: 0.258535, acc:  49%] [G loss: 1.683479, adv: 0.438566, recon: 0.035551, id: 0.030886] time: 0:00:34.137931 \n",
            "[Epoch 4/200] [Batch 12/48] [D loss: 0.278443, acc:  42%] [G loss: 1.687275, adv: 0.416890, recon: 0.037585, id: 0.033916] time: 0:00:34.250521 \n",
            "[Epoch 4/200] [Batch 13/48] [D loss: 0.259443, acc:  46%] [G loss: 1.685294, adv: 0.434954, recon: 0.035960, id: 0.032372] time: 0:00:34.366180 \n",
            "[Epoch 4/200] [Batch 14/48] [D loss: 0.248372, acc:  47%] [G loss: 1.702004, adv: 0.444002, recon: 0.035961, id: 0.029205] time: 0:00:34.478963 \n",
            "[Epoch 4/200] [Batch 15/48] [D loss: 0.268691, acc:  45%] [G loss: 1.683540, adv: 0.426105, recon: 0.036587, id: 0.034589] time: 0:00:34.592313 \n",
            "[Epoch 4/200] [Batch 16/48] [D loss: 0.253465, acc:  49%] [G loss: 1.659532, adv: 0.439776, recon: 0.034323, id: 0.028316] time: 0:00:34.707169 \n",
            "[Epoch 4/200] [Batch 17/48] [D loss: 0.252778, acc:  49%] [G loss: 1.655480, adv: 0.445339, recon: 0.033546, id: 0.032507] time: 0:00:34.825533 \n",
            "[Epoch 4/200] [Batch 18/48] [D loss: 0.255552, acc:  48%] [G loss: 1.670814, adv: 0.435821, recon: 0.035183, id: 0.030599] time: 0:00:34.940753 \n",
            "[Epoch 4/200] [Batch 19/48] [D loss: 0.251770, acc:  51%] [G loss: 1.673892, adv: 0.443134, recon: 0.034543, id: 0.032380] time: 0:00:35.054380 \n",
            "[Epoch 4/200] [Batch 20/48] [D loss: 0.268857, acc:  45%] [G loss: 1.603799, adv: 0.415920, recon: 0.033821, id: 0.033913] time: 0:00:35.170289 \n",
            "[Epoch 4/200] [Batch 21/48] [D loss: 0.284057, acc:  39%] [G loss: 1.649641, adv: 0.409129, recon: 0.036550, id: 0.033812] time: 0:00:35.285108 \n",
            "[Epoch 4/200] [Batch 22/48] [D loss: 0.250607, acc:  50%] [G loss: 1.664248, adv: 0.443387, recon: 0.034120, id: 0.030025] time: 0:00:35.399913 \n",
            "[Epoch 4/200] [Batch 23/48] [D loss: 0.257218, acc:  47%] [G loss: 1.700298, adv: 0.436736, recon: 0.036415, id: 0.031067] time: 0:00:35.510941 \n",
            "[Epoch 4/200] [Batch 24/48] [D loss: 0.250832, acc:  47%] [G loss: 1.646158, adv: 0.436857, recon: 0.033832, id: 0.030454] time: 0:00:35.624219 \n",
            "[Epoch 4/200] [Batch 25/48] [D loss: 0.269124, acc:  42%] [G loss: 1.627522, adv: 0.417712, recon: 0.034772, id: 0.031726] time: 0:00:35.739498 \n",
            "[Epoch 4/200] [Batch 26/48] [D loss: 0.268030, acc:  44%] [G loss: 1.640200, adv: 0.420025, recon: 0.035088, id: 0.032761] time: 0:00:35.851243 \n",
            "[Epoch 4/200] [Batch 27/48] [D loss: 0.259012, acc:  47%] [G loss: 1.662183, adv: 0.431681, recon: 0.035092, id: 0.032094] time: 0:00:35.963697 \n",
            "[Epoch 4/200] [Batch 28/48] [D loss: 0.252715, acc:  49%] [G loss: 1.695799, adv: 0.438839, recon: 0.036213, id: 0.031605] time: 0:00:36.074903 \n",
            "[Epoch 4/200] [Batch 29/48] [D loss: 0.260987, acc:  45%] [G loss: 1.785636, adv: 0.427869, recon: 0.041315, id: 0.031100] time: 0:00:36.189488 \n",
            "[Epoch 4/200] [Batch 30/48] [D loss: 0.243636, acc:  54%] [G loss: 1.813234, adv: 0.453232, recon: 0.040778, id: 0.032077] time: 0:00:36.301963 \n",
            "[Epoch 4/200] [Batch 31/48] [D loss: 0.253895, acc:  48%] [G loss: 1.669307, adv: 0.433024, recon: 0.035503, id: 0.030095] time: 0:00:36.415488 \n",
            "[Epoch 4/200] [Batch 32/48] [D loss: 0.241068, acc:  53%] [G loss: 1.660779, adv: 0.445389, recon: 0.033715, id: 0.031973] time: 0:00:36.531415 \n",
            "[Epoch 4/200] [Batch 33/48] [D loss: 0.263711, acc:  47%] [G loss: 1.679771, adv: 0.424009, recon: 0.036581, id: 0.032746] time: 0:00:36.646584 \n",
            "[Epoch 4/200] [Batch 34/48] [D loss: 0.253689, acc:  48%] [G loss: 1.680862, adv: 0.437316, recon: 0.035474, id: 0.029410] time: 0:00:36.758527 \n",
            "[Epoch 4/200] [Batch 35/48] [D loss: 0.255235, acc:  49%] [G loss: 1.640682, adv: 0.433437, recon: 0.034002, id: 0.030945] time: 0:00:36.871031 \n",
            "[Epoch 4/200] [Batch 36/48] [D loss: 0.271309, acc:  44%] [G loss: 1.651483, adv: 0.420053, recon: 0.035736, id: 0.031297] time: 0:00:36.982987 \n",
            "[Epoch 4/200] [Batch 37/48] [D loss: 0.251529, acc:  50%] [G loss: 1.653356, adv: 0.437741, recon: 0.034166, id: 0.031385] time: 0:00:37.096039 \n",
            "[Epoch 4/200] [Batch 38/48] [D loss: 0.260988, acc:  46%] [G loss: 1.609274, adv: 0.422329, recon: 0.033575, id: 0.028564] time: 0:00:37.209235 \n",
            "[Epoch 4/200] [Batch 39/48] [D loss: 0.256929, acc:  49%] [G loss: 1.650255, adv: 0.431428, recon: 0.034658, id: 0.030577] time: 0:00:37.323729 \n",
            "[Epoch 4/200] [Batch 40/48] [D loss: 0.265610, acc:  47%] [G loss: 1.632896, adv: 0.427805, recon: 0.034126, id: 0.031247] time: 0:00:37.439322 \n",
            "[Epoch 4/200] [Batch 41/48] [D loss: 0.250677, acc:  49%] [G loss: 1.608923, adv: 0.429725, recon: 0.032968, id: 0.028188] time: 0:00:37.555005 \n",
            "[Epoch 4/200] [Batch 42/48] [D loss: 0.255771, acc:  46%] [G loss: 1.595659, adv: 0.427249, recon: 0.032501, id: 0.030046] time: 0:00:37.668488 \n",
            "[Epoch 4/200] [Batch 43/48] [D loss: 0.252680, acc:  48%] [G loss: 1.648914, adv: 0.429541, recon: 0.034695, id: 0.031075] time: 0:00:37.781849 \n",
            "[Epoch 4/200] [Batch 44/48] [D loss: 0.249955, acc:  50%] [G loss: 1.625066, adv: 0.437295, recon: 0.032873, id: 0.031006] time: 0:00:37.892490 \n",
            "[Epoch 4/200] [Batch 45/48] [D loss: 0.253194, acc:  47%] [G loss: 1.643353, adv: 0.430675, recon: 0.034330, id: 0.030789] time: 0:00:38.003890 \n",
            "[Epoch 4/200] [Batch 46/48] [D loss: 0.260085, acc:  46%] [G loss: 1.620899, adv: 0.426573, recon: 0.033643, id: 0.030896] time: 0:00:38.117439 \n",
            "[Epoch 4/200] [Batch 47/48] [D loss: 0.255683, acc:  47%] [G loss: 1.608623, adv: 0.426611, recon: 0.033279, id: 0.027602] time: 0:00:38.233592 \n",
            "[Epoch 5/200] [Batch 0/48] [D loss: 0.259524, acc:  47%] [G loss: 1.642863, adv: 0.426148, recon: 0.034766, id: 0.032607] time: 0:00:38.347514 \n",
            "[Epoch 5/200] [Batch 1/48] [D loss: 0.254915, acc:  46%] [G loss: 1.635449, adv: 0.424490, recon: 0.034614, id: 0.030961] time: 0:00:38.459157 \n",
            "[Epoch 5/200] [Batch 2/48] [D loss: 0.255041, acc:  48%] [G loss: 1.640235, adv: 0.427925, recon: 0.034650, id: 0.030837] time: 0:00:38.571901 \n",
            "[Epoch 5/200] [Batch 3/48] [D loss: 0.261389, acc:  45%] [G loss: 1.720945, adv: 0.424542, recon: 0.038829, id: 0.029502] time: 0:00:38.685220 \n",
            "[Epoch 5/200] [Batch 4/48] [D loss: 0.242373, acc:  54%] [G loss: 1.786985, adv: 0.448818, recon: 0.039711, id: 0.032558] time: 0:00:38.797406 \n",
            "[Epoch 5/200] [Batch 5/48] [D loss: 0.258009, acc:  47%] [G loss: 1.714736, adv: 0.428096, recon: 0.038247, id: 0.028939] time: 0:00:38.911737 \n",
            "[Epoch 5/200] [Batch 6/48] [D loss: 0.263740, acc:  47%] [G loss: 1.631771, adv: 0.419778, recon: 0.034784, id: 0.033704] time: 0:00:39.027462 \n",
            "[Epoch 5/200] [Batch 7/48] [D loss: 0.254775, acc:  48%] [G loss: 1.604553, adv: 0.426498, recon: 0.032929, id: 0.031132] time: 0:00:39.141549 \n",
            "[Epoch 5/200] [Batch 8/48] [D loss: 0.243688, acc:  50%] [G loss: 1.614324, adv: 0.433513, recon: 0.032675, id: 0.029851] time: 0:00:39.255759 \n",
            "[Epoch 5/200] [Batch 9/48] [D loss: 0.257731, acc:  47%] [G loss: 1.603747, adv: 0.420620, recon: 0.033563, id: 0.029136] time: 0:00:39.368970 \n",
            "[Epoch 5/200] [Batch 10/48] [D loss: 0.254201, acc:  48%] [G loss: 1.639083, adv: 0.428792, recon: 0.034417, id: 0.029457] time: 0:00:39.483119 \n",
            "[Epoch 5/200] [Batch 11/48] [D loss: 0.249258, acc:  51%] [G loss: 1.632671, adv: 0.434547, recon: 0.033527, id: 0.030382] time: 0:00:39.594589 \n",
            "[Epoch 5/200] [Batch 12/48] [D loss: 0.269560, acc:  44%] [G loss: 1.631372, adv: 0.413384, recon: 0.035290, id: 0.033397] time: 0:00:39.707766 \n",
            "[Epoch 5/200] [Batch 13/48] [D loss: 0.254636, acc:  48%] [G loss: 1.610722, adv: 0.423353, recon: 0.033497, id: 0.030925] time: 0:00:39.821907 \n",
            "[Epoch 5/200] [Batch 14/48] [D loss: 0.238504, acc:  51%] [G loss: 1.631372, adv: 0.441292, recon: 0.032859, id: 0.028671] time: 0:00:39.938770 \n",
            "[Epoch 5/200] [Batch 15/48] [D loss: 0.262507, acc:  46%] [G loss: 1.624335, adv: 0.418665, recon: 0.034484, id: 0.033217] time: 0:00:40.053521 \n",
            "[Epoch 5/200] [Batch 16/48] [D loss: 0.244557, acc:  52%] [G loss: 1.595218, adv: 0.437421, recon: 0.031526, id: 0.027923] time: 0:00:40.169900 \n",
            "[Epoch 5/200] [Batch 17/48] [D loss: 0.247629, acc:  50%] [G loss: 1.603185, adv: 0.435107, recon: 0.032033, id: 0.031469] time: 0:00:40.282058 \n",
            "[Epoch 5/200] [Batch 18/48] [D loss: 0.247273, acc:  51%] [G loss: 1.625984, adv: 0.434116, recon: 0.033260, id: 0.030563] time: 0:00:40.393314 \n",
            "[Epoch 5/200] [Batch 19/48] [D loss: 0.246635, acc:  53%] [G loss: 1.639092, adv: 0.433988, recon: 0.033810, id: 0.031116] time: 0:00:40.507480 \n",
            "[Epoch 5/200] [Batch 20/48] [D loss: 0.258902, acc:  49%] [G loss: 1.568915, adv: 0.412776, recon: 0.032574, id: 0.033310] time: 0:00:40.620501 \n",
            "[Epoch 5/200] [Batch 21/48] [D loss: 0.276596, acc:  41%] [G loss: 1.642772, adv: 0.404067, recon: 0.036784, id: 0.033099] time: 0:00:40.735167 \n",
            "[Epoch 5/200] [Batch 22/48] [D loss: 0.241564, acc:  53%] [G loss: 1.646932, adv: 0.442693, recon: 0.033484, id: 0.029847] time: 0:00:40.849967 \n",
            "[Epoch 5/200] [Batch 23/48] [D loss: 0.250399, acc:  49%] [G loss: 1.670965, adv: 0.430609, recon: 0.035679, id: 0.030114] time: 0:00:40.971614 \n",
            "[Epoch 5/200] [Batch 24/48] [D loss: 0.241103, acc:  52%] [G loss: 1.612776, adv: 0.435620, recon: 0.032468, id: 0.030005] time: 0:00:41.083543 \n",
            "[Epoch 5/200] [Batch 25/48] [D loss: 0.263169, acc:  44%] [G loss: 1.609924, adv: 0.410629, recon: 0.034680, id: 0.030609] time: 0:00:41.199691 \n",
            "[Epoch 5/200] [Batch 26/48] [D loss: 0.257767, acc:  48%] [G loss: 1.629640, adv: 0.420552, recon: 0.034675, id: 0.032798] time: 0:00:41.314103 \n",
            "[Epoch 5/200] [Batch 27/48] [D loss: 0.251317, acc:  50%] [G loss: 1.652497, adv: 0.428342, recon: 0.035049, id: 0.031512] time: 0:00:41.432280 \n",
            "[Epoch 5/200] [Batch 28/48] [D loss: 0.245943, acc:  52%] [G loss: 1.641834, adv: 0.435196, recon: 0.034016, id: 0.030269] time: 0:00:41.546024 \n",
            "[Epoch 5/200] [Batch 29/48] [D loss: 0.248169, acc:  50%] [G loss: 1.630613, adv: 0.433068, recon: 0.033347, id: 0.030257] time: 0:00:41.660074 \n",
            "[Epoch 5/200] [Batch 30/48] [D loss: 0.245677, acc:  53%] [G loss: 1.610232, adv: 0.431662, recon: 0.032747, id: 0.029932] time: 0:00:41.774297 \n",
            "[Epoch 5/200] [Batch 31/48] [D loss: 0.254362, acc:  49%] [G loss: 1.587846, adv: 0.421395, recon: 0.032641, id: 0.029695] time: 0:00:41.887179 \n",
            "[Epoch 5/200] [Batch 32/48] [D loss: 0.242888, acc:  52%] [G loss: 1.637134, adv: 0.433767, recon: 0.033691, id: 0.032273] time: 0:00:42.000987 \n",
            "[Epoch 5/200] [Batch 33/48] [D loss: 0.258269, acc:  49%] [G loss: 1.620840, adv: 0.421177, recon: 0.034083, id: 0.032103] time: 0:00:42.114216 \n",
            "[Epoch 5/200] [Batch 34/48] [D loss: 0.246657, acc:  50%] [G loss: 1.611579, adv: 0.432384, recon: 0.032662, id: 0.029338] time: 0:00:42.226385 \n",
            "[Epoch 5/200] [Batch 35/48] [D loss: 0.251850, acc:  48%] [G loss: 1.583510, adv: 0.422535, recon: 0.032253, id: 0.030421] time: 0:00:42.338892 \n",
            "[Epoch 5/200] [Batch 36/48] [D loss: 0.265415, acc:  46%] [G loss: 1.617109, adv: 0.417707, recon: 0.034306, id: 0.031215] time: 0:00:42.452548 \n",
            "[Epoch 5/200] [Batch 37/48] [D loss: 0.247609, acc:  51%] [G loss: 1.613032, adv: 0.432174, recon: 0.032807, id: 0.031002] time: 0:00:42.566684 \n",
            "[Epoch 5/200] [Batch 38/48] [D loss: 0.255839, acc:  47%] [G loss: 1.565920, adv: 0.416183, recon: 0.032112, id: 0.028301] time: 0:00:42.680563 \n",
            "[Epoch 5/200] [Batch 39/48] [D loss: 0.250935, acc:  51%] [G loss: 1.606872, adv: 0.426974, recon: 0.033057, id: 0.030483] time: 0:00:42.793899 \n",
            "[Epoch 5/200] [Batch 40/48] [D loss: 0.260416, acc:  48%] [G loss: 1.594461, adv: 0.422551, recon: 0.032817, id: 0.030656] time: 0:00:42.907897 \n",
            "[Epoch 5/200] [Batch 41/48] [D loss: 0.245203, acc:  52%] [G loss: 1.565136, adv: 0.424166, recon: 0.031442, id: 0.028034] time: 0:00:43.020508 \n",
            "[Epoch 5/200] [Batch 42/48] [D loss: 0.251157, acc:  48%] [G loss: 1.570643, adv: 0.422400, recon: 0.031775, id: 0.029800] time: 0:00:43.130666 \n",
            "[Epoch 5/200] [Batch 43/48] [D loss: 0.245834, acc:  50%] [G loss: 1.612583, adv: 0.429499, recon: 0.033042, id: 0.031239] time: 0:00:43.247621 \n",
            "[Epoch 5/200] [Batch 44/48] [D loss: 0.247788, acc:  50%] [G loss: 1.601804, adv: 0.428773, recon: 0.032566, id: 0.030586] time: 0:00:43.363129 \n",
            "[Epoch 5/200] [Batch 45/48] [D loss: 0.244080, acc:  51%] [G loss: 1.620494, adv: 0.432954, recon: 0.033126, id: 0.030812] time: 0:00:43.477471 \n",
            "[Epoch 5/200] [Batch 46/48] [D loss: 0.257116, acc:  46%] [G loss: 1.628427, adv: 0.419707, recon: 0.034699, id: 0.030454] time: 0:00:43.589923 \n",
            "[Epoch 5/200] [Batch 47/48] [D loss: 0.246104, acc:  52%] [G loss: 1.651983, adv: 0.431495, recon: 0.035107, id: 0.029099] time: 0:00:43.704291 \n",
            "[Epoch 6/200] [Batch 0/48] [D loss: 0.259933, acc:  46%] [G loss: 1.738708, adv: 0.415196, recon: 0.040508, id: 0.032503] time: 0:00:43.818932 \n",
            "[Epoch 6/200] [Batch 1/48] [D loss: 0.238726, acc:  54%] [G loss: 1.650458, adv: 0.440675, recon: 0.033942, id: 0.032416] time: 0:00:43.932132 \n",
            "[Epoch 6/200] [Batch 2/48] [D loss: 0.253288, acc:  48%] [G loss: 1.574742, adv: 0.416780, recon: 0.032516, id: 0.029804] time: 0:00:44.045131 \n",
            "[Epoch 6/200] [Batch 3/48] [D loss: 0.249246, acc:  50%] [G loss: 1.600170, adv: 0.430601, recon: 0.032451, id: 0.028813] time: 0:00:44.159207 \n",
            "[Epoch 6/200] [Batch 4/48] [D loss: 0.244159, acc:  53%] [G loss: 1.628074, adv: 0.431352, recon: 0.033524, id: 0.031071] time: 0:00:44.274094 \n",
            "[Epoch 6/200] [Batch 5/48] [D loss: 0.250278, acc:  50%] [G loss: 1.598824, adv: 0.429381, recon: 0.032510, id: 0.028746] time: 0:00:44.385874 \n",
            "[Epoch 6/200] [Batch 6/48] [D loss: 0.263458, acc:  46%] [G loss: 1.592579, adv: 0.409676, recon: 0.033856, id: 0.033397] time: 0:00:44.498590 \n",
            "[Epoch 6/200] [Batch 7/48] [D loss: 0.253712, acc:  48%] [G loss: 1.586075, adv: 0.418008, recon: 0.032847, id: 0.031182] time: 0:00:44.611097 \n",
            "[Epoch 6/200] [Batch 8/48] [D loss: 0.239076, acc:  52%] [G loss: 1.580657, adv: 0.430557, recon: 0.031393, id: 0.030190] time: 0:00:44.723810 \n",
            "[Epoch 6/200] [Batch 9/48] [D loss: 0.255706, acc:  47%] [G loss: 1.571115, adv: 0.411659, recon: 0.032851, id: 0.028880] time: 0:00:44.836754 \n",
            "[Epoch 6/200] [Batch 10/48] [D loss: 0.246677, acc:  51%] [G loss: 1.606476, adv: 0.430405, recon: 0.032754, id: 0.029518] time: 0:00:44.954272 \n",
            "[Epoch 6/200] [Batch 11/48] [D loss: 0.247802, acc:  51%] [G loss: 1.599548, adv: 0.424322, recon: 0.032924, id: 0.030122] time: 0:00:45.070235 \n",
            "[Epoch 6/200] [Batch 12/48] [D loss: 0.262664, acc:  47%] [G loss: 1.589864, adv: 0.412548, recon: 0.033445, id: 0.033308] time: 0:00:45.183741 \n",
            "[Epoch 6/200] [Batch 13/48] [D loss: 0.251477, acc:  49%] [G loss: 1.579333, adv: 0.417154, recon: 0.032612, id: 0.030250] time: 0:00:45.296926 \n",
            "[Epoch 6/200] [Batch 14/48] [D loss: 0.234739, acc:  53%] [G loss: 1.602135, adv: 0.438007, recon: 0.031805, id: 0.028904] time: 0:00:45.408994 \n",
            "[Epoch 6/200] [Batch 15/48] [D loss: 0.260079, acc:  47%] [G loss: 1.595948, adv: 0.412485, recon: 0.033756, id: 0.033086] time: 0:00:45.520919 \n",
            "[Epoch 6/200] [Batch 16/48] [D loss: 0.241457, acc:  54%] [G loss: 1.568087, adv: 0.431689, recon: 0.030821, id: 0.027929] time: 0:00:45.632606 \n",
            "[Epoch 6/200] [Batch 17/48] [D loss: 0.243598, acc:  51%] [G loss: 1.579563, adv: 0.430448, recon: 0.031388, id: 0.031285] time: 0:00:45.745302 \n",
            "[Epoch 6/200] [Batch 18/48] [D loss: 0.242198, acc:  53%] [G loss: 1.615277, adv: 0.433201, recon: 0.032904, id: 0.030555] time: 0:00:45.860292 \n",
            "[Epoch 6/200] [Batch 19/48] [D loss: 0.243288, acc:  53%] [G loss: 1.663703, adv: 0.430544, recon: 0.035406, id: 0.031014] time: 0:00:45.974529 \n",
            "[Epoch 6/200] [Batch 20/48] [D loss: 0.251084, acc:  52%] [G loss: 1.599414, adv: 0.415397, recon: 0.033953, id: 0.033110] time: 0:00:46.088536 \n",
            "[Epoch 6/200] [Batch 21/48] [D loss: 0.271379, acc:  43%] [G loss: 1.648745, adv: 0.400780, recon: 0.037480, id: 0.033002] time: 0:00:46.202566 \n",
            "[Epoch 6/200] [Batch 22/48] [D loss: 0.236426, acc:  55%] [G loss: 1.619135, adv: 0.441378, recon: 0.032309, id: 0.029453] time: 0:00:46.315391 \n",
            "[Epoch 6/200] [Batch 23/48] [D loss: 0.243342, acc:  53%] [G loss: 1.626863, adv: 0.433641, recon: 0.033310, id: 0.029871] time: 0:00:46.427412 \n",
            "[Epoch 6/200] [Batch 24/48] [D loss: 0.239501, acc:  52%] [G loss: 1.577345, adv: 0.428526, recon: 0.031457, id: 0.028914] time: 0:00:46.540785 \n",
            "[Epoch 6/200] [Batch 25/48] [D loss: 0.256820, acc:  47%] [G loss: 1.558275, adv: 0.409057, recon: 0.032454, id: 0.030700] time: 0:00:46.654446 \n",
            "[Epoch 6/200] [Batch 26/48] [D loss: 0.258886, acc:  47%] [G loss: 1.578194, adv: 0.406536, recon: 0.033501, id: 0.031478] time: 0:00:46.769812 \n",
            "[Epoch 6/200] [Batch 27/48] [D loss: 0.244408, acc:  53%] [G loss: 1.595173, adv: 0.432017, recon: 0.031966, id: 0.031832] time: 0:00:46.883638 \n",
            "[Epoch 6/200] [Batch 28/48] [D loss: 0.248911, acc:  50%] [G loss: 1.603685, adv: 0.423141, recon: 0.033253, id: 0.029593] time: 0:00:46.996396 \n",
            "[Epoch 6/200] [Batch 29/48] [D loss: 0.240891, acc:  54%] [G loss: 1.602638, adv: 0.435600, recon: 0.031858, id: 0.030267] time: 0:00:47.109095 \n",
            "[Epoch 6/200] [Batch 30/48] [D loss: 0.244364, acc:  52%] [G loss: 1.585266, adv: 0.422233, recon: 0.032446, id: 0.030006] time: 0:00:47.222061 \n",
            "[Epoch 6/200] [Batch 31/48] [D loss: 0.249433, acc:  50%] [G loss: 1.572234, adv: 0.418521, recon: 0.032217, id: 0.029746] time: 0:00:47.335069 \n",
            "[Epoch 6/200] [Batch 32/48] [D loss: 0.240158, acc:  53%] [G loss: 1.620349, adv: 0.429388, recon: 0.033344, id: 0.032059] time: 0:00:47.446093 \n",
            "[Epoch 6/200] [Batch 33/48] [D loss: 0.254735, acc:  49%] [G loss: 1.600890, adv: 0.416369, recon: 0.033671, id: 0.032002] time: 0:00:47.560536 \n",
            "[Epoch 6/200] [Batch 34/48] [D loss: 0.244204, acc:  51%] [G loss: 1.597134, adv: 0.425597, recon: 0.032668, id: 0.029164] time: 0:00:47.674105 \n",
            "[Epoch 6/200] [Batch 35/48] [D loss: 0.246587, acc:  51%] [G loss: 1.568788, adv: 0.421521, recon: 0.031720, id: 0.030186] time: 0:00:47.790331 \n",
            "[Epoch 6/200] [Batch 36/48] [D loss: 0.262383, acc:  47%] [G loss: 1.602649, adv: 0.411724, recon: 0.034218, id: 0.030984] time: 0:00:47.903762 \n",
            "[Epoch 6/200] [Batch 37/48] [D loss: 0.241599, acc:  53%] [G loss: 1.602761, adv: 0.433172, recon: 0.032286, id: 0.030788] time: 0:00:48.014498 \n",
            "[Epoch 6/200] [Batch 38/48] [D loss: 0.252055, acc:  48%] [G loss: 1.547511, adv: 0.413511, recon: 0.031519, id: 0.028080] time: 0:00:48.125734 \n",
            "[Epoch 6/200] [Batch 39/48] [D loss: 0.246874, acc:  53%] [G loss: 1.580764, adv: 0.424342, recon: 0.032098, id: 0.030176] time: 0:00:48.238389 \n",
            "[Epoch 6/200] [Batch 40/48] [D loss: 0.257211, acc:  49%] [G loss: 1.574549, adv: 0.417971, recon: 0.032334, id: 0.030500] time: 0:00:48.352136 \n",
            "[Epoch 6/200] [Batch 41/48] [D loss: 0.240606, acc:  54%] [G loss: 1.549380, adv: 0.423052, recon: 0.030824, id: 0.028195] time: 0:00:48.467450 \n",
            "[Epoch 6/200] [Batch 42/48] [D loss: 0.248689, acc:  48%] [G loss: 1.566774, adv: 0.419662, recon: 0.031872, id: 0.029896] time: 0:00:48.582588 \n",
            "[Epoch 6/200] [Batch 43/48] [D loss: 0.242492, acc:  51%] [G loss: 1.597718, adv: 0.426283, recon: 0.032700, id: 0.031351] time: 0:00:48.695949 \n",
            "[Epoch 6/200] [Batch 44/48] [D loss: 0.246037, acc:  51%] [G loss: 1.588808, adv: 0.420612, recon: 0.032758, id: 0.030925] time: 0:00:48.809974 \n",
            "[Epoch 6/200] [Batch 45/48] [D loss: 0.238450, acc:  54%] [G loss: 1.611102, adv: 0.431308, recon: 0.032889, id: 0.030714] time: 0:00:48.924476 \n",
            "[Epoch 6/200] [Batch 46/48] [D loss: 0.252392, acc:  47%] [G loss: 1.612811, adv: 0.419513, recon: 0.034013, id: 0.030380] time: 0:00:49.037826 \n",
            "[Epoch 6/200] [Batch 47/48] [D loss: 0.242911, acc:  52%] [G loss: 1.602747, adv: 0.428581, recon: 0.033006, id: 0.028290] time: 0:00:49.149206 \n",
            "[Epoch 7/200] [Batch 0/48] [D loss: 0.255330, acc:  48%] [G loss: 1.643905, adv: 0.413554, recon: 0.036099, id: 0.031875] time: 0:00:49.267017 \n",
            "[Epoch 7/200] [Batch 1/48] [D loss: 0.239508, acc:  53%] [G loss: 1.577696, adv: 0.429655, recon: 0.031434, id: 0.031955] time: 0:00:49.381813 \n",
            "[Epoch 7/200] [Batch 2/48] [D loss: 0.252179, acc:  48%] [G loss: 1.549678, adv: 0.411801, recon: 0.031799, id: 0.030029] time: 0:00:49.494159 \n",
            "[Epoch 7/200] [Batch 3/48] [D loss: 0.247153, acc:  51%] [G loss: 1.575771, adv: 0.426876, recon: 0.031665, id: 0.029052] time: 0:00:49.608442 \n",
            "[Epoch 7/200] [Batch 4/48] [D loss: 0.243673, acc:  53%] [G loss: 1.601143, adv: 0.424180, recon: 0.032910, id: 0.031088] time: 0:00:49.720629 \n",
            "[Epoch 7/200] [Batch 5/48] [D loss: 0.246066, acc:  52%] [G loss: 1.576477, adv: 0.429405, recon: 0.031448, id: 0.028869] time: 0:00:49.832864 \n",
            "[Epoch 7/200] [Batch 6/48] [D loss: 0.262248, acc:  46%] [G loss: 1.572137, adv: 0.404561, recon: 0.033382, id: 0.033042] time: 0:00:49.944769 \n",
            "[Epoch 7/200] [Batch 7/48] [D loss: 0.249657, acc:  49%] [G loss: 1.554513, adv: 0.414838, recon: 0.031680, id: 0.031048] time: 0:00:50.058475 \n",
            "[Epoch 7/200] [Batch 8/48] [D loss: 0.237592, acc:  53%] [G loss: 1.561674, adv: 0.425525, recon: 0.030969, id: 0.030298] time: 0:00:50.173321 \n",
            "[Epoch 7/200] [Batch 9/48] [D loss: 0.252107, acc:  48%] [G loss: 1.549660, adv: 0.411944, recon: 0.031812, id: 0.028896] time: 0:00:50.284794 \n",
            "[Epoch 7/200] [Batch 10/48] [D loss: 0.246668, acc:  50%] [G loss: 1.580653, adv: 0.423621, recon: 0.032186, id: 0.029552] time: 0:00:50.397884 \n",
            "[Epoch 7/200] [Batch 11/48] [D loss: 0.243594, acc:  53%] [G loss: 1.573283, adv: 0.424897, recon: 0.031633, id: 0.029885] time: 0:00:50.510052 \n",
            "[Epoch 7/200] [Batch 12/48] [D loss: 0.261568, acc:  46%] [G loss: 1.581345, adv: 0.407943, recon: 0.033478, id: 0.033338] time: 0:00:50.623284 \n",
            "[Epoch 7/200] [Batch 13/48] [D loss: 0.248557, acc:  50%] [G loss: 1.558598, adv: 0.412801, recon: 0.032101, id: 0.030416] time: 0:00:50.735807 \n",
            "[Epoch 7/200] [Batch 14/48] [D loss: 0.234210, acc:  53%] [G loss: 1.599713, adv: 0.429058, recon: 0.032579, id: 0.028826] time: 0:00:50.850121 \n",
            "[Epoch 7/200] [Batch 15/48] [D loss: 0.253289, acc:  49%] [G loss: 1.607020, adv: 0.415201, recon: 0.034141, id: 0.033229] time: 0:00:50.963853 \n",
            "[Epoch 7/200] [Batch 16/48] [D loss: 0.242218, acc:  53%] [G loss: 1.590425, adv: 0.423951, recon: 0.032666, id: 0.028869] time: 0:00:51.077373 \n",
            "[Epoch 7/200] [Batch 17/48] [D loss: 0.234801, acc:  56%] [G loss: 1.597900, adv: 0.438564, recon: 0.031650, id: 0.031184] time: 0:00:51.190577 \n",
            "[Epoch 7/200] [Batch 18/48] [D loss: 0.244294, acc:  52%] [G loss: 1.610319, adv: 0.421596, recon: 0.033805, id: 0.030250] time: 0:00:51.302205 \n",
            "[Epoch 7/200] [Batch 19/48] [D loss: 0.234306, acc:  57%] [G loss: 1.632184, adv: 0.439023, recon: 0.033187, id: 0.030688] time: 0:00:51.415084 \n",
            "[Epoch 7/200] [Batch 20/48] [D loss: 0.255332, acc:  49%] [G loss: 1.507336, adv: 0.400477, recon: 0.030825, id: 0.032363] time: 0:00:51.539526 \n",
            "[Epoch 7/200] [Batch 21/48] [D loss: 0.263003, acc:  46%] [G loss: 1.571036, adv: 0.406792, recon: 0.033206, id: 0.033285] time: 0:00:51.652228 \n",
            "[Epoch 7/200] [Batch 22/48] [D loss: 0.239699, acc:  53%] [G loss: 1.573511, adv: 0.427392, recon: 0.031423, id: 0.028839] time: 0:00:51.768310 \n",
            "[Epoch 7/200] [Batch 23/48] [D loss: 0.240582, acc:  54%] [G loss: 1.620410, adv: 0.431162, recon: 0.033280, id: 0.030168] time: 0:00:51.881718 \n",
            "[Epoch 7/200] [Batch 24/48] [D loss: 0.240919, acc:  51%] [G loss: 1.592675, adv: 0.421341, recon: 0.032918, id: 0.028987] time: 0:00:51.995163 \n",
            "[Epoch 7/200] [Batch 25/48] [D loss: 0.250768, acc:  50%] [G loss: 1.578146, adv: 0.414289, recon: 0.033025, id: 0.030612] time: 0:00:52.107593 \n",
            "[Epoch 7/200] [Batch 26/48] [D loss: 0.257633, acc:  47%] [G loss: 1.596394, adv: 0.401136, recon: 0.034961, id: 0.031730] time: 0:00:52.220324 \n",
            "[Epoch 7/200] [Batch 27/48] [D loss: 0.240444, acc:  54%] [G loss: 1.606343, adv: 0.431307, recon: 0.032664, id: 0.031899] time: 0:00:52.332537 \n",
            "[Epoch 7/200] [Batch 28/48] [D loss: 0.247295, acc:  50%] [G loss: 1.597707, adv: 0.419935, recon: 0.033327, id: 0.029585] time: 0:00:52.445087 \n",
            "[Epoch 7/200] [Batch 29/48] [D loss: 0.237890, acc:  55%] [G loss: 1.585353, adv: 0.433764, recon: 0.031258, id: 0.030048] time: 0:00:52.557456 \n",
            "[Epoch 7/200] [Batch 30/48] [D loss: 0.240197, acc:  54%] [G loss: 1.553624, adv: 0.420995, recon: 0.031088, id: 0.029914] time: 0:00:52.673501 \n",
            "[Epoch 7/200] [Batch 31/48] [D loss: 0.248054, acc:  50%] [G loss: 1.540318, adv: 0.412746, recon: 0.031271, id: 0.029287] time: 0:00:52.786228 \n",
            "[Epoch 7/200] [Batch 32/48] [D loss: 0.236672, acc:  55%] [G loss: 1.591378, adv: 0.427753, recon: 0.032148, id: 0.032309] time: 0:00:52.898890 \n",
            "[Epoch 7/200] [Batch 33/48] [D loss: 0.254643, acc:  49%] [G loss: 1.582865, adv: 0.413247, recon: 0.033109, id: 0.031414] time: 0:00:53.011584 \n",
            "[Epoch 7/200] [Batch 34/48] [D loss: 0.240403, acc:  53%] [G loss: 1.570799, adv: 0.426282, recon: 0.031427, id: 0.029206] time: 0:00:53.126482 \n",
            "[Epoch 7/200] [Batch 35/48] [D loss: 0.249513, acc:  49%] [G loss: 1.561175, adv: 0.410769, recon: 0.032392, id: 0.029726] time: 0:00:53.242263 \n",
            "[Epoch 7/200] [Batch 36/48] [D loss: 0.254499, acc:  50%] [G loss: 1.607199, adv: 0.416743, recon: 0.034070, id: 0.031885] time: 0:00:53.353760 \n",
            "[Epoch 7/200] [Batch 37/48] [D loss: 0.244465, acc:  51%] [G loss: 1.637872, adv: 0.421241, recon: 0.035132, id: 0.030794] time: 0:00:53.468906 \n",
            "[Epoch 7/200] [Batch 38/48] [D loss: 0.245173, acc:  51%] [G loss: 1.576012, adv: 0.417654, recon: 0.032636, id: 0.028780] time: 0:00:53.580655 \n",
            "[Epoch 7/200] [Batch 39/48] [D loss: 0.248390, acc:  51%] [G loss: 1.611864, adv: 0.415588, recon: 0.034505, id: 0.029966] time: 0:00:53.693409 \n",
            "[Epoch 7/200] [Batch 40/48] [D loss: 0.250877, acc:  51%] [G loss: 1.580910, adv: 0.425426, recon: 0.032034, id: 0.030738] time: 0:00:53.803800 \n",
            "[Epoch 7/200] [Batch 41/48] [D loss: 0.244274, acc:  52%] [G loss: 1.528695, adv: 0.410536, recon: 0.031071, id: 0.027909] time: 0:00:53.915897 \n",
            "[Epoch 7/200] [Batch 42/48] [D loss: 0.243872, acc:  51%] [G loss: 1.537633, adv: 0.420845, recon: 0.030423, id: 0.030123] time: 0:00:54.032070 \n",
            "[Epoch 7/200] [Batch 43/48] [D loss: 0.242706, acc:  51%] [G loss: 1.563936, adv: 0.419633, recon: 0.031697, id: 0.030874] time: 0:00:54.146940 \n",
            "[Epoch 7/200] [Batch 44/48] [D loss: 0.241788, acc:  53%] [G loss: 1.553059, adv: 0.423043, recon: 0.030844, id: 0.030854] time: 0:00:54.260602 \n",
            "[Epoch 7/200] [Batch 45/48] [D loss: 0.242775, acc:  52%] [G loss: 1.564827, adv: 0.419358, recon: 0.031742, id: 0.030838] time: 0:00:54.373199 \n",
            "[Epoch 7/200] [Batch 46/48] [D loss: 0.249366, acc:  49%] [G loss: 1.554368, adv: 0.417869, recon: 0.031382, id: 0.030064] time: 0:00:54.487052 \n",
            "[Epoch 7/200] [Batch 47/48] [D loss: 0.248741, acc:  48%] [G loss: 1.534543, adv: 0.411525, recon: 0.031269, id: 0.027359] time: 0:00:54.600119 \n",
            "[Epoch 8/200] [Batch 0/48] [D loss: 0.250671, acc:  50%] [G loss: 1.573944, adv: 0.414297, recon: 0.032722, id: 0.032057] time: 0:00:54.712204 \n",
            "[Epoch 8/200] [Batch 1/48] [D loss: 0.246807, acc:  50%] [G loss: 1.574395, adv: 0.412667, recon: 0.032914, id: 0.031603] time: 0:00:54.826003 \n",
            "[Epoch 8/200] [Batch 2/48] [D loss: 0.247475, acc:  51%] [G loss: 1.567022, adv: 0.416901, recon: 0.032287, id: 0.030187] time: 0:00:54.939135 \n",
            "[Epoch 8/200] [Batch 3/48] [D loss: 0.252601, acc:  48%] [G loss: 1.614854, adv: 0.412667, recon: 0.034965, id: 0.029904] time: 0:00:55.051779 \n",
            "[Epoch 8/200] [Batch 4/48] [D loss: 0.237459, acc:  55%] [G loss: 1.633462, adv: 0.429798, recon: 0.034098, id: 0.031219] time: 0:00:55.165690 \n",
            "[Epoch 8/200] [Batch 5/48] [D loss: 0.248075, acc:  51%] [G loss: 1.601129, adv: 0.420994, recon: 0.033484, id: 0.029025] time: 0:00:55.278619 \n",
            "[Epoch 8/200] [Batch 6/48] [D loss: 0.259009, acc:  47%] [G loss: 1.561121, adv: 0.403115, recon: 0.033058, id: 0.032778] time: 0:00:55.390780 \n",
            "[Epoch 8/200] [Batch 7/48] [D loss: 0.248715, acc:  50%] [G loss: 1.533771, adv: 0.409885, recon: 0.031206, id: 0.030986] time: 0:00:55.503509 \n",
            "[Epoch 8/200] [Batch 8/48] [D loss: 0.237514, acc:  53%] [G loss: 1.553090, adv: 0.420123, recon: 0.031117, id: 0.030501] time: 0:00:55.617202 \n",
            "[Epoch 8/200] [Batch 9/48] [D loss: 0.248585, acc:  50%] [G loss: 1.539262, adv: 0.411079, recon: 0.031474, id: 0.028802] time: 0:00:55.731003 \n",
            "[Epoch 8/200] [Batch 10/48] [D loss: 0.249559, acc:  49%] [G loss: 1.589698, adv: 0.413323, recon: 0.033679, id: 0.029592] time: 0:00:55.842259 \n",
            "[Epoch 8/200] [Batch 11/48] [D loss: 0.239082, acc:  54%] [G loss: 1.587181, adv: 0.427806, recon: 0.032117, id: 0.029771] time: 0:00:55.957214 \n",
            "[Epoch 8/200] [Batch 12/48] [D loss: 0.262302, acc:  46%] [G loss: 1.588958, adv: 0.400580, recon: 0.034603, id: 0.033371] time: 0:00:56.071416 \n",
            "[Epoch 8/200] [Batch 13/48] [D loss: 0.244736, acc:  51%] [G loss: 1.559009, adv: 0.414271, recon: 0.032037, id: 0.030233] time: 0:00:56.184731 \n",
            "[Epoch 8/200] [Batch 14/48] [D loss: 0.231984, acc:  54%] [G loss: 1.575097, adv: 0.429912, recon: 0.031353, id: 0.028473] time: 0:00:56.298500 \n",
            "[Epoch 8/200] [Batch 15/48] [D loss: 0.255291, acc:  48%] [G loss: 1.559286, adv: 0.405751, recon: 0.032735, id: 0.032746] time: 0:00:56.411927 \n",
            "[Epoch 8/200] [Batch 16/48] [D loss: 0.238351, acc:  55%] [G loss: 1.524029, adv: 0.422449, recon: 0.029643, id: 0.028054] time: 0:00:56.523607 \n",
            "[Epoch 8/200] [Batch 17/48] [D loss: 0.238030, acc:  54%] [G loss: 1.559273, adv: 0.425214, recon: 0.031003, id: 0.031710] time: 0:00:56.637112 \n",
            "[Epoch 8/200] [Batch 18/48] [D loss: 0.239506, acc:  54%] [G loss: 1.585575, adv: 0.425118, recon: 0.032333, id: 0.029953] time: 0:00:56.750445 \n",
            "[Epoch 8/200] [Batch 19/48] [D loss: 0.241146, acc:  54%] [G loss: 1.647674, adv: 0.422364, recon: 0.035543, id: 0.031476] time: 0:00:56.864901 \n",
            "[Epoch 8/200] [Batch 20/48] [D loss: 0.248238, acc:  52%] [G loss: 1.536229, adv: 0.408724, recon: 0.031564, id: 0.032309] time: 0:00:56.977098 \n",
            "[Epoch 8/200] [Batch 21/48] [D loss: 0.268979, acc:  44%] [G loss: 1.566262, adv: 0.390512, recon: 0.034560, id: 0.032724] time: 0:00:57.088494 \n",
            "[Epoch 8/200] [Batch 22/48] [D loss: 0.235881, acc:  55%] [G loss: 1.560964, adv: 0.428766, recon: 0.030749, id: 0.028846] time: 0:00:57.203027 \n",
            "[Epoch 8/200] [Batch 23/48] [D loss: 0.240835, acc:  54%] [G loss: 1.597780, adv: 0.425973, recon: 0.032708, id: 0.030091] time: 0:00:57.315073 \n",
            "[Epoch 8/200] [Batch 24/48] [D loss: 0.240941, acc:  51%] [G loss: 1.547808, adv: 0.413773, recon: 0.031556, id: 0.028563] time: 0:00:57.426990 \n",
            "[Epoch 8/200] [Batch 25/48] [D loss: 0.252760, acc:  49%] [G loss: 1.531497, adv: 0.404792, recon: 0.031662, id: 0.030040] time: 0:00:57.538852 \n",
            "[Epoch 8/200] [Batch 26/48] [D loss: 0.256498, acc:  48%] [G loss: 1.543928, adv: 0.398356, recon: 0.032710, id: 0.031640] time: 0:00:57.654350 \n",
            "[Epoch 8/200] [Batch 27/48] [D loss: 0.243532, acc:  53%] [G loss: 1.558186, adv: 0.421055, recon: 0.031318, id: 0.031443] time: 0:00:57.768272 \n",
            "[Epoch 8/200] [Batch 28/48] [D loss: 0.246364, acc:  51%] [G loss: 1.568665, adv: 0.417178, recon: 0.032253, id: 0.029339] time: 0:00:57.879093 \n",
            "[Epoch 8/200] [Batch 29/48] [D loss: 0.239337, acc:  54%] [G loss: 1.570089, adv: 0.427462, recon: 0.031145, id: 0.030038] time: 0:00:57.989473 \n",
            "[Epoch 8/200] [Batch 30/48] [D loss: 0.240881, acc:  54%] [G loss: 1.541929, adv: 0.415898, recon: 0.031052, id: 0.029860] time: 0:00:58.102256 \n",
            "[Epoch 8/200] [Batch 31/48] [D loss: 0.248461, acc:  50%] [G loss: 1.522416, adv: 0.407059, recon: 0.031017, id: 0.029032] time: 0:00:58.215486 \n",
            "[Epoch 8/200] [Batch 32/48] [D loss: 0.237760, acc:  54%] [G loss: 1.576104, adv: 0.419574, recon: 0.032263, id: 0.032227] time: 0:00:58.327467 \n",
            "[Epoch 8/200] [Batch 33/48] [D loss: 0.252326, acc:  50%] [G loss: 1.566856, adv: 0.411280, recon: 0.032615, id: 0.031232] time: 0:00:58.440955 \n",
            "[Epoch 8/200] [Batch 34/48] [D loss: 0.242569, acc:  51%] [G loss: 1.550613, adv: 0.416270, recon: 0.031441, id: 0.028991] time: 0:00:58.556846 \n",
            "[Epoch 8/200] [Batch 35/48] [D loss: 0.245011, acc:  51%] [G loss: 1.527076, adv: 0.411417, recon: 0.030797, id: 0.029567] time: 0:00:58.670546 \n",
            "[Epoch 8/200] [Batch 36/48] [D loss: 0.257505, acc:  49%] [G loss: 1.565265, adv: 0.406323, recon: 0.033032, id: 0.030969] time: 0:00:58.782339 \n",
            "[Epoch 8/200] [Batch 37/48] [D loss: 0.239061, acc:  54%] [G loss: 1.574203, adv: 0.425899, recon: 0.031711, id: 0.030240] time: 0:00:58.892380 \n",
            "[Epoch 8/200] [Batch 38/48] [D loss: 0.252787, acc:  47%] [G loss: 1.520223, adv: 0.400844, recon: 0.031497, id: 0.028304] time: 0:00:59.004423 \n",
            "[Epoch 8/200] [Batch 39/48] [D loss: 0.241904, acc:  54%] [G loss: 1.575462, adv: 0.420052, recon: 0.032399, id: 0.030253] time: 0:00:59.115541 \n",
            "[Epoch 8/200] [Batch 40/48] [D loss: 0.257820, acc:  48%] [G loss: 1.598098, adv: 0.408483, recon: 0.034512, id: 0.030580] time: 0:00:59.227726 \n",
            "[Epoch 8/200] [Batch 41/48] [D loss: 0.237507, acc:  56%] [G loss: 1.558384, adv: 0.416772, recon: 0.032053, id: 0.028502] time: 0:00:59.342740 \n",
            "[Epoch 8/200] [Batch 42/48] [D loss: 0.248640, acc:  49%] [G loss: 1.566279, adv: 0.406900, recon: 0.033196, id: 0.030548] time: 0:00:59.456784 \n",
            "[Epoch 8/200] [Batch 43/48] [D loss: 0.237057, acc:  54%] [G loss: 1.577986, adv: 0.422158, recon: 0.032308, id: 0.030703] time: 0:00:59.569051 \n",
            "[Epoch 8/200] [Batch 44/48] [D loss: 0.240854, acc:  53%] [G loss: 1.552997, adv: 0.418913, recon: 0.031307, id: 0.030740] time: 0:00:59.684721 \n",
            "[Epoch 8/200] [Batch 45/48] [D loss: 0.238586, acc:  53%] [G loss: 1.549756, adv: 0.420573, recon: 0.031024, id: 0.029932] time: 0:00:59.799327 \n",
            "[Epoch 8/200] [Batch 46/48] [D loss: 0.248609, acc:  49%] [G loss: 1.537946, adv: 0.412345, recon: 0.031194, id: 0.029646] time: 0:00:59.908824 \n",
            "[Epoch 8/200] [Batch 47/48] [D loss: 0.245756, acc:  50%] [G loss: 1.515282, adv: 0.411348, recon: 0.030467, id: 0.026934] time: 0:01:00.023628 \n",
            "[Epoch 9/200] [Batch 0/48] [D loss: 0.252172, acc:  49%] [G loss: 1.545327, adv: 0.407256, recon: 0.032055, id: 0.031486] time: 0:01:00.137161 \n",
            "[Epoch 9/200] [Batch 1/48] [D loss: 0.242553, acc:  52%] [G loss: 1.521245, adv: 0.411359, recon: 0.030546, id: 0.031205] time: 0:01:00.251795 \n",
            "[Epoch 9/200] [Batch 2/48] [D loss: 0.252032, acc:  48%] [G loss: 1.515573, adv: 0.403125, recon: 0.031123, id: 0.029660] time: 0:01:00.363402 \n",
            "[Epoch 9/200] [Batch 3/48] [D loss: 0.246457, acc:  51%] [G loss: 1.542318, adv: 0.417332, recon: 0.031090, id: 0.028708] time: 0:01:00.477273 \n",
            "[Epoch 9/200] [Batch 4/48] [D loss: 0.244749, acc:  52%] [G loss: 1.572128, adv: 0.411077, recon: 0.032882, id: 0.030849] time: 0:01:00.591492 \n",
            "[Epoch 9/200] [Batch 5/48] [D loss: 0.242692, acc:  53%] [G loss: 1.565940, adv: 0.423964, recon: 0.031606, id: 0.028417] time: 0:01:00.709033 \n",
            "[Epoch 9/200] [Batch 6/48] [D loss: 0.262485, acc:  45%] [G loss: 1.568872, adv: 0.392998, recon: 0.034459, id: 0.032544] time: 0:01:00.820509 \n",
            "[Epoch 9/200] [Batch 7/48] [D loss: 0.244563, acc:  52%] [G loss: 1.549885, adv: 0.410722, recon: 0.032048, id: 0.030942] time: 0:01:00.933483 \n",
            "[Epoch 9/200] [Batch 8/48] [D loss: 0.239876, acc:  51%] [G loss: 1.588101, adv: 0.411565, recon: 0.033705, id: 0.030662] time: 0:01:01.049804 \n",
            "[Epoch 9/200] [Batch 9/48] [D loss: 0.242709, acc:  52%] [G loss: 1.560677, adv: 0.415586, recon: 0.032227, id: 0.028661] time: 0:01:01.161467 \n",
            "[Epoch 9/200] [Batch 10/48] [D loss: 0.249133, acc:  49%] [G loss: 1.581519, adv: 0.407275, recon: 0.033977, id: 0.029369] time: 0:01:01.273538 \n",
            "[Epoch 9/200] [Batch 11/48] [D loss: 0.237738, acc:  54%] [G loss: 1.565894, adv: 0.423807, recon: 0.031560, id: 0.029119] time: 0:01:01.384040 \n",
            "[Epoch 9/200] [Batch 12/48] [D loss: 0.259654, acc:  47%] [G loss: 1.557451, adv: 0.399595, recon: 0.033267, id: 0.032832] time: 0:01:01.495947 \n",
            "[Epoch 9/200] [Batch 13/48] [D loss: 0.244974, acc:  51%] [G loss: 1.533580, adv: 0.408628, recon: 0.031429, id: 0.029456] time: 0:01:01.607765 \n",
            "[Epoch 9/200] [Batch 14/48] [D loss: 0.230759, acc:  55%] [G loss: 1.545891, adv: 0.425683, recon: 0.030458, id: 0.027897] time: 0:01:01.723313 \n",
            "[Epoch 9/200] [Batch 15/48] [D loss: 0.253453, acc:  49%] [G loss: 1.539847, adv: 0.402097, recon: 0.032229, id: 0.032012] time: 0:01:01.835519 \n",
            "[Epoch 9/200] [Batch 16/48] [D loss: 0.235538, acc:  56%] [G loss: 1.511955, adv: 0.423083, recon: 0.029097, id: 0.027518] time: 0:01:01.950884 \n",
            "[Epoch 9/200] [Batch 17/48] [D loss: 0.238291, acc:  54%] [G loss: 1.536173, adv: 0.418851, recon: 0.030586, id: 0.031044] time: 0:01:02.069438 \n",
            "[Epoch 9/200] [Batch 18/48] [D loss: 0.240122, acc:  54%] [G loss: 1.561851, adv: 0.417644, recon: 0.032011, id: 0.029361] time: 0:01:02.181625 \n",
            "[Epoch 9/200] [Batch 19/48] [D loss: 0.240786, acc:  54%] [G loss: 1.620133, adv: 0.418686, recon: 0.034654, id: 0.030796] time: 0:01:02.294071 \n",
            "[Epoch 9/200] [Batch 20/48] [D loss: 0.246774, acc:  53%] [G loss: 1.512373, adv: 0.406259, recon: 0.030742, id: 0.031922] time: 0:01:02.407432 \n",
            "[Epoch 9/200] [Batch 21/48] [D loss: 0.267628, acc:  44%] [G loss: 1.554229, adv: 0.388417, recon: 0.034276, id: 0.032339] time: 0:01:02.520249 \n",
            "[Epoch 9/200] [Batch 22/48] [D loss: 0.235281, acc:  55%] [G loss: 1.546260, adv: 0.424368, recon: 0.030552, id: 0.028326] time: 0:01:02.630942 \n",
            "[Epoch 9/200] [Batch 23/48] [D loss: 0.240755, acc:  54%] [G loss: 1.587606, adv: 0.420102, recon: 0.032885, id: 0.029702] time: 0:01:02.749483 \n",
            "[Epoch 9/200] [Batch 24/48] [D loss: 0.238782, acc:  52%] [G loss: 1.537514, adv: 0.414382, recon: 0.031115, id: 0.027829] time: 0:01:02.862580 \n",
            "[Epoch 9/200] [Batch 25/48] [D loss: 0.252007, acc:  49%] [G loss: 1.518106, adv: 0.402724, recon: 0.031317, id: 0.029333] time: 0:01:02.978492 \n",
            "[Epoch 9/200] [Batch 26/48] [D loss: 0.255342, acc:  48%] [G loss: 1.530021, adv: 0.395596, recon: 0.032412, id: 0.031048] time: 0:01:03.091270 \n",
            "[Epoch 9/200] [Batch 27/48] [D loss: 0.242913, acc:  53%] [G loss: 1.540197, adv: 0.416789, recon: 0.030991, id: 0.030495] time: 0:01:03.202153 \n",
            "[Epoch 9/200] [Batch 28/48] [D loss: 0.245300, acc:  51%] [G loss: 1.554731, adv: 0.415159, recon: 0.031898, id: 0.028532] time: 0:01:03.313948 \n",
            "[Epoch 9/200] [Batch 29/48] [D loss: 0.237457, acc:  54%] [G loss: 1.559731, adv: 0.425494, recon: 0.030963, id: 0.029265] time: 0:01:03.425515 \n",
            "[Epoch 9/200] [Batch 30/48] [D loss: 0.241254, acc:  54%] [G loss: 1.528939, adv: 0.410172, recon: 0.031075, id: 0.029171] time: 0:01:03.539917 \n",
            "[Epoch 9/200] [Batch 31/48] [D loss: 0.246456, acc:  51%] [G loss: 1.517570, adv: 0.407557, recon: 0.030846, id: 0.028308] time: 0:01:03.652867 \n",
            "[Epoch 9/200] [Batch 32/48] [D loss: 0.239976, acc:  53%] [G loss: 1.564677, adv: 0.412618, recon: 0.032507, id: 0.031467] time: 0:01:03.769627 \n",
            "[Epoch 9/200] [Batch 33/48] [D loss: 0.251324, acc:  51%] [G loss: 1.552778, adv: 0.408207, recon: 0.032360, id: 0.030363] time: 0:01:03.880630 \n",
            "[Epoch 9/200] [Batch 34/48] [D loss: 0.242049, acc:  51%] [G loss: 1.542258, adv: 0.411118, recon: 0.031660, id: 0.028351] time: 0:01:03.992008 \n",
            "[Epoch 9/200] [Batch 35/48] [D loss: 0.242109, acc:  52%] [G loss: 1.523650, adv: 0.411926, recon: 0.030727, id: 0.028826] time: 0:01:04.103302 \n",
            "[Epoch 9/200] [Batch 36/48] [D loss: 0.259618, acc:  48%] [G loss: 1.557288, adv: 0.396935, recon: 0.033666, id: 0.030555] time: 0:01:04.213838 \n",
            "[Epoch 9/200] [Batch 37/48] [D loss: 0.237712, acc:  55%] [G loss: 1.558394, adv: 0.422715, recon: 0.031377, id: 0.029433] time: 0:01:04.324106 \n",
            "[Epoch 9/200] [Batch 38/48] [D loss: 0.251389, acc:  49%] [G loss: 1.506536, adv: 0.397995, recon: 0.031220, id: 0.027765] time: 0:01:04.439862 \n",
            "[Epoch 9/200] [Batch 39/48] [D loss: 0.241855, acc:  54%] [G loss: 1.555382, adv: 0.416306, recon: 0.031892, id: 0.029476] time: 0:01:04.551847 \n",
            "[Epoch 9/200] [Batch 40/48] [D loss: 0.257238, acc:  48%] [G loss: 1.567352, adv: 0.404003, recon: 0.033554, id: 0.029859] time: 0:01:04.663830 \n",
            "[Epoch 9/200] [Batch 41/48] [D loss: 0.239056, acc:  55%] [G loss: 1.519896, adv: 0.409244, recon: 0.030977, id: 0.027794] time: 0:01:04.779742 \n",
            "[Epoch 9/200] [Batch 42/48] [D loss: 0.248783, acc:  50%] [G loss: 1.540774, adv: 0.404242, recon: 0.032324, id: 0.029823] time: 0:01:04.890432 \n",
            "[Epoch 9/200] [Batch 43/48] [D loss: 0.237288, acc:  54%] [G loss: 1.567275, adv: 0.419761, recon: 0.032138, id: 0.029815] time: 0:01:05.002554 \n",
            "[Epoch 9/200] [Batch 44/48] [D loss: 0.241625, acc:  53%] [G loss: 1.542579, adv: 0.411862, recon: 0.031605, id: 0.030161] time: 0:01:05.113540 \n",
            "[Epoch 9/200] [Batch 45/48] [D loss: 0.238532, acc:  53%] [G loss: 1.546887, adv: 0.416049, recon: 0.031472, id: 0.029114] time: 0:01:05.227065 \n",
            "[Epoch 9/200] [Batch 46/48] [D loss: 0.248951, acc:  50%] [G loss: 1.531139, adv: 0.406740, recon: 0.031540, id: 0.029095] time: 0:01:05.340926 \n",
            "[Epoch 9/200] [Batch 47/48] [D loss: 0.245605, acc:  50%] [G loss: 1.500734, adv: 0.405827, recon: 0.030436, id: 0.026179] time: 0:01:05.452633 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npQZcRCRQasBFRAL1f2goWJs4wXujAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 10/200] [Batch 0/48] [D loss: 0.251133, acc:  49%] [G loss: 1.532102, adv: 0.403324, recon: 0.031910, id: 0.030995] time: 0:01:05.882970 \n",
            "[Epoch 10/200] [Batch 1/48] [D loss: 0.242483, acc:  52%] [G loss: 1.508314, adv: 0.406600, recon: 0.030504, id: 0.030400] time: 0:01:05.996545 \n",
            "[Epoch 10/200] [Batch 2/48] [D loss: 0.251970, acc:  48%] [G loss: 1.503835, adv: 0.398506, recon: 0.031136, id: 0.029038] time: 0:01:06.109463 \n",
            "[Epoch 10/200] [Batch 3/48] [D loss: 0.245669, acc:  51%] [G loss: 1.529789, adv: 0.413525, recon: 0.030952, id: 0.027913] time: 0:01:06.223664 \n",
            "[Epoch 10/200] [Batch 4/48] [D loss: 0.243769, acc:  52%] [G loss: 1.557753, adv: 0.407737, recon: 0.032648, id: 0.030083] time: 0:01:06.335013 \n",
            "[Epoch 10/200] [Batch 5/48] [D loss: 0.244316, acc:  53%] [G loss: 1.550145, adv: 0.418429, recon: 0.031465, id: 0.027668] time: 0:01:06.447546 \n",
            "[Epoch 10/200] [Batch 6/48] [D loss: 0.260555, acc:  46%] [G loss: 1.544139, adv: 0.391896, recon: 0.033496, id: 0.031571] time: 0:01:06.558664 \n",
            "[Epoch 10/200] [Batch 7/48] [D loss: 0.246251, acc:  51%] [G loss: 1.509089, adv: 0.403991, recon: 0.030801, id: 0.029746] time: 0:01:06.670968 \n",
            "[Epoch 10/200] [Batch 8/48] [D loss: 0.239517, acc:  52%] [G loss: 1.536724, adv: 0.405599, recon: 0.031906, id: 0.029575] time: 0:01:06.781094 \n",
            "[Epoch 10/200] [Batch 9/48] [D loss: 0.243009, acc:  52%] [G loss: 1.533597, adv: 0.409775, recon: 0.031579, id: 0.027834] time: 0:01:06.894506 \n",
            "[Epoch 10/200] [Batch 10/48] [D loss: 0.252398, acc:  48%] [G loss: 1.594051, adv: 0.397096, recon: 0.035688, id: 0.029345] time: 0:01:07.008114 \n",
            "[Epoch 10/200] [Batch 11/48] [D loss: 0.233980, acc:  56%] [G loss: 1.601428, adv: 0.426777, recon: 0.033181, id: 0.028668] time: 0:01:07.121120 \n",
            "[Epoch 10/200] [Batch 12/48] [D loss: 0.259645, acc:  47%] [G loss: 1.569881, adv: 0.394038, recon: 0.034535, id: 0.032372] time: 0:01:07.231191 \n",
            "[Epoch 10/200] [Batch 13/48] [D loss: 0.243571, acc:  51%] [G loss: 1.518540, adv: 0.403493, recon: 0.031326, id: 0.028786] time: 0:01:07.342045 \n",
            "[Epoch 10/200] [Batch 14/48] [D loss: 0.229591, acc:  55%] [G loss: 1.539768, adv: 0.422796, recon: 0.030561, id: 0.027207] time: 0:01:07.454882 \n",
            "[Epoch 10/200] [Batch 15/48] [D loss: 0.251964, acc:  49%] [G loss: 1.531355, adv: 0.399982, recon: 0.032136, id: 0.031440] time: 0:01:07.565989 \n",
            "[Epoch 10/200] [Batch 16/48] [D loss: 0.236556, acc:  55%] [G loss: 1.499472, adv: 0.418426, recon: 0.029057, id: 0.026776] time: 0:01:07.677435 \n",
            "[Epoch 10/200] [Batch 17/48] [D loss: 0.239762, acc:  54%] [G loss: 1.514066, adv: 0.411950, recon: 0.030299, id: 0.030235] time: 0:01:07.789638 \n",
            "[Epoch 10/200] [Batch 18/48] [D loss: 0.239771, acc:  54%] [G loss: 1.545802, adv: 0.413885, recon: 0.031711, id: 0.028449] time: 0:01:07.903802 \n",
            "[Epoch 10/200] [Batch 19/48] [D loss: 0.240523, acc:  54%] [G loss: 1.607430, adv: 0.414524, recon: 0.034578, id: 0.030209] time: 0:01:08.015442 \n",
            "[Epoch 10/200] [Batch 20/48] [D loss: 0.247805, acc:  52%] [G loss: 1.494944, adv: 0.401540, recon: 0.030463, id: 0.030826] time: 0:01:08.128181 \n",
            "[Epoch 10/200] [Batch 21/48] [D loss: 0.267732, acc:  44%] [G loss: 1.541662, adv: 0.383972, recon: 0.034215, id: 0.031545] time: 0:01:08.240814 \n",
            "[Epoch 10/200] [Batch 22/48] [D loss: 0.235510, acc:  55%] [G loss: 1.530131, adv: 0.419674, recon: 0.030321, id: 0.027874] time: 0:01:08.351950 \n",
            "[Epoch 10/200] [Batch 23/48] [D loss: 0.242175, acc:  53%] [G loss: 1.569519, adv: 0.412184, recon: 0.032902, id: 0.028739] time: 0:01:08.461921 \n",
            "[Epoch 10/200] [Batch 24/48] [D loss: 0.239076, acc:  53%] [G loss: 1.527621, adv: 0.408589, recon: 0.031310, id: 0.027176] time: 0:01:08.574794 \n",
            "[Epoch 10/200] [Batch 25/48] [D loss: 0.249059, acc:  51%] [G loss: 1.515174, adv: 0.403323, recon: 0.031261, id: 0.028444] time: 0:01:08.688094 \n",
            "[Epoch 10/200] [Batch 26/48] [D loss: 0.256392, acc:  47%] [G loss: 1.519578, adv: 0.385308, recon: 0.033008, id: 0.030694] time: 0:01:08.802081 \n",
            "[Epoch 10/200] [Batch 27/48] [D loss: 0.239814, acc:  54%] [G loss: 1.547711, adv: 0.412982, recon: 0.031879, id: 0.030037] time: 0:01:08.912770 \n",
            "[Epoch 10/200] [Batch 28/48] [D loss: 0.245878, acc:  51%] [G loss: 1.568517, adv: 0.410744, recon: 0.033112, id: 0.028007] time: 0:01:09.024893 \n",
            "[Epoch 10/200] [Batch 29/48] [D loss: 0.232748, acc:  57%] [G loss: 1.567136, adv: 0.430540, recon: 0.031012, id: 0.028331] time: 0:01:09.138401 \n",
            "[Epoch 10/200] [Batch 30/48] [D loss: 0.242702, acc:  53%] [G loss: 1.523240, adv: 0.402253, recon: 0.031671, id: 0.028587] time: 0:01:09.253113 \n",
            "[Epoch 10/200] [Batch 31/48] [D loss: 0.245906, acc:  51%] [G loss: 1.502128, adv: 0.401051, recon: 0.030868, id: 0.027441] time: 0:01:09.363787 \n",
            "[Epoch 10/200] [Batch 32/48] [D loss: 0.238394, acc:  54%] [G loss: 1.556661, adv: 0.409400, recon: 0.032549, id: 0.030566] time: 0:01:09.477503 \n",
            "[Epoch 10/200] [Batch 33/48] [D loss: 0.249064, acc:  51%] [G loss: 1.541325, adv: 0.408322, recon: 0.031901, id: 0.029839] time: 0:01:09.589206 \n",
            "[Epoch 10/200] [Batch 34/48] [D loss: 0.241084, acc:  52%] [G loss: 1.525738, adv: 0.407518, recon: 0.031322, id: 0.027782] time: 0:01:09.701502 \n",
            "[Epoch 10/200] [Batch 35/48] [D loss: 0.242493, acc:  52%] [G loss: 1.502280, adv: 0.407039, recon: 0.030261, id: 0.027994] time: 0:01:09.812951 \n",
            "[Epoch 10/200] [Batch 36/48] [D loss: 0.258371, acc:  48%] [G loss: 1.532559, adv: 0.393455, recon: 0.032904, id: 0.029938] time: 0:01:09.926843 \n",
            "[Epoch 10/200] [Batch 37/48] [D loss: 0.237989, acc:  54%] [G loss: 1.536716, adv: 0.418118, recon: 0.030845, id: 0.028871] time: 0:01:10.036920 \n",
            "[Epoch 10/200] [Batch 38/48] [D loss: 0.251381, acc:  48%] [G loss: 1.482966, adv: 0.392258, recon: 0.030734, id: 0.027236] time: 0:01:10.147185 \n",
            "[Epoch 10/200] [Batch 39/48] [D loss: 0.243900, acc:  53%] [G loss: 1.524799, adv: 0.408039, recon: 0.031289, id: 0.028900] time: 0:01:10.257155 \n",
            "[Epoch 10/200] [Batch 40/48] [D loss: 0.256651, acc:  49%] [G loss: 1.535755, adv: 0.398695, recon: 0.032628, id: 0.029227] time: 0:01:10.368994 \n",
            "[Epoch 10/200] [Batch 41/48] [D loss: 0.239766, acc:  55%] [G loss: 1.496593, adv: 0.403657, recon: 0.030473, id: 0.027250] time: 0:01:10.482040 \n",
            "[Epoch 10/200] [Batch 42/48] [D loss: 0.247771, acc:  51%] [G loss: 1.510960, adv: 0.399947, recon: 0.031386, id: 0.029167] time: 0:01:10.592918 \n",
            "[Epoch 10/200] [Batch 43/48] [D loss: 0.238679, acc:  54%] [G loss: 1.536572, adv: 0.411460, recon: 0.031519, id: 0.029374] time: 0:01:10.704288 \n",
            "[Epoch 10/200] [Batch 44/48] [D loss: 0.240608, acc:  53%] [G loss: 1.540770, adv: 0.407083, recon: 0.032080, id: 0.029811] time: 0:01:10.815526 \n",
            "[Epoch 10/200] [Batch 45/48] [D loss: 0.235582, acc:  55%] [G loss: 1.551465, adv: 0.417603, recon: 0.031698, id: 0.028474] time: 0:01:10.928991 \n",
            "[Epoch 10/200] [Batch 46/48] [D loss: 0.251381, acc:  48%] [G loss: 1.531837, adv: 0.397717, recon: 0.032537, id: 0.028753] time: 0:01:11.041824 \n",
            "[Epoch 10/200] [Batch 47/48] [D loss: 0.241978, acc:  52%] [G loss: 1.495875, adv: 0.406390, recon: 0.030289, id: 0.025589] time: 0:01:11.155150 \n",
            "[Epoch 11/200] [Batch 0/48] [D loss: 0.252441, acc:  49%] [G loss: 1.546962, adv: 0.399202, recon: 0.033088, id: 0.030913] time: 0:01:11.266715 \n",
            "[Epoch 11/200] [Batch 1/48] [D loss: 0.239272, acc:  53%] [G loss: 1.501421, adv: 0.407611, recon: 0.030202, id: 0.029817] time: 0:01:11.378039 \n",
            "[Epoch 11/200] [Batch 2/48] [D loss: 0.257167, acc:  47%] [G loss: 1.507680, adv: 0.383279, recon: 0.032888, id: 0.028825] time: 0:01:11.487678 \n",
            "[Epoch 11/200] [Batch 3/48] [D loss: 0.238698, acc:  55%] [G loss: 1.537063, adv: 0.419355, recon: 0.030898, id: 0.027331] time: 0:01:11.598627 \n",
            "[Epoch 11/200] [Batch 4/48] [D loss: 0.245719, acc:  52%] [G loss: 1.557028, adv: 0.398293, recon: 0.033597, id: 0.029997] time: 0:01:11.709571 \n",
            "[Epoch 11/200] [Batch 5/48] [D loss: 0.241841, acc:  53%] [G loss: 1.539730, adv: 0.415086, recon: 0.031403, id: 0.027239] time: 0:01:11.819402 \n",
            "[Epoch 11/200] [Batch 6/48] [D loss: 0.257869, acc:  47%] [G loss: 1.531745, adv: 0.389493, recon: 0.033222, id: 0.030823] time: 0:01:11.937180 \n",
            "[Epoch 11/200] [Batch 7/48] [D loss: 0.246980, acc:  51%] [G loss: 1.495783, adv: 0.396547, recon: 0.030977, id: 0.029559] time: 0:01:12.052578 \n",
            "[Epoch 11/200] [Batch 8/48] [D loss: 0.238120, acc:  53%] [G loss: 1.522408, adv: 0.401527, recon: 0.031692, id: 0.029157] time: 0:01:12.164538 \n",
            "[Epoch 11/200] [Batch 9/48] [D loss: 0.244687, acc:  52%] [G loss: 1.504922, adv: 0.402740, recon: 0.030922, id: 0.027391] time: 0:01:12.277100 \n",
            "[Epoch 11/200] [Batch 10/48] [D loss: 0.249570, acc:  49%] [G loss: 1.528209, adv: 0.396564, recon: 0.032615, id: 0.028132] time: 0:01:12.386635 \n",
            "[Epoch 11/200] [Batch 11/48] [D loss: 0.239267, acc:  54%] [G loss: 1.524757, adv: 0.411565, recon: 0.030916, id: 0.028353] time: 0:01:12.497416 \n",
            "[Epoch 11/200] [Batch 12/48] [D loss: 0.259245, acc:  48%] [G loss: 1.521943, adv: 0.391146, recon: 0.032555, id: 0.031516] time: 0:01:12.612447 \n",
            "[Epoch 11/200] [Batch 13/48] [D loss: 0.244833, acc:  51%] [G loss: 1.506665, adv: 0.398932, recon: 0.031269, id: 0.028679] time: 0:01:12.723701 \n",
            "[Epoch 11/200] [Batch 14/48] [D loss: 0.231917, acc:  55%] [G loss: 1.529310, adv: 0.415004, recon: 0.030881, id: 0.027180] time: 0:01:12.837457 \n",
            "[Epoch 11/200] [Batch 15/48] [D loss: 0.250614, acc:  50%] [G loss: 1.524172, adv: 0.398271, recon: 0.032028, id: 0.031029] time: 0:01:12.957466 \n",
            "[Epoch 11/200] [Batch 16/48] [D loss: 0.237946, acc:  55%] [G loss: 1.489380, adv: 0.411914, recon: 0.029279, id: 0.026660] time: 0:01:13.068168 \n",
            "[Epoch 11/200] [Batch 17/48] [D loss: 0.239782, acc:  54%] [G loss: 1.493716, adv: 0.405942, recon: 0.029989, id: 0.029745] time: 0:01:13.184289 \n",
            "[Epoch 11/200] [Batch 18/48] [D loss: 0.242661, acc:  53%] [G loss: 1.515525, adv: 0.403453, recon: 0.031309, id: 0.028309] time: 0:01:13.294414 \n",
            "[Epoch 11/200] [Batch 19/48] [D loss: 0.238063, acc:  55%] [G loss: 1.561989, adv: 0.414613, recon: 0.032426, id: 0.029229] time: 0:01:13.407156 \n",
            "[Epoch 11/200] [Batch 20/48] [D loss: 0.250860, acc:  51%] [G loss: 1.456625, adv: 0.392158, recon: 0.029549, id: 0.030896] time: 0:01:13.518307 \n",
            "[Epoch 11/200] [Batch 21/48] [D loss: 0.267825, acc:  44%] [G loss: 1.505724, adv: 0.379286, recon: 0.032976, id: 0.031227] time: 0:01:13.628905 \n",
            "[Epoch 11/200] [Batch 22/48] [D loss: 0.235922, acc:  55%] [G loss: 1.513506, adv: 0.413097, recon: 0.030219, id: 0.027613] time: 0:01:13.739950 \n",
            "[Epoch 11/200] [Batch 23/48] [D loss: 0.245090, acc:  53%] [G loss: 1.571499, adv: 0.400743, recon: 0.034201, id: 0.028605] time: 0:01:13.855690 \n",
            "[Epoch 11/200] [Batch 24/48] [D loss: 0.234778, acc:  55%] [G loss: 1.513256, adv: 0.411192, recon: 0.030462, id: 0.026733] time: 0:01:13.977664 \n",
            "[Epoch 11/200] [Batch 25/48] [D loss: 0.255556, acc:  48%] [G loss: 1.494340, adv: 0.388234, recon: 0.031733, id: 0.028261] time: 0:01:14.089089 \n",
            "[Epoch 11/200] [Batch 26/48] [D loss: 0.252603, acc:  49%] [G loss: 1.499630, adv: 0.386150, recon: 0.032069, id: 0.030223] time: 0:01:14.206719 \n",
            "[Epoch 11/200] [Batch 27/48] [D loss: 0.243719, acc:  52%] [G loss: 1.527916, adv: 0.402658, recon: 0.031939, id: 0.029718] time: 0:01:14.317824 \n",
            "[Epoch 11/200] [Batch 28/48] [D loss: 0.240090, acc:  54%] [G loss: 1.545587, adv: 0.414480, recon: 0.031776, id: 0.027320] time: 0:01:14.429879 \n",
            "[Epoch 11/200] [Batch 29/48] [D loss: 0.242190, acc:  53%] [G loss: 1.575070, adv: 0.408956, recon: 0.033473, id: 0.028985] time: 0:01:14.541652 \n",
            "[Epoch 11/200] [Batch 30/48] [D loss: 0.234656, acc:  57%] [G loss: 1.542145, adv: 0.415785, recon: 0.031421, id: 0.028413] time: 0:01:14.656520 \n",
            "[Epoch 11/200] [Batch 31/48] [D loss: 0.256478, acc:  48%] [G loss: 1.515539, adv: 0.381450, recon: 0.033402, id: 0.028909] time: 0:01:14.767810 \n",
            "[Epoch 11/200] [Batch 32/48] [D loss: 0.232924, acc:  57%] [G loss: 1.563247, adv: 0.415000, recon: 0.032429, id: 0.030633] time: 0:01:14.884209 \n",
            "[Epoch 11/200] [Batch 33/48] [D loss: 0.252937, acc:  50%] [G loss: 1.532886, adv: 0.396831, recon: 0.032630, id: 0.029319] time: 0:01:15.001514 \n",
            "[Epoch 11/200] [Batch 34/48] [D loss: 0.236812, acc:  54%] [G loss: 1.509712, adv: 0.410529, recon: 0.030349, id: 0.027489] time: 0:01:15.113153 \n",
            "[Epoch 11/200] [Batch 35/48] [D loss: 0.248042, acc:  50%] [G loss: 1.487487, adv: 0.393250, recon: 0.030922, id: 0.027662] time: 0:01:15.225220 \n",
            "[Epoch 11/200] [Batch 36/48] [D loss: 0.254596, acc:  50%] [G loss: 1.519032, adv: 0.395081, recon: 0.032167, id: 0.029979] time: 0:01:15.335281 \n",
            "[Epoch 11/200] [Batch 37/48] [D loss: 0.241793, acc:  53%] [G loss: 1.525728, adv: 0.407986, recon: 0.031319, id: 0.028816] time: 0:01:15.449066 \n",
            "[Epoch 11/200] [Batch 38/48] [D loss: 0.250008, acc:  49%] [G loss: 1.469062, adv: 0.390214, recon: 0.030362, id: 0.026590] time: 0:01:15.561292 \n",
            "[Epoch 11/200] [Batch 39/48] [D loss: 0.249041, acc:  52%] [G loss: 1.508891, adv: 0.394517, recon: 0.031875, id: 0.028698] time: 0:01:15.674718 \n",
            "[Epoch 11/200] [Batch 40/48] [D loss: 0.250780, acc:  51%] [G loss: 1.519208, adv: 0.404181, recon: 0.031373, id: 0.029055] time: 0:01:15.785726 \n",
            "[Epoch 11/200] [Batch 41/48] [D loss: 0.250654, acc:  50%] [G loss: 1.507067, adv: 0.382797, recon: 0.033039, id: 0.027026] time: 0:01:15.900502 \n",
            "[Epoch 11/200] [Batch 42/48] [D loss: 0.234507, acc:  56%] [G loss: 1.547359, adv: 0.423394, recon: 0.031084, id: 0.028747] time: 0:01:16.019001 \n",
            "[Epoch 11/200] [Batch 43/48] [D loss: 0.246031, acc:  51%] [G loss: 1.549617, adv: 0.393262, recon: 0.033925, id: 0.029632] time: 0:01:16.129640 \n",
            "[Epoch 11/200] [Batch 44/48] [D loss: 0.233012, acc:  56%] [G loss: 1.539184, adv: 0.409816, recon: 0.031909, id: 0.028897] time: 0:01:16.242693 \n",
            "[Epoch 11/200] [Batch 45/48] [D loss: 0.240985, acc:  53%] [G loss: 1.529197, adv: 0.400911, recon: 0.032245, id: 0.029262] time: 0:01:16.356341 \n",
            "[Epoch 11/200] [Batch 46/48] [D loss: 0.246730, acc:  51%] [G loss: 1.498793, adv: 0.400264, recon: 0.030795, id: 0.027648] time: 0:01:16.467847 \n",
            "[Epoch 11/200] [Batch 47/48] [D loss: 0.246828, acc:  50%] [G loss: 1.458857, adv: 0.391640, recon: 0.029945, id: 0.025510] time: 0:01:16.581158 \n",
            "[Epoch 12/200] [Batch 0/48] [D loss: 0.252772, acc:  50%] [G loss: 1.504242, adv: 0.392652, recon: 0.031733, id: 0.030203] time: 0:01:16.691394 \n",
            "[Epoch 12/200] [Batch 1/48] [D loss: 0.244407, acc:  52%] [G loss: 1.473001, adv: 0.394138, recon: 0.030171, id: 0.029566] time: 0:01:16.804284 \n",
            "[Epoch 12/200] [Batch 2/48] [D loss: 0.252790, acc:  49%] [G loss: 1.472701, adv: 0.387470, recon: 0.030840, id: 0.028446] time: 0:01:16.914713 \n",
            "[Epoch 12/200] [Batch 3/48] [D loss: 0.244677, acc:  52%] [G loss: 1.499099, adv: 0.405211, recon: 0.030403, id: 0.027612] time: 0:01:17.032015 \n",
            "[Epoch 12/200] [Batch 4/48] [D loss: 0.247610, acc:  52%] [G loss: 1.532276, adv: 0.393165, recon: 0.032965, id: 0.029364] time: 0:01:17.148100 \n",
            "[Epoch 12/200] [Batch 5/48] [D loss: 0.243503, acc:  53%] [G loss: 1.516472, adv: 0.409621, recon: 0.030834, id: 0.027319] time: 0:01:17.261898 \n",
            "[Epoch 12/200] [Batch 6/48] [D loss: 0.260049, acc:  46%] [G loss: 1.523574, adv: 0.382294, recon: 0.033563, id: 0.030616] time: 0:01:17.373294 \n",
            "[Epoch 12/200] [Batch 7/48] [D loss: 0.248286, acc:  51%] [G loss: 1.502123, adv: 0.392645, recon: 0.031736, id: 0.029625] time: 0:01:17.483774 \n",
            "[Epoch 12/200] [Batch 8/48] [D loss: 0.240038, acc:  52%] [G loss: 1.529148, adv: 0.395533, recon: 0.032636, id: 0.029811] time: 0:01:17.594891 \n",
            "[Epoch 12/200] [Batch 9/48] [D loss: 0.244975, acc:  52%] [G loss: 1.502665, adv: 0.398394, recon: 0.031308, id: 0.027310] time: 0:01:17.706741 \n",
            "[Epoch 12/200] [Batch 10/48] [D loss: 0.253632, acc:  49%] [G loss: 1.519917, adv: 0.386091, recon: 0.033272, id: 0.028084] time: 0:01:17.818148 \n",
            "[Epoch 12/200] [Batch 11/48] [D loss: 0.237775, acc:  55%] [G loss: 1.518134, adv: 0.411106, recon: 0.030715, id: 0.028056] time: 0:01:17.930319 \n",
            "[Epoch 12/200] [Batch 12/48] [D loss: 0.258749, acc:  48%] [G loss: 1.508938, adv: 0.387700, recon: 0.032296, id: 0.031749] time: 0:01:18.049657 \n",
            "[Epoch 12/200] [Batch 13/48] [D loss: 0.247336, acc:  50%] [G loss: 1.488823, adv: 0.389965, recon: 0.031318, id: 0.028461] time: 0:01:18.159693 \n",
            "[Epoch 12/200] [Batch 14/48] [D loss: 0.232348, acc:  55%] [G loss: 1.506581, adv: 0.408361, recon: 0.030482, id: 0.026982] time: 0:01:18.272331 \n",
            "[Epoch 12/200] [Batch 15/48] [D loss: 0.251497, acc:  50%] [G loss: 1.505319, adv: 0.389604, recon: 0.031999, id: 0.030968] time: 0:01:18.383754 \n",
            "[Epoch 12/200] [Batch 16/48] [D loss: 0.239054, acc:  56%] [G loss: 1.470795, adv: 0.403719, recon: 0.029223, id: 0.026405] time: 0:01:18.495629 \n",
            "[Epoch 12/200] [Batch 17/48] [D loss: 0.239955, acc:  55%] [G loss: 1.480711, adv: 0.402216, recon: 0.029772, id: 0.029709] time: 0:01:18.605808 \n",
            "[Epoch 12/200] [Batch 18/48] [D loss: 0.246208, acc:  52%] [G loss: 1.496296, adv: 0.392336, recon: 0.031505, id: 0.028460] time: 0:01:18.715311 \n",
            "[Epoch 12/200] [Batch 19/48] [D loss: 0.238322, acc:  56%] [G loss: 1.551180, adv: 0.407509, recon: 0.032662, id: 0.028957] time: 0:01:18.829014 \n",
            "[Epoch 12/200] [Batch 20/48] [D loss: 0.252138, acc:  51%] [G loss: 1.438301, adv: 0.382884, recon: 0.029611, id: 0.030880] time: 0:01:18.943470 \n",
            "[Epoch 12/200] [Batch 21/48] [D loss: 0.267294, acc:  45%] [G loss: 1.486938, adv: 0.374355, recon: 0.032609, id: 0.031167] time: 0:01:19.056727 \n",
            "[Epoch 12/200] [Batch 22/48] [D loss: 0.236974, acc:  55%] [G loss: 1.505262, adv: 0.407111, recon: 0.030448, id: 0.027708] time: 0:01:19.168321 \n",
            "[Epoch 12/200] [Batch 23/48] [D loss: 0.243281, acc:  53%] [G loss: 1.554823, adv: 0.401638, recon: 0.033345, id: 0.028540] time: 0:01:19.279549 \n",
            "[Epoch 12/200] [Batch 24/48] [D loss: 0.239444, acc:  53%] [G loss: 1.494651, adv: 0.399241, recon: 0.030730, id: 0.027013] time: 0:01:19.390565 \n",
            "[Epoch 12/200] [Batch 25/48] [D loss: 0.250167, acc:  50%] [G loss: 1.487724, adv: 0.394061, recon: 0.030930, id: 0.028119] time: 0:01:19.502794 \n",
            "[Epoch 12/200] [Batch 26/48] [D loss: 0.257633, acc:  47%] [G loss: 1.485161, adv: 0.374248, recon: 0.032526, id: 0.030188] time: 0:01:19.612822 \n",
            "[Epoch 12/200] [Batch 27/48] [D loss: 0.238863, acc:  55%] [G loss: 1.517360, adv: 0.404330, recon: 0.031368, id: 0.029653] time: 0:01:19.727937 \n",
            "[Epoch 12/200] [Batch 28/48] [D loss: 0.247313, acc:  52%] [G loss: 1.551810, adv: 0.397202, recon: 0.033714, id: 0.028157] time: 0:01:19.841569 \n",
            "[Epoch 12/200] [Batch 29/48] [D loss: 0.231605, acc:  58%] [G loss: 1.567831, adv: 0.422393, recon: 0.031987, id: 0.028237] time: 0:01:19.954116 \n",
            "[Epoch 12/200] [Batch 30/48] [D loss: 0.246448, acc:  53%] [G loss: 1.541535, adv: 0.384522, recon: 0.034382, id: 0.029078] time: 0:01:20.063061 \n",
            "[Epoch 12/200] [Batch 31/48] [D loss: 0.241072, acc:  54%] [G loss: 1.519351, adv: 0.400093, recon: 0.032001, id: 0.027083] time: 0:01:20.174312 \n",
            "[Epoch 12/200] [Batch 32/48] [D loss: 0.237582, acc:  55%] [G loss: 1.547406, adv: 0.398819, recon: 0.033257, id: 0.030385] time: 0:01:20.283981 \n",
            "[Epoch 12/200] [Batch 33/48] [D loss: 0.247971, acc:  52%] [G loss: 1.526983, adv: 0.399406, recon: 0.032176, id: 0.029589] time: 0:01:20.394005 \n",
            "[Epoch 12/200] [Batch 34/48] [D loss: 0.239512, acc:  53%] [G loss: 1.497882, adv: 0.402461, recon: 0.030603, id: 0.027627] time: 0:01:20.506470 \n",
            "[Epoch 12/200] [Batch 35/48] [D loss: 0.247440, acc:  51%] [G loss: 1.463372, adv: 0.389731, recon: 0.030138, id: 0.027593] time: 0:01:20.620431 \n",
            "[Epoch 12/200] [Batch 36/48] [D loss: 0.255921, acc:  50%] [G loss: 1.499464, adv: 0.385808, recon: 0.032164, id: 0.029572] time: 0:01:20.732949 \n",
            "[Epoch 12/200] [Batch 37/48] [D loss: 0.242289, acc:  53%] [G loss: 1.516091, adv: 0.400732, recon: 0.031599, id: 0.028904] time: 0:01:20.846112 \n",
            "[Epoch 12/200] [Batch 38/48] [D loss: 0.247833, acc:  50%] [G loss: 1.463252, adv: 0.387673, recon: 0.030409, id: 0.026366] time: 0:01:20.959341 \n",
            "[Epoch 12/200] [Batch 39/48] [D loss: 0.249373, acc:  52%] [G loss: 1.500710, adv: 0.388243, recon: 0.032150, id: 0.028790] time: 0:01:21.071227 \n",
            "[Epoch 12/200] [Batch 40/48] [D loss: 0.250589, acc:  51%] [G loss: 1.503406, adv: 0.397786, recon: 0.031289, id: 0.029017] time: 0:01:21.182316 \n",
            "[Epoch 12/200] [Batch 41/48] [D loss: 0.248465, acc:  51%] [G loss: 1.478397, adv: 0.378901, recon: 0.032071, id: 0.027032] time: 0:01:21.291690 \n",
            "[Epoch 12/200] [Batch 42/48] [D loss: 0.235561, acc:  56%] [G loss: 1.509731, adv: 0.414863, recon: 0.030081, id: 0.028665] time: 0:01:21.406380 \n",
            "[Epoch 12/200] [Batch 43/48] [D loss: 0.247307, acc:  50%] [G loss: 1.523793, adv: 0.388947, recon: 0.033128, id: 0.029288] time: 0:01:21.518931 \n",
            "[Epoch 12/200] [Batch 44/48] [D loss: 0.233236, acc:  57%] [G loss: 1.511118, adv: 0.406586, recon: 0.030904, id: 0.028916] time: 0:01:21.632613 \n",
            "[Epoch 12/200] [Batch 45/48] [D loss: 0.240805, acc:  53%] [G loss: 1.510784, adv: 0.397358, recon: 0.031743, id: 0.029094] time: 0:01:21.744390 \n",
            "[Epoch 12/200] [Batch 46/48] [D loss: 0.247351, acc:  51%] [G loss: 1.481232, adv: 0.394622, recon: 0.030528, id: 0.027612] time: 0:01:21.858344 \n",
            "[Epoch 12/200] [Batch 47/48] [D loss: 0.245587, acc:  50%] [G loss: 1.446606, adv: 0.390760, recon: 0.029476, id: 0.025632] time: 0:01:21.970011 \n",
            "[Epoch 13/200] [Batch 0/48] [D loss: 0.252293, acc:  50%] [G loss: 1.488955, adv: 0.388480, recon: 0.031454, id: 0.030018] time: 0:01:22.082138 \n",
            "[Epoch 13/200] [Batch 1/48] [D loss: 0.244328, acc:  52%] [G loss: 1.464884, adv: 0.390269, recon: 0.030197, id: 0.029629] time: 0:01:22.194300 \n",
            "[Epoch 13/200] [Batch 2/48] [D loss: 0.250515, acc:  50%] [G loss: 1.455280, adv: 0.385747, recon: 0.030214, id: 0.028415] time: 0:01:22.304469 \n",
            "[Epoch 13/200] [Batch 3/48] [D loss: 0.246712, acc:  51%] [G loss: 1.477178, adv: 0.393935, recon: 0.030466, id: 0.027746] time: 0:01:22.416004 \n",
            "[Epoch 13/200] [Batch 4/48] [D loss: 0.245274, acc:  53%] [G loss: 1.514513, adv: 0.390715, recon: 0.032402, id: 0.029241] time: 0:01:22.525827 \n",
            "[Epoch 13/200] [Batch 5/48] [D loss: 0.245386, acc:  53%] [G loss: 1.492231, adv: 0.400762, recon: 0.030536, id: 0.027287] time: 0:01:22.636250 \n",
            "[Epoch 13/200] [Batch 6/48] [D loss: 0.257176, acc:  48%] [G loss: 1.496990, adv: 0.382108, recon: 0.032352, id: 0.030300] time: 0:01:22.750265 \n",
            "[Epoch 13/200] [Batch 7/48] [D loss: 0.250654, acc:  50%] [G loss: 1.467730, adv: 0.383968, recon: 0.030931, id: 0.029421] time: 0:01:22.862490 \n",
            "[Epoch 13/200] [Batch 8/48] [D loss: 0.238832, acc:  53%] [G loss: 1.496272, adv: 0.391941, recon: 0.031442, id: 0.029443] time: 0:01:22.973246 \n",
            "[Epoch 13/200] [Batch 9/48] [D loss: 0.246513, acc:  51%] [G loss: 1.492821, adv: 0.391971, recon: 0.031501, id: 0.027285] time: 0:01:23.086867 \n",
            "[Epoch 13/200] [Batch 10/48] [D loss: 0.253171, acc:  49%] [G loss: 1.521642, adv: 0.382427, recon: 0.033763, id: 0.028237] time: 0:01:23.200121 \n",
            "[Epoch 13/200] [Batch 11/48] [D loss: 0.237284, acc:  56%] [G loss: 1.524658, adv: 0.406728, recon: 0.031531, id: 0.028386] time: 0:01:23.318646 \n",
            "[Epoch 13/200] [Batch 12/48] [D loss: 0.255973, acc:  49%] [G loss: 1.504560, adv: 0.387290, recon: 0.032196, id: 0.031529] time: 0:01:23.430760 \n",
            "[Epoch 13/200] [Batch 13/48] [D loss: 0.246967, acc:  50%] [G loss: 1.474606, adv: 0.386160, recon: 0.031043, id: 0.028336] time: 0:01:23.544679 \n",
            "[Epoch 13/200] [Batch 14/48] [D loss: 0.231174, acc:  55%] [G loss: 1.487287, adv: 0.404695, recon: 0.029969, id: 0.026790] time: 0:01:23.658577 \n",
            "[Epoch 13/200] [Batch 15/48] [D loss: 0.250616, acc:  50%] [G loss: 1.494880, adv: 0.385702, recon: 0.031917, id: 0.030957] time: 0:01:23.769928 \n",
            "[Epoch 13/200] [Batch 16/48] [D loss: 0.236803, acc:  57%] [G loss: 1.458202, adv: 0.402264, recon: 0.028825, id: 0.026175] time: 0:01:23.880289 \n",
            "[Epoch 13/200] [Batch 17/48] [D loss: 0.241831, acc:  54%] [G loss: 1.473968, adv: 0.391246, recon: 0.030560, id: 0.029826] time: 0:01:23.990667 \n",
            "[Epoch 13/200] [Batch 18/48] [D loss: 0.240692, acc:  54%] [G loss: 1.502362, adv: 0.395271, recon: 0.031629, id: 0.027945] time: 0:01:24.101773 \n",
            "[Epoch 13/200] [Batch 19/48] [D loss: 0.237181, acc:  56%] [G loss: 1.578862, adv: 0.403884, recon: 0.034384, id: 0.029920] time: 0:01:24.215419 \n",
            "[Epoch 13/200] [Batch 20/48] [D loss: 0.246402, acc:  53%] [G loss: 1.459644, adv: 0.388439, recon: 0.030225, id: 0.030260] time: 0:01:24.325577 \n",
            "[Epoch 13/200] [Batch 21/48] [D loss: 0.267677, acc:  45%] [G loss: 1.491078, adv: 0.367451, recon: 0.033538, id: 0.031228] time: 0:01:24.441663 \n",
            "[Epoch 13/200] [Batch 22/48] [D loss: 0.234004, acc:  56%] [G loss: 1.493099, adv: 0.405249, recon: 0.030110, id: 0.027775] time: 0:01:24.555453 \n",
            "[Epoch 13/200] [Batch 23/48] [D loss: 0.239440, acc:  55%] [G loss: 1.542231, adv: 0.402021, recon: 0.032764, id: 0.028199] time: 0:01:24.666520 \n",
            "[Epoch 13/200] [Batch 24/48] [D loss: 0.236851, acc:  54%] [G loss: 1.494454, adv: 0.398129, recon: 0.030902, id: 0.026604] time: 0:01:24.782061 \n",
            "[Epoch 13/200] [Batch 25/48] [D loss: 0.249520, acc:  51%] [G loss: 1.470696, adv: 0.390244, recon: 0.030529, id: 0.027890] time: 0:01:24.892246 \n",
            "[Epoch 13/200] [Batch 26/48] [D loss: 0.252227, acc:  49%] [G loss: 1.467230, adv: 0.377283, recon: 0.031432, id: 0.029978] time: 0:01:25.003625 \n",
            "[Epoch 13/200] [Batch 27/48] [D loss: 0.239463, acc:  55%] [G loss: 1.486109, adv: 0.396323, recon: 0.030650, id: 0.029129] time: 0:01:25.116242 \n",
            "[Epoch 13/200] [Batch 28/48] [D loss: 0.242764, acc:  54%] [G loss: 1.497370, adv: 0.397947, recon: 0.031066, id: 0.027385] time: 0:01:25.231481 \n",
            "[Epoch 13/200] [Batch 29/48] [D loss: 0.234662, acc:  57%] [G loss: 1.517574, adv: 0.410898, recon: 0.030653, id: 0.027993] time: 0:01:25.347103 \n",
            "[Epoch 13/200] [Batch 30/48] [D loss: 0.240694, acc:  55%] [G loss: 1.484024, adv: 0.392265, recon: 0.030875, id: 0.028503] time: 0:01:25.459451 \n",
            "[Epoch 13/200] [Batch 31/48] [D loss: 0.245868, acc:  52%] [G loss: 1.463382, adv: 0.388403, recon: 0.030380, id: 0.027202] time: 0:01:25.570188 \n",
            "[Epoch 13/200] [Batch 32/48] [D loss: 0.239528, acc:  54%] [G loss: 1.539654, adv: 0.394852, recon: 0.033295, id: 0.030371] time: 0:01:25.680367 \n",
            "[Epoch 13/200] [Batch 33/48] [D loss: 0.244041, acc:  54%] [G loss: 1.532249, adv: 0.403207, recon: 0.032155, id: 0.029530] time: 0:01:25.791538 \n",
            "[Epoch 13/200] [Batch 34/48] [D loss: 0.241274, acc:  53%] [G loss: 1.509043, adv: 0.393035, recon: 0.032077, id: 0.028355] time: 0:01:25.900520 \n",
            "[Epoch 13/200] [Batch 35/48] [D loss: 0.242758, acc:  53%] [G loss: 1.477786, adv: 0.391962, recon: 0.030723, id: 0.027650] time: 0:01:26.012028 \n",
            "[Epoch 13/200] [Batch 36/48] [D loss: 0.257474, acc:  50%] [G loss: 1.506697, adv: 0.381245, recon: 0.032972, id: 0.030087] time: 0:01:26.126148 \n",
            "[Epoch 13/200] [Batch 37/48] [D loss: 0.236785, acc:  56%] [G loss: 1.508516, adv: 0.406896, recon: 0.030734, id: 0.028529] time: 0:01:26.246173 \n",
            "[Epoch 13/200] [Batch 38/48] [D loss: 0.250094, acc:  49%] [G loss: 1.444914, adv: 0.377806, recon: 0.030466, id: 0.027138] time: 0:01:26.355817 \n",
            "[Epoch 13/200] [Batch 39/48] [D loss: 0.245311, acc:  53%] [G loss: 1.478918, adv: 0.388640, recon: 0.031128, id: 0.028530] time: 0:01:26.467318 \n",
            "[Epoch 13/200] [Batch 40/48] [D loss: 0.250116, acc:  51%] [G loss: 1.496376, adv: 0.395216, recon: 0.031208, id: 0.028619] time: 0:01:26.577830 \n",
            "[Epoch 13/200] [Batch 41/48] [D loss: 0.241609, acc:  54%] [G loss: 1.440969, adv: 0.386468, recon: 0.029564, id: 0.026940] time: 0:01:26.690984 \n",
            "[Epoch 13/200] [Batch 42/48] [D loss: 0.242951, acc:  53%] [G loss: 1.453360, adv: 0.392473, recon: 0.029463, id: 0.028617] time: 0:01:26.799444 \n",
            "[Epoch 13/200] [Batch 43/48] [D loss: 0.242933, acc:  53%] [G loss: 1.480845, adv: 0.391275, recon: 0.030855, id: 0.029198] time: 0:01:26.911743 \n",
            "[Epoch 13/200] [Batch 44/48] [D loss: 0.233483, acc:  57%] [G loss: 1.490206, adv: 0.403786, recon: 0.030147, id: 0.029333] time: 0:01:27.026459 \n",
            "[Epoch 13/200] [Batch 45/48] [D loss: 0.241881, acc:  53%] [G loss: 1.496389, adv: 0.391443, recon: 0.031642, id: 0.029056] time: 0:01:27.139292 \n",
            "[Epoch 13/200] [Batch 46/48] [D loss: 0.246224, acc:  51%] [G loss: 1.471588, adv: 0.391209, recon: 0.030431, id: 0.027782] time: 0:01:27.255116 \n",
            "[Epoch 13/200] [Batch 47/48] [D loss: 0.245718, acc:  50%] [G loss: 1.451930, adv: 0.385354, recon: 0.030267, id: 0.026144] time: 0:01:27.366754 \n",
            "[Epoch 14/200] [Batch 0/48] [D loss: 0.244764, acc:  53%] [G loss: 1.532109, adv: 0.397755, recon: 0.032796, id: 0.029727] time: 0:01:27.477947 \n",
            "[Epoch 14/200] [Batch 1/48] [D loss: 0.246858, acc:  50%] [G loss: 1.530266, adv: 0.383015, recon: 0.034120, id: 0.030742] time: 0:01:27.589395 \n",
            "[Epoch 14/200] [Batch 2/48] [D loss: 0.241162, acc:  53%] [G loss: 1.484877, adv: 0.398399, recon: 0.030541, id: 0.028182] time: 0:01:27.700135 \n",
            "[Epoch 14/200] [Batch 3/48] [D loss: 0.245436, acc:  51%] [G loss: 1.491578, adv: 0.393578, recon: 0.031212, id: 0.028194] time: 0:01:27.813718 \n",
            "[Epoch 14/200] [Batch 4/48] [D loss: 0.240378, acc:  55%] [G loss: 1.510318, adv: 0.394943, recon: 0.031838, id: 0.029049] time: 0:01:27.926698 \n",
            "[Epoch 14/200] [Batch 5/48] [D loss: 0.243285, acc:  54%] [G loss: 1.480606, adv: 0.398267, recon: 0.030252, id: 0.027292] time: 0:01:28.039239 \n",
            "[Epoch 14/200] [Batch 6/48] [D loss: 0.254178, acc:  49%] [G loss: 1.481014, adv: 0.381733, recon: 0.031655, id: 0.029909] time: 0:01:28.150025 \n",
            "[Epoch 14/200] [Batch 7/48] [D loss: 0.251235, acc:  50%] [G loss: 1.449156, adv: 0.377566, recon: 0.030689, id: 0.029031] time: 0:01:28.271145 \n",
            "[Epoch 14/200] [Batch 8/48] [D loss: 0.236059, acc:  55%] [G loss: 1.467808, adv: 0.389634, recon: 0.030315, id: 0.029261] time: 0:01:28.385644 \n",
            "[Epoch 14/200] [Batch 9/48] [D loss: 0.246544, acc:  52%] [G loss: 1.460084, adv: 0.386747, recon: 0.030432, id: 0.027098] time: 0:01:28.498196 \n",
            "[Epoch 14/200] [Batch 10/48] [D loss: 0.249157, acc:  50%] [G loss: 1.481979, adv: 0.384768, recon: 0.031627, id: 0.027938] time: 0:01:28.613506 \n",
            "[Epoch 14/200] [Batch 11/48] [D loss: 0.239131, acc:  55%] [G loss: 1.487663, adv: 0.399975, recon: 0.030382, id: 0.028130] time: 0:01:28.732714 \n",
            "[Epoch 14/200] [Batch 12/48] [D loss: 0.255355, acc:  49%] [G loss: 1.482578, adv: 0.385346, recon: 0.031340, id: 0.031552] time: 0:01:28.845162 \n",
            "[Epoch 14/200] [Batch 13/48] [D loss: 0.245824, acc:  51%] [G loss: 1.464051, adv: 0.384949, recon: 0.030683, id: 0.028312] time: 0:01:28.958194 \n",
            "[Epoch 14/200] [Batch 14/48] [D loss: 0.229845, acc:  56%] [G loss: 1.486233, adv: 0.404710, recon: 0.029931, id: 0.026847] time: 0:01:29.068547 \n",
            "[Epoch 14/200] [Batch 15/48] [D loss: 0.248493, acc:  51%] [G loss: 1.486503, adv: 0.386900, recon: 0.031427, id: 0.030911] time: 0:01:29.181564 \n",
            "[Epoch 14/200] [Batch 16/48] [D loss: 0.236054, acc:  57%] [G loss: 1.443244, adv: 0.400526, recon: 0.028276, id: 0.026266] time: 0:01:29.300866 \n",
            "[Epoch 14/200] [Batch 17/48] [D loss: 0.238409, acc:  55%] [G loss: 1.469066, adv: 0.394768, recon: 0.030016, id: 0.029876] time: 0:01:29.413213 \n",
            "[Epoch 14/200] [Batch 18/48] [D loss: 0.240513, acc:  54%] [G loss: 1.493224, adv: 0.393916, recon: 0.031339, id: 0.027841] time: 0:01:29.525614 \n",
            "[Epoch 14/200] [Batch 19/48] [D loss: 0.235073, acc:  57%] [G loss: 1.560523, adv: 0.402776, recon: 0.033648, id: 0.030194] time: 0:01:29.639915 \n",
            "[Epoch 14/200] [Batch 20/48] [D loss: 0.246389, acc:  53%] [G loss: 1.456000, adv: 0.384270, recon: 0.030474, id: 0.030290] time: 0:01:29.751982 \n",
            "[Epoch 14/200] [Batch 21/48] [D loss: 0.263600, acc:  46%] [G loss: 1.487493, adv: 0.373128, recon: 0.032858, id: 0.030923] time: 0:01:29.863559 \n",
            "[Epoch 14/200] [Batch 22/48] [D loss: 0.233678, acc:  56%] [G loss: 1.490252, adv: 0.404337, recon: 0.030092, id: 0.027911] time: 0:01:29.975873 \n",
            "[Epoch 14/200] [Batch 23/48] [D loss: 0.237667, acc:  56%] [G loss: 1.530148, adv: 0.402509, recon: 0.032161, id: 0.028083] time: 0:01:30.089881 \n",
            "[Epoch 14/200] [Batch 24/48] [D loss: 0.237487, acc:  54%] [G loss: 1.476590, adv: 0.390941, recon: 0.030752, id: 0.026850] time: 0:01:30.202161 \n",
            "[Epoch 14/200] [Batch 25/48] [D loss: 0.245910, acc:  53%] [G loss: 1.467728, adv: 0.391008, recon: 0.030376, id: 0.027846] time: 0:01:30.319654 \n",
            "[Epoch 14/200] [Batch 26/48] [D loss: 0.251144, acc:  50%] [G loss: 1.470816, adv: 0.374325, recon: 0.031920, id: 0.030320] time: 0:01:30.438460 \n",
            "[Epoch 14/200] [Batch 27/48] [D loss: 0.236353, acc:  56%] [G loss: 1.502650, adv: 0.396714, recon: 0.031494, id: 0.029310] time: 0:01:30.550751 \n",
            "[Epoch 14/200] [Batch 28/48] [D loss: 0.241347, acc:  55%] [G loss: 1.514111, adv: 0.396599, recon: 0.032043, id: 0.027869] time: 0:01:30.664757 \n",
            "[Epoch 14/200] [Batch 29/48] [D loss: 0.230596, acc:  58%] [G loss: 1.520280, adv: 0.414498, recon: 0.030502, id: 0.027719] time: 0:01:30.778910 \n",
            "[Epoch 14/200] [Batch 30/48] [D loss: 0.240469, acc:  55%] [G loss: 1.481659, adv: 0.388882, recon: 0.031137, id: 0.028226] time: 0:01:30.892315 \n",
            "[Epoch 14/200] [Batch 31/48] [D loss: 0.243942, acc:  53%] [G loss: 1.457317, adv: 0.387192, recon: 0.030257, id: 0.027162] time: 0:01:31.007145 \n",
            "[Epoch 14/200] [Batch 32/48] [D loss: 0.235647, acc:  56%] [G loss: 1.515966, adv: 0.396836, recon: 0.031989, id: 0.030293] time: 0:01:31.117600 \n",
            "[Epoch 14/200] [Batch 33/48] [D loss: 0.245320, acc:  53%] [G loss: 1.503333, adv: 0.395263, recon: 0.031532, id: 0.029284] time: 0:01:31.233874 \n",
            "[Epoch 14/200] [Batch 34/48] [D loss: 0.235495, acc:  55%] [G loss: 1.479126, adv: 0.400894, recon: 0.029916, id: 0.027890] time: 0:01:31.349595 \n",
            "[Epoch 14/200] [Batch 35/48] [D loss: 0.243036, acc:  53%] [G loss: 1.450309, adv: 0.390474, recon: 0.029528, id: 0.027421] time: 0:01:31.463268 \n",
            "[Epoch 14/200] [Batch 36/48] [D loss: 0.255536, acc:  50%] [G loss: 1.475856, adv: 0.380476, recon: 0.031596, id: 0.029428] time: 0:01:31.575636 \n",
            "[Epoch 14/200] [Batch 37/48] [D loss: 0.235909, acc:  56%] [G loss: 1.492639, adv: 0.403580, recon: 0.030283, id: 0.028550] time: 0:01:31.687957 \n",
            "[Epoch 14/200] [Batch 38/48] [D loss: 0.246962, acc:  51%] [G loss: 1.437406, adv: 0.379864, recon: 0.029959, id: 0.026745] time: 0:01:31.801820 \n",
            "[Epoch 14/200] [Batch 39/48] [D loss: 0.244302, acc:  54%] [G loss: 1.469429, adv: 0.387067, recon: 0.030845, id: 0.028645] time: 0:01:31.914134 \n",
            "[Epoch 14/200] [Batch 40/48] [D loss: 0.246936, acc:  53%] [G loss: 1.487604, adv: 0.397721, recon: 0.030574, id: 0.028548] time: 0:01:32.027828 \n",
            "[Epoch 14/200] [Batch 41/48] [D loss: 0.241224, acc:  54%] [G loss: 1.435030, adv: 0.385023, recon: 0.029435, id: 0.026989] time: 0:01:32.143395 \n",
            "[Epoch 14/200] [Batch 42/48] [D loss: 0.239932, acc:  55%] [G loss: 1.446731, adv: 0.395337, recon: 0.028897, id: 0.028566] time: 0:01:32.257106 \n",
            "[Epoch 14/200] [Batch 43/48] [D loss: 0.242768, acc:  53%] [G loss: 1.465074, adv: 0.387194, recon: 0.030513, id: 0.029175] time: 0:01:32.375672 \n",
            "[Epoch 14/200] [Batch 44/48] [D loss: 0.230662, acc:  58%] [G loss: 1.479718, adv: 0.403093, recon: 0.029742, id: 0.029318] time: 0:01:32.487841 \n",
            "[Epoch 14/200] [Batch 45/48] [D loss: 0.241355, acc:  53%] [G loss: 1.483598, adv: 0.386952, recon: 0.031490, id: 0.029078] time: 0:01:32.602448 \n",
            "[Epoch 14/200] [Batch 46/48] [D loss: 0.244113, acc:  52%] [G loss: 1.466307, adv: 0.390216, recon: 0.030317, id: 0.027640] time: 0:01:32.712773 \n",
            "[Epoch 14/200] [Batch 47/48] [D loss: 0.243221, acc:  51%] [G loss: 1.452036, adv: 0.385799, recon: 0.030267, id: 0.026210] time: 0:01:32.823628 \n",
            "[Epoch 15/200] [Batch 0/48] [D loss: 0.243603, acc:  54%] [G loss: 1.534584, adv: 0.395250, recon: 0.033224, id: 0.029523] time: 0:01:32.938855 \n",
            "[Epoch 15/200] [Batch 1/48] [D loss: 0.243745, acc:  52%] [G loss: 1.523473, adv: 0.385559, recon: 0.033592, id: 0.030554] time: 0:01:33.052304 \n",
            "[Epoch 15/200] [Batch 2/48] [D loss: 0.239693, acc:  54%] [G loss: 1.466581, adv: 0.397259, recon: 0.029785, id: 0.027986] time: 0:01:33.167538 \n",
            "[Epoch 15/200] [Batch 3/48] [D loss: 0.243655, acc:  52%] [G loss: 1.473241, adv: 0.392901, recon: 0.030454, id: 0.027611] time: 0:01:33.280844 \n",
            "[Epoch 15/200] [Batch 4/48] [D loss: 0.240160, acc:  55%] [G loss: 1.495037, adv: 0.390960, recon: 0.031514, id: 0.029287] time: 0:01:33.398532 \n",
            "[Epoch 15/200] [Batch 5/48] [D loss: 0.240546, acc:  55%] [G loss: 1.478061, adv: 0.399669, recon: 0.030039, id: 0.027078] time: 0:01:33.510019 \n",
            "[Epoch 15/200] [Batch 6/48] [D loss: 0.251691, acc:  50%] [G loss: 1.484780, adv: 0.384670, recon: 0.031593, id: 0.029939] time: 0:01:33.622418 \n",
            "[Epoch 15/200] [Batch 7/48] [D loss: 0.248497, acc:  51%] [G loss: 1.455917, adv: 0.382117, recon: 0.030618, id: 0.029071] time: 0:01:33.735777 \n",
            "[Epoch 15/200] [Batch 8/48] [D loss: 0.234464, acc:  56%] [G loss: 1.476263, adv: 0.392426, recon: 0.030495, id: 0.029255] time: 0:01:33.852217 \n",
            "[Epoch 15/200] [Batch 9/48] [D loss: 0.244194, acc:  53%] [G loss: 1.467827, adv: 0.390121, recon: 0.030529, id: 0.027153] time: 0:01:33.966948 \n",
            "[Epoch 15/200] [Batch 10/48] [D loss: 0.248218, acc:  51%] [G loss: 1.469276, adv: 0.381389, recon: 0.031384, id: 0.027790] time: 0:01:34.085487 \n",
            "[Epoch 15/200] [Batch 11/48] [D loss: 0.237439, acc:  56%] [G loss: 1.482722, adv: 0.396756, recon: 0.030493, id: 0.028149] time: 0:01:34.199925 \n",
            "[Epoch 15/200] [Batch 12/48] [D loss: 0.251028, acc:  51%] [G loss: 1.483382, adv: 0.389407, recon: 0.031059, id: 0.031226] time: 0:01:34.313840 \n",
            "[Epoch 15/200] [Batch 13/48] [D loss: 0.244019, acc:  51%] [G loss: 1.464806, adv: 0.387027, recon: 0.030558, id: 0.028184] time: 0:01:34.431917 \n",
            "[Epoch 15/200] [Batch 14/48] [D loss: 0.229741, acc:  56%] [G loss: 1.467579, adv: 0.399999, recon: 0.029539, id: 0.026656] time: 0:01:34.547148 \n",
            "[Epoch 15/200] [Batch 15/48] [D loss: 0.246471, acc:  52%] [G loss: 1.473667, adv: 0.383731, recon: 0.031149, id: 0.030778] time: 0:01:34.662136 \n",
            "[Epoch 15/200] [Batch 16/48] [D loss: 0.232703, acc:  59%] [G loss: 1.444601, adv: 0.403064, recon: 0.028135, id: 0.026381] time: 0:01:34.775011 \n",
            "[Epoch 15/200] [Batch 17/48] [D loss: 0.236500, acc:  56%] [G loss: 1.459022, adv: 0.395575, recon: 0.029492, id: 0.029913] time: 0:01:34.887662 \n",
            "[Epoch 15/200] [Batch 18/48] [D loss: 0.240755, acc:  54%] [G loss: 1.471186, adv: 0.389213, recon: 0.030731, id: 0.028020] time: 0:01:35.001504 \n",
            "[Epoch 15/200] [Batch 19/48] [D loss: 0.230908, acc:  58%] [G loss: 1.511003, adv: 0.407980, recon: 0.030738, id: 0.029431] time: 0:01:35.114402 \n",
            "[Epoch 15/200] [Batch 20/48] [D loss: 0.243941, acc:  54%] [G loss: 1.427970, adv: 0.385761, recon: 0.028970, id: 0.030404] time: 0:01:35.228140 \n",
            "[Epoch 15/200] [Batch 21/48] [D loss: 0.262464, acc:  46%] [G loss: 1.473735, adv: 0.373304, recon: 0.032177, id: 0.031018] time: 0:01:35.340747 \n",
            "[Epoch 15/200] [Batch 22/48] [D loss: 0.230757, acc:  58%] [G loss: 1.479919, adv: 0.406247, recon: 0.029442, id: 0.027983] time: 0:01:35.461121 \n",
            "[Epoch 15/200] [Batch 23/48] [D loss: 0.236913, acc:  56%] [G loss: 1.522520, adv: 0.397899, recon: 0.032277, id: 0.028236] time: 0:01:35.575777 \n",
            "[Epoch 15/200] [Batch 24/48] [D loss: 0.232110, acc:  56%] [G loss: 1.480416, adv: 0.399325, recon: 0.030178, id: 0.026633] time: 0:01:35.688445 \n",
            "[Epoch 15/200] [Batch 25/48] [D loss: 0.246628, acc:  52%] [G loss: 1.456008, adv: 0.388821, recon: 0.030031, id: 0.027849] time: 0:01:35.803496 \n",
            "[Epoch 15/200] [Batch 26/48] [D loss: 0.248412, acc:  51%] [G loss: 1.459778, adv: 0.376079, recon: 0.031287, id: 0.029857] time: 0:01:35.916777 \n",
            "[Epoch 15/200] [Batch 27/48] [D loss: 0.234002, acc:  57%] [G loss: 1.484362, adv: 0.400328, recon: 0.030259, id: 0.028932] time: 0:01:36.028960 \n",
            "[Epoch 15/200] [Batch 28/48] [D loss: 0.240310, acc:  55%] [G loss: 1.481868, adv: 0.394788, recon: 0.030705, id: 0.027237] time: 0:01:36.141345 \n",
            "[Epoch 15/200] [Batch 29/48] [D loss: 0.230680, acc:  58%] [G loss: 1.505432, adv: 0.410213, recon: 0.030222, id: 0.027769] time: 0:01:36.255612 \n",
            "[Epoch 15/200] [Batch 30/48] [D loss: 0.237441, acc:  57%] [G loss: 1.468039, adv: 0.392461, recon: 0.030161, id: 0.028405] time: 0:01:36.371480 \n",
            "[Epoch 15/200] [Batch 31/48] [D loss: 0.243767, acc:  53%] [G loss: 1.448384, adv: 0.386281, recon: 0.029936, id: 0.027130] time: 0:01:36.488144 \n",
            "[Epoch 15/200] [Batch 32/48] [D loss: 0.235671, acc:  56%] [G loss: 1.522484, adv: 0.394831, recon: 0.032538, id: 0.030309] time: 0:01:36.601862 \n",
            "[Epoch 15/200] [Batch 33/48] [D loss: 0.240600, acc:  56%] [G loss: 1.515503, adv: 0.401036, recon: 0.031636, id: 0.029307] time: 0:01:36.718218 \n",
            "[Epoch 15/200] [Batch 34/48] [D loss: 0.235057, acc:  55%] [G loss: 1.493217, adv: 0.399396, recon: 0.030755, id: 0.028330] time: 0:01:36.833082 \n",
            "[Epoch 15/200] [Batch 35/48] [D loss: 0.240070, acc:  54%] [G loss: 1.468045, adv: 0.392798, recon: 0.030241, id: 0.027325] time: 0:01:36.945480 \n",
            "[Epoch 15/200] [Batch 36/48] [D loss: 0.252383, acc:  51%] [G loss: 1.502980, adv: 0.386743, recon: 0.032327, id: 0.030081] time: 0:01:37.058049 \n",
            "[Epoch 15/200] [Batch 37/48] [D loss: 0.233695, acc:  57%] [G loss: 1.500837, adv: 0.406212, recon: 0.030508, id: 0.028095] time: 0:01:37.173124 \n",
            "[Epoch 15/200] [Batch 38/48] [D loss: 0.245927, acc:  51%] [G loss: 1.436212, adv: 0.378386, recon: 0.030062, id: 0.027236] time: 0:01:37.287838 \n",
            "[Epoch 15/200] [Batch 39/48] [D loss: 0.240501, acc:  56%] [G loss: 1.471309, adv: 0.391453, recon: 0.030572, id: 0.028375] time: 0:01:37.400095 \n",
            "[Epoch 15/200] [Batch 40/48] [D loss: 0.247180, acc:  53%] [G loss: 1.478913, adv: 0.392629, recon: 0.030678, id: 0.028320] time: 0:01:37.515242 \n",
            "[Epoch 15/200] [Batch 41/48] [D loss: 0.237954, acc:  56%] [G loss: 1.429459, adv: 0.386064, recon: 0.029112, id: 0.026822] time: 0:01:37.626729 \n",
            "[Epoch 15/200] [Batch 42/48] [D loss: 0.238405, acc:  55%] [G loss: 1.444377, adv: 0.394822, recon: 0.028857, id: 0.028640] time: 0:01:37.740480 \n",
            "[Epoch 15/200] [Batch 43/48] [D loss: 0.239256, acc:  54%] [G loss: 1.464148, adv: 0.393505, recon: 0.029913, id: 0.028772] time: 0:01:37.853564 \n",
            "[Epoch 15/200] [Batch 44/48] [D loss: 0.229641, acc:  59%] [G loss: 1.469679, adv: 0.403199, recon: 0.029249, id: 0.029414] time: 0:01:37.967091 \n",
            "[Epoch 15/200] [Batch 45/48] [D loss: 0.235762, acc:  56%] [G loss: 1.480275, adv: 0.397072, recon: 0.030415, id: 0.028290] time: 0:01:38.081119 \n",
            "[Epoch 15/200] [Batch 46/48] [D loss: 0.246003, acc:  52%] [G loss: 1.464222, adv: 0.386111, recon: 0.030620, id: 0.028063] time: 0:01:38.194902 \n",
            "[Epoch 15/200] [Batch 47/48] [D loss: 0.239471, acc:  53%] [G loss: 1.436759, adv: 0.389862, recon: 0.029228, id: 0.024980] time: 0:01:38.309625 \n",
            "[Epoch 16/200] [Batch 0/48] [D loss: 0.246213, acc:  53%] [G loss: 1.490717, adv: 0.388391, recon: 0.031663, id: 0.030217] time: 0:01:38.425849 \n",
            "[Epoch 16/200] [Batch 1/48] [D loss: 0.236353, acc:  56%] [G loss: 1.472024, adv: 0.396495, recon: 0.030109, id: 0.029023] time: 0:01:38.542088 \n",
            "[Epoch 16/200] [Batch 2/48] [D loss: 0.247788, acc:  50%] [G loss: 1.481130, adv: 0.378740, recon: 0.032262, id: 0.028951] time: 0:01:38.655080 \n",
            "[Epoch 16/200] [Batch 3/48] [D loss: 0.234937, acc:  57%] [G loss: 1.516019, adv: 0.405473, recon: 0.031476, id: 0.026734] time: 0:01:38.766790 \n",
            "[Epoch 16/200] [Batch 4/48] [D loss: 0.242262, acc:  54%] [G loss: 1.536635, adv: 0.385469, recon: 0.034073, id: 0.030352] time: 0:01:38.879041 \n",
            "[Epoch 16/200] [Batch 5/48] [D loss: 0.238369, acc:  56%] [G loss: 1.509869, adv: 0.401113, recon: 0.031544, id: 0.026816] time: 0:01:38.991256 \n",
            "[Epoch 16/200] [Batch 6/48] [D loss: 0.246589, acc:  52%] [G loss: 1.514198, adv: 0.391817, recon: 0.032401, id: 0.030428] time: 0:01:39.103534 \n",
            "[Epoch 16/200] [Batch 7/48] [D loss: 0.246036, acc:  52%] [G loss: 1.456996, adv: 0.383158, recon: 0.030638, id: 0.029013] time: 0:01:39.213573 \n",
            "[Epoch 16/200] [Batch 8/48] [D loss: 0.231783, acc:  57%] [G loss: 1.465149, adv: 0.392512, recon: 0.030017, id: 0.028453] time: 0:01:39.325840 \n",
            "[Epoch 16/200] [Batch 9/48] [D loss: 0.243385, acc:  53%] [G loss: 1.445626, adv: 0.386895, recon: 0.029789, id: 0.026817] time: 0:01:39.437669 \n",
            "[Epoch 16/200] [Batch 10/48] [D loss: 0.242477, acc:  53%] [G loss: 1.463395, adv: 0.389412, recon: 0.030374, id: 0.027449] time: 0:01:39.555702 \n",
            "[Epoch 16/200] [Batch 11/48] [D loss: 0.238304, acc:  55%] [G loss: 1.485760, adv: 0.395452, recon: 0.030785, id: 0.028321] time: 0:01:39.668177 \n",
            "[Epoch 16/200] [Batch 12/48] [D loss: 0.248422, acc:  52%] [G loss: 1.484553, adv: 0.393090, recon: 0.030804, id: 0.031137] time: 0:01:39.780474 \n",
            "[Epoch 16/200] [Batch 13/48] [D loss: 0.243267, acc:  52%] [G loss: 1.454801, adv: 0.385989, recon: 0.030199, id: 0.028130] time: 0:01:39.891063 \n",
            "[Epoch 16/200] [Batch 14/48] [D loss: 0.227048, acc:  57%] [G loss: 1.476032, adv: 0.404989, recon: 0.029501, id: 0.026568] time: 0:01:40.002237 \n",
            "[Epoch 16/200] [Batch 15/48] [D loss: 0.244264, acc:  53%] [G loss: 1.481315, adv: 0.388901, recon: 0.031064, id: 0.030692] time: 0:01:40.112591 \n",
            "[Epoch 16/200] [Batch 16/48] [D loss: 0.232029, acc:  59%] [G loss: 1.444616, adv: 0.403302, recon: 0.028146, id: 0.026438] time: 0:01:40.222680 \n",
            "[Epoch 16/200] [Batch 17/48] [D loss: 0.234223, acc:  57%] [G loss: 1.462308, adv: 0.398233, recon: 0.029443, id: 0.029741] time: 0:01:40.331958 \n",
            "[Epoch 16/200] [Batch 18/48] [D loss: 0.239499, acc:  55%] [G loss: 1.462891, adv: 0.388807, recon: 0.030401, id: 0.027950] time: 0:01:40.445510 \n",
            "[Epoch 16/200] [Batch 19/48] [D loss: 0.228066, acc:  60%] [G loss: 1.504292, adv: 0.409451, recon: 0.030310, id: 0.029261] time: 0:01:40.564255 \n",
            "[Epoch 16/200] [Batch 20/48] [D loss: 0.242025, acc:  55%] [G loss: 1.422276, adv: 0.386838, recon: 0.028619, id: 0.030412] time: 0:01:40.678245 \n",
            "[Epoch 16/200] [Batch 21/48] [D loss: 0.260652, acc:  47%] [G loss: 1.461539, adv: 0.373973, recon: 0.031556, id: 0.030870] time: 0:01:40.793922 \n",
            "[Epoch 16/200] [Batch 22/48] [D loss: 0.230045, acc:  58%] [G loss: 1.465866, adv: 0.403761, recon: 0.029025, id: 0.027907] time: 0:01:40.903987 \n",
            "[Epoch 16/200] [Batch 23/48] [D loss: 0.233498, acc:  58%] [G loss: 1.519429, adv: 0.400705, recon: 0.031891, id: 0.028173] time: 0:01:41.020672 \n",
            "[Epoch 16/200] [Batch 24/48] [D loss: 0.231458, acc:  57%] [G loss: 1.479929, adv: 0.397293, recon: 0.030382, id: 0.026685] time: 0:01:41.130427 \n",
            "[Epoch 16/200] [Batch 25/48] [D loss: 0.243476, acc:  54%] [G loss: 1.460943, adv: 0.394334, recon: 0.029765, id: 0.027790] time: 0:01:41.243239 \n",
            "[Epoch 16/200] [Batch 26/48] [D loss: 0.246896, acc:  52%] [G loss: 1.450272, adv: 0.378952, recon: 0.030572, id: 0.029802] time: 0:01:41.354150 \n",
            "[Epoch 16/200] [Batch 27/48] [D loss: 0.232877, acc:  58%] [G loss: 1.474711, adv: 0.399804, recon: 0.029870, id: 0.028987] time: 0:01:41.464936 \n",
            "[Epoch 16/200] [Batch 28/48] [D loss: 0.236789, acc:  57%] [G loss: 1.480318, adv: 0.400692, recon: 0.030082, id: 0.027091] time: 0:01:41.582144 \n",
            "[Epoch 16/200] [Batch 29/48] [D loss: 0.228206, acc:  59%] [G loss: 1.501590, adv: 0.414059, recon: 0.029692, id: 0.027707] time: 0:01:41.694757 \n",
            "[Epoch 16/200] [Batch 30/48] [D loss: 0.237269, acc:  57%] [G loss: 1.459224, adv: 0.389667, recon: 0.030043, id: 0.028274] time: 0:01:41.805501 \n",
            "[Epoch 16/200] [Batch 31/48] [D loss: 0.243558, acc:  54%] [G loss: 1.436362, adv: 0.383318, recon: 0.029657, id: 0.027315] time: 0:01:41.916732 \n",
            "[Epoch 16/200] [Batch 32/48] [D loss: 0.231163, acc:  58%] [G loss: 1.512661, adv: 0.400458, recon: 0.031548, id: 0.030364] time: 0:01:42.027745 \n",
            "[Epoch 16/200] [Batch 33/48] [D loss: 0.240892, acc:  55%] [G loss: 1.507001, adv: 0.400355, recon: 0.031297, id: 0.029220] time: 0:01:42.137857 \n",
            "[Epoch 16/200] [Batch 34/48] [D loss: 0.233039, acc:  56%] [G loss: 1.475334, adv: 0.401916, recon: 0.029687, id: 0.028015] time: 0:01:42.250106 \n",
            "[Epoch 16/200] [Batch 35/48] [D loss: 0.240425, acc:  55%] [G loss: 1.445647, adv: 0.389129, recon: 0.029504, id: 0.027529] time: 0:01:42.364201 \n",
            "[Epoch 16/200] [Batch 36/48] [D loss: 0.249897, acc:  52%] [G loss: 1.488909, adv: 0.388670, recon: 0.031516, id: 0.029394] time: 0:01:42.475492 \n",
            "[Epoch 16/200] [Batch 37/48] [D loss: 0.233788, acc:  57%] [G loss: 1.491105, adv: 0.403881, recon: 0.030253, id: 0.028479] time: 0:01:42.591573 \n",
            "[Epoch 16/200] [Batch 38/48] [D loss: 0.242945, acc:  52%] [G loss: 1.434067, adv: 0.382112, recon: 0.029656, id: 0.026806] time: 0:01:42.704167 \n",
            "[Epoch 16/200] [Batch 39/48] [D loss: 0.240395, acc:  56%] [G loss: 1.468425, adv: 0.390908, recon: 0.030493, id: 0.028641] time: 0:01:42.814873 \n",
            "[Epoch 16/200] [Batch 40/48] [D loss: 0.244258, acc:  54%] [G loss: 1.470837, adv: 0.396747, recon: 0.029935, id: 0.028153] time: 0:01:42.926122 \n",
            "[Epoch 16/200] [Batch 41/48] [D loss: 0.238806, acc:  56%] [G loss: 1.426309, adv: 0.384077, recon: 0.029163, id: 0.027012] time: 0:01:43.035949 \n",
            "[Epoch 16/200] [Batch 42/48] [D loss: 0.235592, acc:  56%] [G loss: 1.444076, adv: 0.398334, recon: 0.028560, id: 0.028451] time: 0:01:43.150901 \n",
            "[Epoch 16/200] [Batch 43/48] [D loss: 0.238721, acc:  55%] [G loss: 1.483486, adv: 0.393536, recon: 0.030862, id: 0.029146] time: 0:01:43.263237 \n",
            "[Epoch 16/200] [Batch 44/48] [D loss: 0.225992, acc:  60%] [G loss: 1.513999, adv: 0.407849, recon: 0.031092, id: 0.029114] time: 0:01:43.374806 \n",
            "[Epoch 16/200] [Batch 45/48] [D loss: 0.236295, acc:  55%] [G loss: 1.530554, adv: 0.395083, recon: 0.033066, id: 0.029639] time: 0:01:43.485850 \n",
            "[Epoch 16/200] [Batch 46/48] [D loss: 0.238858, acc:  55%] [G loss: 1.493007, adv: 0.400147, recon: 0.030770, id: 0.027177] time: 0:01:43.602933 \n",
            "[Epoch 16/200] [Batch 47/48] [D loss: 0.240216, acc:  52%] [G loss: 1.451268, adv: 0.389022, recon: 0.029990, id: 0.026217] time: 0:01:43.713790 \n",
            "[Epoch 17/200] [Batch 0/48] [D loss: 0.242346, acc:  55%] [G loss: 1.495983, adv: 0.390364, recon: 0.031865, id: 0.029217] time: 0:01:43.826136 \n",
            "[Epoch 17/200] [Batch 1/48] [D loss: 0.237487, acc:  55%] [G loss: 1.477761, adv: 0.390793, recon: 0.030923, id: 0.029872] time: 0:01:43.936678 \n",
            "[Epoch 17/200] [Batch 2/48] [D loss: 0.237210, acc:  55%] [G loss: 1.452406, adv: 0.395694, recon: 0.029295, id: 0.027947] time: 0:01:44.049066 \n",
            "[Epoch 17/200] [Batch 3/48] [D loss: 0.238866, acc:  54%] [G loss: 1.472316, adv: 0.399058, recon: 0.029902, id: 0.027236] time: 0:01:44.158635 \n",
            "[Epoch 17/200] [Batch 4/48] [D loss: 0.237218, acc:  57%] [G loss: 1.495719, adv: 0.393640, recon: 0.031364, id: 0.029158] time: 0:01:44.271336 \n",
            "[Epoch 17/200] [Batch 5/48] [D loss: 0.238518, acc:  56%] [G loss: 1.474168, adv: 0.400366, recon: 0.029834, id: 0.026974] time: 0:01:44.384628 \n",
            "[Epoch 17/200] [Batch 6/48] [D loss: 0.246995, acc:  52%] [G loss: 1.482195, adv: 0.389626, recon: 0.031059, id: 0.029658] time: 0:01:44.496097 \n",
            "[Epoch 17/200] [Batch 7/48] [D loss: 0.245532, acc:  52%] [G loss: 1.444875, adv: 0.385580, recon: 0.029795, id: 0.028948] time: 0:01:44.609794 \n",
            "[Epoch 17/200] [Batch 8/48] [D loss: 0.229931, acc:  58%] [G loss: 1.454516, adv: 0.396462, recon: 0.029121, id: 0.028877] time: 0:01:44.724829 \n",
            "[Epoch 17/200] [Batch 9/48] [D loss: 0.245803, acc:  52%] [G loss: 1.429717, adv: 0.379676, recon: 0.029726, id: 0.027131] time: 0:01:44.836897 \n",
            "[Epoch 17/200] [Batch 10/48] [D loss: 0.241716, acc:  54%] [G loss: 1.458643, adv: 0.388353, recon: 0.030243, id: 0.027929] time: 0:01:44.952504 \n",
            "[Epoch 17/200] [Batch 11/48] [D loss: 0.235312, acc:  57%] [G loss: 1.484452, adv: 0.400425, recon: 0.030269, id: 0.028350] time: 0:01:45.064881 \n",
            "[Epoch 17/200] [Batch 12/48] [D loss: 0.249897, acc:  52%] [G loss: 1.479897, adv: 0.388342, recon: 0.031044, id: 0.031315] time: 0:01:45.174453 \n",
            "[Epoch 17/200] [Batch 13/48] [D loss: 0.241549, acc:  53%] [G loss: 1.455769, adv: 0.387130, recon: 0.030174, id: 0.028092] time: 0:01:45.285509 \n",
            "[Epoch 17/200] [Batch 14/48] [D loss: 0.226883, acc:  58%] [G loss: 1.474757, adv: 0.404060, recon: 0.029543, id: 0.026763] time: 0:01:45.396137 \n",
            "[Epoch 17/200] [Batch 15/48] [D loss: 0.242531, acc:  54%] [G loss: 1.469541, adv: 0.388435, recon: 0.030576, id: 0.030649] time: 0:01:45.506923 \n",
            "[Epoch 17/200] [Batch 16/48] [D loss: 0.231046, acc:  59%] [G loss: 1.430202, adv: 0.401646, recon: 0.027610, id: 0.026521] time: 0:01:45.617499 \n",
            "[Epoch 17/200] [Batch 17/48] [D loss: 0.232468, acc:  58%] [G loss: 1.451158, adv: 0.398064, recon: 0.028942, id: 0.029801] time: 0:01:45.732191 \n",
            "[Epoch 17/200] [Batch 18/48] [D loss: 0.236083, acc:  57%] [G loss: 1.476484, adv: 0.396068, recon: 0.030382, id: 0.027932] time: 0:01:45.845535 \n",
            "[Epoch 17/200] [Batch 19/48] [D loss: 0.229193, acc:  59%] [G loss: 1.536843, adv: 0.409019, recon: 0.031976, id: 0.029849] time: 0:01:45.957897 \n",
            "[Epoch 17/200] [Batch 20/48] [D loss: 0.243070, acc:  54%] [G loss: 1.433356, adv: 0.382936, recon: 0.029601, id: 0.030160] time: 0:01:46.071348 \n",
            "[Epoch 17/200] [Batch 21/48] [D loss: 0.257085, acc:  49%] [G loss: 1.477127, adv: 0.377161, recon: 0.032058, id: 0.031036] time: 0:01:46.182543 \n",
            "[Epoch 17/200] [Batch 22/48] [D loss: 0.228466, acc:  59%] [G loss: 1.493861, adv: 0.408533, recon: 0.029978, id: 0.027828] time: 0:01:46.297116 \n",
            "[Epoch 17/200] [Batch 23/48] [D loss: 0.233807, acc:  58%] [G loss: 1.529383, adv: 0.401374, recon: 0.032358, id: 0.028018] time: 0:01:46.408094 \n",
            "[Epoch 17/200] [Batch 24/48] [D loss: 0.230154, acc:  57%] [G loss: 1.478481, adv: 0.398584, recon: 0.030229, id: 0.026752] time: 0:01:46.523065 \n",
            "[Epoch 17/200] [Batch 25/48] [D loss: 0.241945, acc:  54%] [G loss: 1.463614, adv: 0.395112, recon: 0.029858, id: 0.027682] time: 0:01:46.637600 \n",
            "[Epoch 17/200] [Batch 26/48] [D loss: 0.246621, acc:  52%] [G loss: 1.461209, adv: 0.379081, recon: 0.031121, id: 0.029840] time: 0:01:46.750338 \n",
            "[Epoch 17/200] [Batch 27/48] [D loss: 0.231132, acc:  58%] [G loss: 1.483064, adv: 0.400810, recon: 0.030233, id: 0.029098] time: 0:01:46.862149 \n",
            "[Epoch 17/200] [Batch 28/48] [D loss: 0.237389, acc:  56%] [G loss: 1.495532, adv: 0.398367, recon: 0.031070, id: 0.027468] time: 0:01:46.976556 \n",
            "[Epoch 17/200] [Batch 29/48] [D loss: 0.226817, acc:  60%] [G loss: 1.510422, adv: 0.414660, recon: 0.030119, id: 0.027568] time: 0:01:47.087537 \n",
            "[Epoch 17/200] [Batch 30/48] [D loss: 0.238844, acc:  57%] [G loss: 1.487152, adv: 0.387475, recon: 0.031660, id: 0.028383] time: 0:01:47.198511 \n",
            "[Epoch 17/200] [Batch 31/48] [D loss: 0.240093, acc:  55%] [G loss: 1.464053, adv: 0.391331, recon: 0.030313, id: 0.027086] time: 0:01:47.310175 \n",
            "[Epoch 17/200] [Batch 32/48] [D loss: 0.232997, acc:  57%] [G loss: 1.527627, adv: 0.397183, recon: 0.032643, id: 0.030306] time: 0:01:47.424192 \n",
            "[Epoch 17/200] [Batch 33/48] [D loss: 0.238598, acc:  56%] [G loss: 1.507168, adv: 0.402787, recon: 0.031118, id: 0.029279] time: 0:01:47.535846 \n",
            "[Epoch 17/200] [Batch 34/48] [D loss: 0.231767, acc:  57%] [G loss: 1.481517, adv: 0.403448, recon: 0.029863, id: 0.028086] time: 0:01:47.648161 \n",
            "[Epoch 17/200] [Batch 35/48] [D loss: 0.240358, acc:  54%] [G loss: 1.451655, adv: 0.389901, recon: 0.029763, id: 0.027269] time: 0:01:47.759875 \n",
            "[Epoch 17/200] [Batch 36/48] [D loss: 0.252815, acc:  51%] [G loss: 1.484527, adv: 0.383412, recon: 0.031808, id: 0.029709] time: 0:01:47.871644 \n",
            "[Epoch 17/200] [Batch 37/48] [D loss: 0.231955, acc:  58%] [G loss: 1.493575, adv: 0.409375, recon: 0.029873, id: 0.028188] time: 0:01:47.981956 \n",
            "[Epoch 17/200] [Batch 38/48] [D loss: 0.246304, acc:  51%] [G loss: 1.441018, adv: 0.380921, recon: 0.030124, id: 0.027188] time: 0:01:48.092085 \n",
            "[Epoch 17/200] [Batch 39/48] [D loss: 0.241032, acc:  55%] [G loss: 1.464054, adv: 0.389410, recon: 0.030482, id: 0.028459] time: 0:01:48.204748 \n",
            "[Epoch 17/200] [Batch 40/48] [D loss: 0.247973, acc:  53%] [G loss: 1.502430, adv: 0.390778, recon: 0.032075, id: 0.028392] time: 0:01:48.317937 \n",
            "[Epoch 17/200] [Batch 41/48] [D loss: 0.233549, acc:  58%] [G loss: 1.458080, adv: 0.396765, recon: 0.029563, id: 0.026941] time: 0:01:48.430110 \n",
            "[Epoch 17/200] [Batch 42/48] [D loss: 0.241050, acc:  54%] [G loss: 1.480371, adv: 0.391571, recon: 0.031003, id: 0.029014] time: 0:01:48.541332 \n",
            "[Epoch 17/200] [Batch 43/48] [D loss: 0.236111, acc:  56%] [G loss: 1.486085, adv: 0.399816, recon: 0.030469, id: 0.028809] time: 0:01:48.654011 \n",
            "[Epoch 17/200] [Batch 44/48] [D loss: 0.230572, acc:  58%] [G loss: 1.484991, adv: 0.400357, recon: 0.030334, id: 0.029762] time: 0:01:48.765693 \n",
            "[Epoch 17/200] [Batch 45/48] [D loss: 0.235605, acc:  56%] [G loss: 1.486197, adv: 0.397982, recon: 0.030711, id: 0.027769] time: 0:01:48.876254 \n",
            "[Epoch 17/200] [Batch 46/48] [D loss: 0.247362, acc:  52%] [G loss: 1.469140, adv: 0.384749, recon: 0.031054, id: 0.027921] time: 0:01:48.987502 \n",
            "[Epoch 17/200] [Batch 47/48] [D loss: 0.239303, acc:  54%] [G loss: 1.439641, adv: 0.393057, recon: 0.029130, id: 0.024730] time: 0:01:49.102192 \n",
            "[Epoch 18/200] [Batch 0/48] [D loss: 0.250944, acc:  51%] [G loss: 1.482355, adv: 0.382824, recon: 0.031857, id: 0.029804] time: 0:01:49.216131 \n",
            "[Epoch 18/200] [Batch 1/48] [D loss: 0.233344, acc:  57%] [G loss: 1.463469, adv: 0.402109, recon: 0.029217, id: 0.028808] time: 0:01:49.327785 \n",
            "[Epoch 18/200] [Batch 2/48] [D loss: 0.249042, acc:  51%] [G loss: 1.496887, adv: 0.377807, recon: 0.033176, id: 0.028869] time: 0:01:49.438865 \n",
            "[Epoch 18/200] [Batch 3/48] [D loss: 0.232941, acc:  57%] [G loss: 1.530517, adv: 0.411423, recon: 0.031675, id: 0.026916] time: 0:01:49.551017 \n",
            "[Epoch 18/200] [Batch 4/48] [D loss: 0.237145, acc:  56%] [G loss: 1.543558, adv: 0.395374, recon: 0.033525, id: 0.030677] time: 0:01:49.660902 \n",
            "[Epoch 18/200] [Batch 5/48] [D loss: 0.242850, acc:  54%] [G loss: 1.488695, adv: 0.390141, recon: 0.031604, id: 0.026656] time: 0:01:49.775436 \n",
            "[Epoch 18/200] [Batch 6/48] [D loss: 0.246527, acc:  52%] [G loss: 1.482195, adv: 0.389662, recon: 0.031125, id: 0.029278] time: 0:01:49.888777 \n",
            "[Epoch 18/200] [Batch 7/48] [D loss: 0.247551, acc:  52%] [G loss: 1.447074, adv: 0.382747, recon: 0.030201, id: 0.029080] time: 0:01:50.001583 \n",
            "[Epoch 18/200] [Batch 8/48] [D loss: 0.232577, acc:  57%] [G loss: 1.455178, adv: 0.392620, recon: 0.029595, id: 0.028226] time: 0:01:50.112302 \n",
            "[Epoch 18/200] [Batch 9/48] [D loss: 0.243910, acc:  53%] [G loss: 1.450978, adv: 0.389374, recon: 0.029846, id: 0.026861] time: 0:01:50.223469 \n",
            "[Epoch 18/200] [Batch 10/48] [D loss: 0.247273, acc:  52%] [G loss: 1.472139, adv: 0.384980, recon: 0.031284, id: 0.027615] time: 0:01:50.333943 \n",
            "[Epoch 18/200] [Batch 11/48] [D loss: 0.233428, acc:  58%] [G loss: 1.495920, adv: 0.405749, recon: 0.030365, id: 0.028131] time: 0:01:50.445937 \n",
            "[Epoch 18/200] [Batch 12/48] [D loss: 0.251593, acc:  51%] [G loss: 1.482753, adv: 0.386042, recon: 0.031435, id: 0.031252] time: 0:01:50.558176 \n",
            "[Epoch 18/200] [Batch 13/48] [D loss: 0.241597, acc:  53%] [G loss: 1.461687, adv: 0.389328, recon: 0.030293, id: 0.027783] time: 0:01:50.672107 \n",
            "[Epoch 18/200] [Batch 14/48] [D loss: 0.228937, acc:  57%] [G loss: 1.476203, adv: 0.403378, recon: 0.029716, id: 0.026696] time: 0:01:50.792438 \n",
            "[Epoch 18/200] [Batch 15/48] [D loss: 0.243899, acc:  53%] [G loss: 1.465374, adv: 0.384739, recon: 0.030776, id: 0.030210] time: 0:01:50.909523 \n",
            "[Epoch 18/200] [Batch 16/48] [D loss: 0.231860, acc:  59%] [G loss: 1.433488, adv: 0.402799, recon: 0.027692, id: 0.026301] time: 0:01:51.022149 \n",
            "[Epoch 18/200] [Batch 17/48] [D loss: 0.236209, acc:  56%] [G loss: 1.447053, adv: 0.392956, recon: 0.029285, id: 0.029472] time: 0:01:51.132305 \n",
            "[Epoch 18/200] [Batch 18/48] [D loss: 0.236667, acc:  56%] [G loss: 1.464008, adv: 0.394109, recon: 0.029988, id: 0.027900] time: 0:01:51.242561 \n",
            "[Epoch 18/200] [Batch 19/48] [D loss: 0.232458, acc:  58%] [G loss: 1.502061, adv: 0.401752, recon: 0.031034, id: 0.028834] time: 0:01:51.353536 \n",
            "[Epoch 18/200] [Batch 20/48] [D loss: 0.240680, acc:  55%] [G loss: 1.415462, adv: 0.387734, recon: 0.028275, id: 0.030007] time: 0:01:51.463433 \n",
            "[Epoch 18/200] [Batch 21/48] [D loss: 0.260975, acc:  47%] [G loss: 1.467031, adv: 0.373495, recon: 0.031940, id: 0.030706] time: 0:01:51.575219 \n",
            "[Epoch 18/200] [Batch 22/48] [D loss: 0.228404, acc:  59%] [G loss: 1.478420, adv: 0.410259, recon: 0.029087, id: 0.027497] time: 0:01:51.687486 \n",
            "[Epoch 18/200] [Batch 23/48] [D loss: 0.237834, acc:  56%] [G loss: 1.523326, adv: 0.393374, recon: 0.032872, id: 0.027929] time: 0:01:51.804870 \n",
            "[Epoch 18/200] [Batch 24/48] [D loss: 0.228921, acc:  58%] [G loss: 1.474869, adv: 0.403090, recon: 0.029659, id: 0.026406] time: 0:01:51.917697 \n",
            "[Epoch 18/200] [Batch 25/48] [D loss: 0.246653, acc:  52%] [G loss: 1.458717, adv: 0.390096, recon: 0.030096, id: 0.027652] time: 0:01:52.029661 \n",
            "[Epoch 18/200] [Batch 26/48] [D loss: 0.247075, acc:  53%] [G loss: 1.459974, adv: 0.378887, recon: 0.031154, id: 0.029304] time: 0:01:52.141402 \n",
            "[Epoch 18/200] [Batch 27/48] [D loss: 0.232763, acc:  57%] [G loss: 1.483403, adv: 0.403043, recon: 0.030014, id: 0.029111] time: 0:01:52.252515 \n",
            "[Epoch 18/200] [Batch 28/48] [D loss: 0.237952, acc:  56%] [G loss: 1.477038, adv: 0.396868, recon: 0.030389, id: 0.026500] time: 0:01:52.363839 \n",
            "[Epoch 18/200] [Batch 29/48] [D loss: 0.231934, acc:  58%] [G loss: 1.502897, adv: 0.406715, recon: 0.030504, id: 0.027831] time: 0:01:52.477753 \n",
            "[Epoch 18/200] [Batch 30/48] [D loss: 0.234310, acc:  59%] [G loss: 1.475632, adv: 0.398271, recon: 0.030116, id: 0.027625] time: 0:01:52.593223 \n",
            "[Epoch 18/200] [Batch 31/48] [D loss: 0.251771, acc:  50%] [G loss: 1.435776, adv: 0.372214, recon: 0.030741, id: 0.027723] time: 0:01:52.705523 \n",
            "[Epoch 18/200] [Batch 32/48] [D loss: 0.227477, acc:  59%] [G loss: 1.519149, adv: 0.407962, recon: 0.031249, id: 0.030238] time: 0:01:52.821704 \n",
            "[Epoch 18/200] [Batch 33/48] [D loss: 0.248124, acc:  53%] [G loss: 1.540036, adv: 0.392839, recon: 0.033686, id: 0.028935] time: 0:01:52.932574 \n",
            "[Epoch 18/200] [Batch 34/48] [D loss: 0.228137, acc:  59%] [G loss: 1.497345, adv: 0.415680, recon: 0.029550, id: 0.027337] time: 0:01:53.043693 \n",
            "[Epoch 18/200] [Batch 35/48] [D loss: 0.245393, acc:  52%] [G loss: 1.444935, adv: 0.379596, recon: 0.030422, id: 0.028093] time: 0:01:53.156501 \n",
            "[Epoch 18/200] [Batch 36/48] [D loss: 0.246696, acc:  53%] [G loss: 1.487888, adv: 0.391020, recon: 0.031337, id: 0.029027] time: 0:01:53.268492 \n",
            "[Epoch 18/200] [Batch 37/48] [D loss: 0.233748, acc:  57%] [G loss: 1.494023, adv: 0.402016, recon: 0.030593, id: 0.028762] time: 0:01:53.383219 \n",
            "[Epoch 18/200] [Batch 38/48] [D loss: 0.246350, acc:  51%] [G loss: 1.438876, adv: 0.374813, recon: 0.030687, id: 0.026188] time: 0:01:53.494761 \n",
            "[Epoch 18/200] [Batch 39/48] [D loss: 0.238173, acc:  57%] [G loss: 1.468261, adv: 0.394471, recon: 0.030195, id: 0.028368] time: 0:01:53.606971 \n",
            "[Epoch 18/200] [Batch 40/48] [D loss: 0.247464, acc:  52%] [G loss: 1.472024, adv: 0.388893, recon: 0.030827, id: 0.027740] time: 0:01:53.717927 \n",
            "[Epoch 18/200] [Batch 41/48] [D loss: 0.238021, acc:  56%] [G loss: 1.415700, adv: 0.384233, recon: 0.028699, id: 0.026933] time: 0:01:53.830829 \n",
            "[Epoch 18/200] [Batch 42/48] [D loss: 0.240423, acc:  54%] [G loss: 1.447295, adv: 0.390012, recon: 0.029567, id: 0.028532] time: 0:01:53.943149 \n",
            "[Epoch 18/200] [Batch 43/48] [D loss: 0.236955, acc:  56%] [G loss: 1.471983, adv: 0.398342, recon: 0.029903, id: 0.028886] time: 0:01:54.053707 \n",
            "[Epoch 18/200] [Batch 44/48] [D loss: 0.234435, acc:  57%] [G loss: 1.491611, adv: 0.392617, recon: 0.031448, id: 0.029635] time: 0:01:54.169290 \n",
            "[Epoch 18/200] [Batch 45/48] [D loss: 0.229002, acc:  59%] [G loss: 1.519903, adv: 0.410664, recon: 0.031186, id: 0.027998] time: 0:01:54.281990 \n",
            "[Epoch 18/200] [Batch 46/48] [D loss: 0.243299, acc:  53%] [G loss: 1.488832, adv: 0.390193, recon: 0.031484, id: 0.028528] time: 0:01:54.394503 \n",
            "[Epoch 18/200] [Batch 47/48] [D loss: 0.240767, acc:  53%] [G loss: 1.429023, adv: 0.382146, recon: 0.029696, id: 0.025053] time: 0:01:54.505345 \n",
            "[Epoch 19/200] [Batch 0/48] [D loss: 0.245716, acc:  53%] [G loss: 1.487512, adv: 0.385414, recon: 0.031896, id: 0.029862] time: 0:01:54.618154 \n",
            "[Epoch 19/200] [Batch 1/48] [D loss: 0.233572, acc:  57%] [G loss: 1.456667, adv: 0.399154, recon: 0.029193, id: 0.028577] time: 0:01:54.730275 \n",
            "[Epoch 19/200] [Batch 2/48] [D loss: 0.248918, acc:  51%] [G loss: 1.483520, adv: 0.375848, recon: 0.032766, id: 0.028376] time: 0:01:54.843670 \n",
            "[Epoch 19/200] [Batch 3/48] [D loss: 0.230300, acc:  59%] [G loss: 1.533466, adv: 0.415569, recon: 0.031452, id: 0.026973] time: 0:01:54.957337 \n",
            "[Epoch 19/200] [Batch 4/48] [D loss: 0.233966, acc:  57%] [G loss: 1.511052, adv: 0.396122, recon: 0.031888, id: 0.030148] time: 0:01:55.071088 \n",
            "[Epoch 19/200] [Batch 5/48] [D loss: 0.239950, acc:  55%] [G loss: 1.481714, adv: 0.391069, recon: 0.031187, id: 0.026738] time: 0:01:55.184009 \n",
            "[Epoch 19/200] [Batch 6/48] [D loss: 0.246342, acc:  52%] [G loss: 1.477147, adv: 0.384899, recon: 0.031361, id: 0.029548] time: 0:01:55.298193 \n",
            "[Epoch 19/200] [Batch 7/48] [D loss: 0.245281, acc:  52%] [G loss: 1.446504, adv: 0.384009, recon: 0.030105, id: 0.028862] time: 0:01:55.407316 \n",
            "[Epoch 19/200] [Batch 8/48] [D loss: 0.235233, acc:  55%] [G loss: 1.460901, adv: 0.385722, recon: 0.030569, id: 0.028258] time: 0:01:55.518301 \n",
            "[Epoch 19/200] [Batch 9/48] [D loss: 0.238498, acc:  55%] [G loss: 1.463218, adv: 0.396315, recon: 0.029826, id: 0.026876] time: 0:01:55.628572 \n",
            "[Epoch 19/200] [Batch 10/48] [D loss: 0.244172, acc:  53%] [G loss: 1.450563, adv: 0.385682, recon: 0.030155, id: 0.028185] time: 0:01:55.739432 \n",
            "[Epoch 19/200] [Batch 11/48] [D loss: 0.238312, acc:  56%] [G loss: 1.466334, adv: 0.391923, recon: 0.030278, id: 0.027759] time: 0:01:55.851663 \n",
            "[Epoch 19/200] [Batch 12/48] [D loss: 0.249936, acc:  51%] [G loss: 1.471401, adv: 0.388517, recon: 0.030670, id: 0.031181] time: 0:01:55.966563 \n",
            "[Epoch 19/200] [Batch 13/48] [D loss: 0.242477, acc:  52%] [G loss: 1.450415, adv: 0.387776, recon: 0.029905, id: 0.027537] time: 0:01:56.079493 \n",
            "[Epoch 19/200] [Batch 14/48] [D loss: 0.229146, acc:  57%] [G loss: 1.476843, adv: 0.403080, recon: 0.029819, id: 0.026414] time: 0:01:56.192603 \n",
            "[Epoch 19/200] [Batch 15/48] [D loss: 0.243376, acc:  54%] [G loss: 1.469118, adv: 0.387158, recon: 0.030735, id: 0.030602] time: 0:01:56.304108 \n",
            "[Epoch 19/200] [Batch 16/48] [D loss: 0.231917, acc:  59%] [G loss: 1.427473, adv: 0.400422, recon: 0.027680, id: 0.026027] time: 0:01:56.417602 \n",
            "[Epoch 19/200] [Batch 17/48] [D loss: 0.237661, acc:  56%] [G loss: 1.481135, adv: 0.390469, recon: 0.031243, id: 0.029588] time: 0:01:56.531227 \n",
            "[Epoch 19/200] [Batch 18/48] [D loss: 0.230513, acc:  59%] [G loss: 1.510702, adv: 0.404600, recon: 0.031345, id: 0.027718] time: 0:01:56.642462 \n",
            "[Epoch 19/200] [Batch 19/48] [D loss: 0.224336, acc:  61%] [G loss: 1.503055, adv: 0.410322, recon: 0.030261, id: 0.029712] time: 0:01:56.758119 \n",
            "[Epoch 19/200] [Batch 20/48] [D loss: 0.239325, acc:  55%] [G loss: 1.407863, adv: 0.382082, recon: 0.028488, id: 0.029583] time: 0:01:56.871172 \n",
            "[Epoch 19/200] [Batch 21/48] [D loss: 0.255481, acc:  49%] [G loss: 1.458054, adv: 0.376929, recon: 0.031225, id: 0.030536] time: 0:01:56.982636 \n",
            "[Epoch 19/200] [Batch 22/48] [D loss: 0.229646, acc:  58%] [G loss: 1.491144, adv: 0.406548, recon: 0.030114, id: 0.027344] time: 0:01:57.096088 \n",
            "[Epoch 19/200] [Batch 23/48] [D loss: 0.235893, acc:  57%] [G loss: 1.530572, adv: 0.395041, recon: 0.033105, id: 0.027586] time: 0:01:57.208985 \n",
            "[Epoch 19/200] [Batch 24/48] [D loss: 0.230356, acc:  57%] [G loss: 1.485646, adv: 0.396237, recon: 0.030883, id: 0.026669] time: 0:01:57.322594 \n",
            "[Epoch 19/200] [Batch 25/48] [D loss: 0.240603, acc:  55%] [G loss: 1.472449, adv: 0.398146, recon: 0.030035, id: 0.027660] time: 0:01:57.434339 \n",
            "[Epoch 19/200] [Batch 26/48] [D loss: 0.247630, acc:  52%] [G loss: 1.437266, adv: 0.373092, recon: 0.030628, id: 0.029318] time: 0:01:57.548855 \n",
            "[Epoch 19/200] [Batch 27/48] [D loss: 0.230818, acc:  58%] [G loss: 1.459929, adv: 0.396541, recon: 0.029552, id: 0.028603] time: 0:01:57.661085 \n",
            "[Epoch 19/200] [Batch 28/48] [D loss: 0.237048, acc:  56%] [G loss: 1.469834, adv: 0.396005, recon: 0.030121, id: 0.026606] time: 0:01:57.775634 \n",
            "[Epoch 19/200] [Batch 29/48] [D loss: 0.224877, acc:  61%] [G loss: 1.499905, adv: 0.417514, recon: 0.029379, id: 0.027425] time: 0:01:57.888950 \n",
            "[Epoch 19/200] [Batch 30/48] [D loss: 0.239735, acc:  57%] [G loss: 1.488127, adv: 0.384779, recon: 0.032053, id: 0.027845] time: 0:01:57.999873 \n",
            "[Epoch 19/200] [Batch 31/48] [D loss: 0.237432, acc:  56%] [G loss: 1.471369, adv: 0.396430, recon: 0.030231, id: 0.026992] time: 0:01:58.110917 \n",
            "[Epoch 19/200] [Batch 32/48] [D loss: 0.226302, acc:  60%] [G loss: 1.515704, adv: 0.405889, recon: 0.031272, id: 0.030354] time: 0:01:58.223944 \n",
            "[Epoch 19/200] [Batch 33/48] [D loss: 0.237601, acc:  56%] [G loss: 1.484991, adv: 0.397640, recon: 0.030577, id: 0.028779] time: 0:01:58.333767 \n",
            "[Epoch 19/200] [Batch 34/48] [D loss: 0.232129, acc:  57%] [G loss: 1.457449, adv: 0.396865, recon: 0.029388, id: 0.027674] time: 0:01:58.449387 \n",
            "[Epoch 19/200] [Batch 35/48] [D loss: 0.238234, acc:  55%] [G loss: 1.438592, adv: 0.391067, recon: 0.029051, id: 0.027253] time: 0:01:58.561740 \n",
            "[Epoch 19/200] [Batch 36/48] [D loss: 0.250377, acc:  52%] [G loss: 1.477063, adv: 0.385976, recon: 0.031254, id: 0.029375] time: 0:01:58.673202 \n",
            "[Epoch 19/200] [Batch 37/48] [D loss: 0.230367, acc:  58%] [G loss: 1.489475, adv: 0.409345, recon: 0.029725, id: 0.027940] time: 0:01:58.784748 \n",
            "[Epoch 19/200] [Batch 38/48] [D loss: 0.243283, acc:  52%] [G loss: 1.439911, adv: 0.381998, recon: 0.030017, id: 0.027292] time: 0:01:58.899063 \n",
            "[Epoch 19/200] [Batch 39/48] [D loss: 0.237152, acc:  57%] [G loss: 1.477754, adv: 0.393625, recon: 0.030806, id: 0.028166] time: 0:01:59.009808 \n",
            "[Epoch 19/200] [Batch 40/48] [D loss: 0.244262, acc:  53%] [G loss: 1.521280, adv: 0.392811, recon: 0.032856, id: 0.028507] time: 0:01:59.121421 \n",
            "[Epoch 19/200] [Batch 41/48] [D loss: 0.231800, acc:  59%] [G loss: 1.464698, adv: 0.395589, recon: 0.030058, id: 0.026918] time: 0:01:59.235616 \n",
            "[Epoch 19/200] [Batch 42/48] [D loss: 0.234176, acc:  57%] [G loss: 1.460485, adv: 0.397034, recon: 0.029558, id: 0.029046] time: 0:01:59.347957 \n",
            "[Epoch 19/200] [Batch 43/48] [D loss: 0.236563, acc:  56%] [G loss: 1.458963, adv: 0.393153, recon: 0.029819, id: 0.028536] time: 0:01:59.459831 \n",
            "[Epoch 19/200] [Batch 44/48] [D loss: 0.226387, acc:  59%] [G loss: 1.465225, adv: 0.400264, recon: 0.029447, id: 0.029206] time: 0:01:59.571926 \n",
            "[Epoch 19/200] [Batch 45/48] [D loss: 0.233911, acc:  56%] [G loss: 1.460394, adv: 0.395991, recon: 0.029679, id: 0.027888] time: 0:01:59.685642 \n",
            "[Epoch 19/200] [Batch 46/48] [D loss: 0.242527, acc:  53%] [G loss: 1.456408, adv: 0.389351, recon: 0.030035, id: 0.027567] time: 0:01:59.796872 \n",
            "[Epoch 19/200] [Batch 47/48] [D loss: 0.241207, acc:  53%] [G loss: 1.411656, adv: 0.382080, recon: 0.028850, id: 0.025115] time: 0:01:59.909189 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5Cg24EREjmIDTkTkqP8HvopRlW2Nu5QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 20/200] [Batch 0/48] [D loss: 0.239327, acc:  56%] [G loss: 1.472860, adv: 0.397075, recon: 0.030094, id: 0.029454] time: 0:02:00.297274 \n",
            "[Epoch 20/200] [Batch 1/48] [D loss: 0.240297, acc:  54%] [G loss: 1.444344, adv: 0.385256, recon: 0.029924, id: 0.029085] time: 0:02:00.408847 \n",
            "[Epoch 20/200] [Batch 2/48] [D loss: 0.235207, acc:  56%] [G loss: 1.450691, adv: 0.392692, recon: 0.029565, id: 0.028124] time: 0:02:00.520084 \n",
            "[Epoch 20/200] [Batch 3/48] [D loss: 0.239523, acc:  54%] [G loss: 1.479975, adv: 0.394189, recon: 0.030820, id: 0.027652] time: 0:02:00.632142 \n",
            "[Epoch 20/200] [Batch 4/48] [D loss: 0.235804, acc:  57%] [G loss: 1.498609, adv: 0.392465, recon: 0.031738, id: 0.028920] time: 0:02:00.742771 \n",
            "[Epoch 20/200] [Batch 5/48] [D loss: 0.240227, acc:  55%] [G loss: 1.475243, adv: 0.397126, recon: 0.030279, id: 0.027089] time: 0:02:00.854167 \n",
            "[Epoch 20/200] [Batch 6/48] [D loss: 0.243833, acc:  53%] [G loss: 1.474806, adv: 0.392045, recon: 0.030568, id: 0.029433] time: 0:02:00.969071 \n",
            "[Epoch 20/200] [Batch 7/48] [D loss: 0.246965, acc:  52%] [G loss: 1.434595, adv: 0.380634, recon: 0.029845, id: 0.028938] time: 0:02:01.081181 \n",
            "[Epoch 20/200] [Batch 8/48] [D loss: 0.227221, acc:  59%] [G loss: 1.441662, adv: 0.396906, recon: 0.028562, id: 0.028460] time: 0:02:01.192608 \n",
            "[Epoch 20/200] [Batch 9/48] [D loss: 0.243676, acc:  53%] [G loss: 1.438041, adv: 0.384334, recon: 0.029752, id: 0.026962] time: 0:02:01.304980 \n",
            "[Epoch 20/200] [Batch 10/48] [D loss: 0.241673, acc:  54%] [G loss: 1.449467, adv: 0.391156, recon: 0.029604, id: 0.027888] time: 0:02:01.415141 \n",
            "[Epoch 20/200] [Batch 11/48] [D loss: 0.233489, acc:  58%] [G loss: 1.473821, adv: 0.402957, recon: 0.029608, id: 0.027723] time: 0:02:01.525535 \n",
            "[Epoch 20/200] [Batch 12/48] [D loss: 0.250240, acc:  51%] [G loss: 1.497189, adv: 0.386627, recon: 0.032114, id: 0.031806] time: 0:02:01.638721 \n",
            "[Epoch 20/200] [Batch 13/48] [D loss: 0.236004, acc:  56%] [G loss: 1.485467, adv: 0.399339, recon: 0.030580, id: 0.027481] time: 0:02:01.750150 \n",
            "[Epoch 20/200] [Batch 14/48] [D loss: 0.227172, acc:  58%] [G loss: 1.503163, adv: 0.403830, recon: 0.031033, id: 0.027689] time: 0:02:01.865649 \n",
            "[Epoch 20/200] [Batch 15/48] [D loss: 0.238806, acc:  56%] [G loss: 1.491675, adv: 0.388699, recon: 0.031780, id: 0.030281] time: 0:02:01.980699 \n",
            "[Epoch 20/200] [Batch 16/48] [D loss: 0.229438, acc:  60%] [G loss: 1.440809, adv: 0.401135, recon: 0.028258, id: 0.027217] time: 0:02:02.093882 \n",
            "[Epoch 20/200] [Batch 17/48] [D loss: 0.229043, acc:  59%] [G loss: 1.447483, adv: 0.399692, recon: 0.028732, id: 0.029259] time: 0:02:02.209008 \n",
            "[Epoch 20/200] [Batch 18/48] [D loss: 0.236283, acc:  57%] [G loss: 1.463799, adv: 0.390180, recon: 0.030402, id: 0.028241] time: 0:02:02.323054 \n",
            "[Epoch 20/200] [Batch 19/48] [D loss: 0.223559, acc:  61%] [G loss: 1.508950, adv: 0.411774, recon: 0.030495, id: 0.028762] time: 0:02:02.435698 \n",
            "[Epoch 20/200] [Batch 20/48] [D loss: 0.240402, acc:  55%] [G loss: 1.430773, adv: 0.383245, recon: 0.029485, id: 0.030204] time: 0:02:02.547240 \n",
            "[Epoch 20/200] [Batch 21/48] [D loss: 0.248100, acc:  52%] [G loss: 1.502138, adv: 0.390206, recon: 0.032156, id: 0.030737] time: 0:02:02.662000 \n",
            "[Epoch 20/200] [Batch 22/48] [D loss: 0.227419, acc:  59%] [G loss: 1.504206, adv: 0.407949, recon: 0.030598, id: 0.028156] time: 0:02:02.774820 \n",
            "[Epoch 20/200] [Batch 23/48] [D loss: 0.229118, acc:  60%] [G loss: 1.510166, adv: 0.402780, recon: 0.031361, id: 0.027474] time: 0:02:02.886555 \n",
            "[Epoch 20/200] [Batch 24/48] [D loss: 0.228426, acc:  58%] [G loss: 1.464843, adv: 0.395635, recon: 0.029908, id: 0.027288] time: 0:02:02.999156 \n",
            "[Epoch 20/200] [Batch 25/48] [D loss: 0.238834, acc:  56%] [G loss: 1.472196, adv: 0.399463, recon: 0.029941, id: 0.027468] time: 0:02:03.109997 \n",
            "[Epoch 20/200] [Batch 26/48] [D loss: 0.244472, acc:  54%] [G loss: 1.462415, adv: 0.380728, recon: 0.031100, id: 0.029743] time: 0:02:03.223710 \n",
            "[Epoch 20/200] [Batch 27/48] [D loss: 0.225913, acc:  60%] [G loss: 1.485687, adv: 0.406296, recon: 0.029905, id: 0.028858] time: 0:02:03.335311 \n",
            "[Epoch 20/200] [Batch 28/48] [D loss: 0.233516, acc:  58%] [G loss: 1.479580, adv: 0.401057, recon: 0.030099, id: 0.027323] time: 0:02:03.447294 \n",
            "[Epoch 20/200] [Batch 29/48] [D loss: 0.223022, acc:  61%] [G loss: 1.496984, adv: 0.416261, recon: 0.029394, id: 0.027406] time: 0:02:03.562158 \n",
            "[Epoch 20/200] [Batch 30/48] [D loss: 0.233438, acc:  59%] [G loss: 1.470414, adv: 0.392997, recon: 0.030401, id: 0.027997] time: 0:02:03.675191 \n",
            "[Epoch 20/200] [Batch 31/48] [D loss: 0.237487, acc:  56%] [G loss: 1.449193, adv: 0.394294, recon: 0.029341, id: 0.027116] time: 0:02:03.787706 \n",
            "[Epoch 20/200] [Batch 32/48] [D loss: 0.227305, acc:  59%] [G loss: 1.500028, adv: 0.402983, recon: 0.030809, id: 0.030073] time: 0:02:03.903414 \n",
            "[Epoch 20/200] [Batch 33/48] [D loss: 0.236204, acc:  56%] [G loss: 1.502842, adv: 0.402262, recon: 0.031020, id: 0.029018] time: 0:02:04.017652 \n",
            "[Epoch 20/200] [Batch 34/48] [D loss: 0.228805, acc:  58%] [G loss: 1.472291, adv: 0.406114, recon: 0.029248, id: 0.027726] time: 0:02:04.132059 \n",
            "[Epoch 20/200] [Batch 35/48] [D loss: 0.237033, acc:  56%] [G loss: 1.448287, adv: 0.393048, recon: 0.029357, id: 0.027539] time: 0:02:04.242057 \n",
            "[Epoch 20/200] [Batch 36/48] [D loss: 0.249412, acc:  52%] [G loss: 1.486284, adv: 0.384664, recon: 0.031902, id: 0.029086] time: 0:02:04.353947 \n",
            "[Epoch 20/200] [Batch 37/48] [D loss: 0.227366, acc:  59%] [G loss: 1.500959, adv: 0.408258, recon: 0.030412, id: 0.028464] time: 0:02:04.467787 \n",
            "[Epoch 20/200] [Batch 38/48] [D loss: 0.240113, acc:  54%] [G loss: 1.438572, adv: 0.381945, recon: 0.030036, id: 0.026785] time: 0:02:04.579630 \n",
            "[Epoch 20/200] [Batch 39/48] [D loss: 0.236462, acc:  57%] [G loss: 1.461209, adv: 0.390042, recon: 0.030339, id: 0.028687] time: 0:02:04.692514 \n",
            "[Epoch 20/200] [Batch 40/48] [D loss: 0.238182, acc:  56%] [G loss: 1.478896, adv: 0.400352, recon: 0.030106, id: 0.027606] time: 0:02:04.804517 \n",
            "[Epoch 20/200] [Batch 41/48] [D loss: 0.233799, acc:  58%] [G loss: 1.418040, adv: 0.389023, recon: 0.028388, id: 0.026970] time: 0:02:04.916011 \n",
            "[Epoch 20/200] [Batch 42/48] [D loss: 0.232948, acc:  57%] [G loss: 1.438869, adv: 0.397980, recon: 0.028433, id: 0.028542] time: 0:02:05.025173 \n",
            "[Epoch 20/200] [Batch 43/48] [D loss: 0.235214, acc:  56%] [G loss: 1.460159, adv: 0.397573, recon: 0.029429, id: 0.028826] time: 0:02:05.136747 \n",
            "[Epoch 20/200] [Batch 44/48] [D loss: 0.223865, acc:  61%] [G loss: 1.464420, adv: 0.403824, recon: 0.029085, id: 0.029311] time: 0:02:05.251815 \n",
            "[Epoch 20/200] [Batch 45/48] [D loss: 0.232265, acc:  57%] [G loss: 1.465718, adv: 0.399165, recon: 0.029646, id: 0.028057] time: 0:02:05.364036 \n",
            "[Epoch 20/200] [Batch 46/48] [D loss: 0.238150, acc:  56%] [G loss: 1.455101, adv: 0.398217, recon: 0.029135, id: 0.027377] time: 0:02:05.476389 \n",
            "[Epoch 20/200] [Batch 47/48] [D loss: 0.239035, acc:  54%] [G loss: 1.417412, adv: 0.386982, recon: 0.028647, id: 0.025752] time: 0:02:05.588430 \n",
            "[Epoch 21/200] [Batch 0/48] [D loss: 0.237558, acc:  57%] [G loss: 1.482400, adv: 0.396762, recon: 0.030653, id: 0.029085] time: 0:02:05.701702 \n",
            "[Epoch 21/200] [Batch 1/48] [D loss: 0.234351, acc:  56%] [G loss: 1.485674, adv: 0.393894, recon: 0.031107, id: 0.029929] time: 0:02:05.813777 \n",
            "[Epoch 21/200] [Batch 2/48] [D loss: 0.232793, acc:  57%] [G loss: 1.459242, adv: 0.395555, recon: 0.029746, id: 0.027924] time: 0:02:05.925953 \n",
            "[Epoch 21/200] [Batch 3/48] [D loss: 0.236467, acc:  56%] [G loss: 1.481467, adv: 0.397666, recon: 0.030563, id: 0.028290] time: 0:02:06.038480 \n",
            "[Epoch 21/200] [Batch 4/48] [D loss: 0.232088, acc:  58%] [G loss: 1.499567, adv: 0.398942, recon: 0.031181, id: 0.028621] time: 0:02:06.152398 \n",
            "[Epoch 21/200] [Batch 5/48] [D loss: 0.239913, acc:  56%] [G loss: 1.469318, adv: 0.393851, recon: 0.030317, id: 0.027481] time: 0:02:06.264588 \n",
            "[Epoch 21/200] [Batch 6/48] [D loss: 0.240464, acc:  55%] [G loss: 1.484058, adv: 0.394996, recon: 0.030770, id: 0.029345] time: 0:02:06.377206 \n",
            "[Epoch 21/200] [Batch 7/48] [D loss: 0.244319, acc:  53%] [G loss: 1.451184, adv: 0.385511, recon: 0.030183, id: 0.029392] time: 0:02:06.490518 \n",
            "[Epoch 21/200] [Batch 8/48] [D loss: 0.222871, acc:  61%] [G loss: 1.471727, adv: 0.405395, recon: 0.029243, id: 0.028625] time: 0:02:06.602173 \n",
            "[Epoch 21/200] [Batch 9/48] [D loss: 0.243209, acc:  53%] [G loss: 1.463833, adv: 0.380898, recon: 0.031386, id: 0.027427] time: 0:02:06.725807 \n",
            "[Epoch 21/200] [Batch 10/48] [D loss: 0.240150, acc:  55%] [G loss: 1.499030, adv: 0.389498, recon: 0.032256, id: 0.027942] time: 0:02:06.838350 \n",
            "[Epoch 21/200] [Batch 11/48] [D loss: 0.228133, acc:  60%] [G loss: 1.519886, adv: 0.410333, recon: 0.031197, id: 0.028218] time: 0:02:06.952826 \n",
            "[Epoch 21/200] [Batch 12/48] [D loss: 0.244319, acc:  54%] [G loss: 1.472865, adv: 0.391860, recon: 0.030465, id: 0.031006] time: 0:02:07.066125 \n",
            "[Epoch 21/200] [Batch 13/48] [D loss: 0.236604, acc:  55%] [G loss: 1.444018, adv: 0.390478, recon: 0.029391, id: 0.027688] time: 0:02:07.178825 \n",
            "[Epoch 21/200] [Batch 14/48] [D loss: 0.222881, acc:  60%] [G loss: 1.471422, adv: 0.407835, recon: 0.029139, id: 0.026631] time: 0:02:07.291754 \n",
            "[Epoch 21/200] [Batch 15/48] [D loss: 0.238432, acc:  56%] [G loss: 1.466265, adv: 0.390743, recon: 0.030308, id: 0.030106] time: 0:02:07.401676 \n",
            "[Epoch 21/200] [Batch 16/48] [D loss: 0.226280, acc:  61%] [G loss: 1.439711, adv: 0.406752, recon: 0.027728, id: 0.026148] time: 0:02:07.511946 \n",
            "[Epoch 21/200] [Batch 17/48] [D loss: 0.228041, acc:  59%] [G loss: 1.458612, adv: 0.401631, recon: 0.029092, id: 0.029412] time: 0:02:07.624451 \n",
            "[Epoch 21/200] [Batch 18/48] [D loss: 0.231524, acc:  59%] [G loss: 1.478008, adv: 0.398621, recon: 0.030322, id: 0.028001] time: 0:02:07.734662 \n",
            "[Epoch 21/200] [Batch 19/48] [D loss: 0.222923, acc:  61%] [G loss: 1.503143, adv: 0.412799, recon: 0.030103, id: 0.028963] time: 0:02:07.849726 \n",
            "[Epoch 21/200] [Batch 20/48] [D loss: 0.236048, acc:  57%] [G loss: 1.411398, adv: 0.389462, recon: 0.027974, id: 0.029764] time: 0:02:07.961970 \n",
            "[Epoch 21/200] [Batch 21/48] [D loss: 0.251335, acc:  51%] [G loss: 1.469984, adv: 0.385045, recon: 0.031045, id: 0.030705] time: 0:02:08.073630 \n",
            "[Epoch 21/200] [Batch 22/48] [D loss: 0.226613, acc:  59%] [G loss: 1.475823, adv: 0.409367, recon: 0.029112, id: 0.027605] time: 0:02:08.186251 \n",
            "[Epoch 21/200] [Batch 23/48] [D loss: 0.229200, acc:  60%] [G loss: 1.507830, adv: 0.402917, recon: 0.031244, id: 0.027669] time: 0:02:08.297268 \n",
            "[Epoch 21/200] [Batch 24/48] [D loss: 0.225845, acc:  59%] [G loss: 1.470802, adv: 0.402151, recon: 0.029581, id: 0.027050] time: 0:02:08.409273 \n",
            "[Epoch 21/200] [Batch 25/48] [D loss: 0.236965, acc:  56%] [G loss: 1.475111, adv: 0.404560, recon: 0.029604, id: 0.027481] time: 0:02:08.521373 \n",
            "[Epoch 21/200] [Batch 26/48] [D loss: 0.242065, acc:  55%] [G loss: 1.458936, adv: 0.384642, recon: 0.030560, id: 0.030056] time: 0:02:08.633507 \n",
            "[Epoch 21/200] [Batch 27/48] [D loss: 0.224316, acc:  61%] [G loss: 1.491983, adv: 0.407965, recon: 0.030072, id: 0.028891] time: 0:02:08.747304 \n",
            "[Epoch 21/200] [Batch 28/48] [D loss: 0.232428, acc:  59%] [G loss: 1.496522, adv: 0.401877, recon: 0.030860, id: 0.027727] time: 0:02:08.858331 \n",
            "[Epoch 21/200] [Batch 29/48] [D loss: 0.219943, acc:  63%] [G loss: 1.517746, adv: 0.421083, recon: 0.029987, id: 0.027416] time: 0:02:08.970198 \n",
            "[Epoch 21/200] [Batch 30/48] [D loss: 0.232694, acc:  59%] [G loss: 1.483587, adv: 0.393474, recon: 0.031013, id: 0.028202] time: 0:02:09.081256 \n",
            "[Epoch 21/200] [Batch 31/48] [D loss: 0.237156, acc:  57%] [G loss: 1.442903, adv: 0.391228, recon: 0.029364, id: 0.027009] time: 0:02:09.199510 \n",
            "[Epoch 21/200] [Batch 32/48] [D loss: 0.223447, acc:  61%] [G loss: 1.496520, adv: 0.406115, recon: 0.030361, id: 0.029969] time: 0:02:09.309267 \n",
            "[Epoch 21/200] [Batch 33/48] [D loss: 0.234095, acc:  57%] [G loss: 1.485855, adv: 0.402935, recon: 0.030145, id: 0.028823] time: 0:02:09.422649 \n",
            "[Epoch 21/200] [Batch 34/48] [D loss: 0.226766, acc:  59%] [G loss: 1.456320, adv: 0.406753, recon: 0.028402, id: 0.027746] time: 0:02:09.536019 \n",
            "[Epoch 21/200] [Batch 35/48] [D loss: 0.236282, acc:  56%] [G loss: 1.428981, adv: 0.391164, recon: 0.028608, id: 0.027346] time: 0:02:09.648887 \n",
            "[Epoch 21/200] [Batch 36/48] [D loss: 0.245522, acc:  54%] [G loss: 1.476504, adv: 0.390782, recon: 0.030804, id: 0.029444] time: 0:02:09.760636 \n",
            "[Epoch 21/200] [Batch 37/48] [D loss: 0.225955, acc:  60%] [G loss: 1.484433, adv: 0.412431, recon: 0.029200, id: 0.028158] time: 0:02:09.872816 \n",
            "[Epoch 21/200] [Batch 38/48] [D loss: 0.238663, acc:  54%] [G loss: 1.424314, adv: 0.385539, recon: 0.028981, id: 0.026792] time: 0:02:09.987342 \n",
            "[Epoch 21/200] [Batch 39/48] [D loss: 0.234568, acc:  58%] [G loss: 1.450989, adv: 0.393746, recon: 0.029492, id: 0.028228] time: 0:02:10.100144 \n",
            "[Epoch 21/200] [Batch 40/48] [D loss: 0.235874, acc:  57%] [G loss: 1.482068, adv: 0.405051, recon: 0.029804, id: 0.027870] time: 0:02:10.216816 \n",
            "[Epoch 21/200] [Batch 41/48] [D loss: 0.235390, acc:  57%] [G loss: 1.424610, adv: 0.386025, recon: 0.029020, id: 0.026921] time: 0:02:10.331805 \n",
            "[Epoch 21/200] [Batch 42/48] [D loss: 0.229644, acc:  59%] [G loss: 1.457201, adv: 0.404853, recon: 0.028678, id: 0.028829] time: 0:02:10.446603 \n",
            "[Epoch 21/200] [Batch 43/48] [D loss: 0.234654, acc:  57%] [G loss: 1.480260, adv: 0.397249, recon: 0.030481, id: 0.028797] time: 0:02:10.555792 \n",
            "[Epoch 21/200] [Batch 44/48] [D loss: 0.221205, acc:  62%] [G loss: 1.492762, adv: 0.407592, recon: 0.030131, id: 0.029669] time: 0:02:10.666958 \n",
            "[Epoch 21/200] [Batch 45/48] [D loss: 0.230110, acc:  58%] [G loss: 1.486202, adv: 0.402462, recon: 0.030402, id: 0.027632] time: 0:02:10.780712 \n",
            "[Epoch 21/200] [Batch 46/48] [D loss: 0.237060, acc:  57%] [G loss: 1.479858, adv: 0.399029, recon: 0.030288, id: 0.028113] time: 0:02:10.894503 \n",
            "[Epoch 21/200] [Batch 47/48] [D loss: 0.234543, acc:  56%] [G loss: 1.430604, adv: 0.394568, recon: 0.028637, id: 0.024874] time: 0:02:11.006034 \n",
            "[Epoch 22/200] [Batch 0/48] [D loss: 0.239249, acc:  56%] [G loss: 1.483857, adv: 0.395671, recon: 0.030767, id: 0.030306] time: 0:02:11.117086 \n",
            "[Epoch 22/200] [Batch 1/48] [D loss: 0.227737, acc:  59%] [G loss: 1.470100, adv: 0.406298, recon: 0.029255, id: 0.028443] time: 0:02:11.242013 \n",
            "[Epoch 22/200] [Batch 2/48] [D loss: 0.237923, acc:  55%] [G loss: 1.498957, adv: 0.389342, recon: 0.032247, id: 0.028937] time: 0:02:11.354209 \n",
            "[Epoch 22/200] [Batch 3/48] [D loss: 0.229713, acc:  59%] [G loss: 1.522055, adv: 0.409873, recon: 0.031522, id: 0.026823] time: 0:02:11.468937 \n",
            "[Epoch 22/200] [Batch 4/48] [D loss: 0.229715, acc:  60%] [G loss: 1.531494, adv: 0.402516, recon: 0.032345, id: 0.030274] time: 0:02:11.581071 \n",
            "[Epoch 22/200] [Batch 5/48] [D loss: 0.234280, acc:  58%] [G loss: 1.477956, adv: 0.400506, recon: 0.030153, id: 0.026606] time: 0:02:11.694095 \n",
            "[Epoch 22/200] [Batch 6/48] [D loss: 0.240030, acc:  55%] [G loss: 1.463253, adv: 0.390893, recon: 0.030155, id: 0.029684] time: 0:02:11.805726 \n",
            "[Epoch 22/200] [Batch 7/48] [D loss: 0.241618, acc:  54%] [G loss: 1.439368, adv: 0.387288, recon: 0.029477, id: 0.029066] time: 0:02:11.921210 \n",
            "[Epoch 22/200] [Batch 8/48] [D loss: 0.222894, acc:  61%] [G loss: 1.450203, adv: 0.402731, recon: 0.028468, id: 0.027955] time: 0:02:12.035226 \n",
            "[Epoch 22/200] [Batch 9/48] [D loss: 0.238680, acc:  56%] [G loss: 1.435436, adv: 0.390754, recon: 0.029047, id: 0.026751] time: 0:02:12.148759 \n",
            "[Epoch 22/200] [Batch 10/48] [D loss: 0.237375, acc:  56%] [G loss: 1.438011, adv: 0.395446, recon: 0.028676, id: 0.027637] time: 0:02:12.266868 \n",
            "[Epoch 22/200] [Batch 11/48] [D loss: 0.233626, acc:  58%] [G loss: 1.456940, adv: 0.399094, recon: 0.029192, id: 0.027607] time: 0:02:12.377867 \n",
            "[Epoch 22/200] [Batch 12/48] [D loss: 0.243636, acc:  54%] [G loss: 1.476939, adv: 0.394285, recon: 0.030438, id: 0.031425] time: 0:02:12.488853 \n",
            "[Epoch 22/200] [Batch 13/48] [D loss: 0.236397, acc:  55%] [G loss: 1.454696, adv: 0.394449, recon: 0.029542, id: 0.027620] time: 0:02:12.600129 \n",
            "[Epoch 22/200] [Batch 14/48] [D loss: 0.222126, acc:  60%] [G loss: 1.475086, adv: 0.409665, recon: 0.029169, id: 0.026508] time: 0:02:12.711623 \n",
            "[Epoch 22/200] [Batch 15/48] [D loss: 0.237487, acc:  56%] [G loss: 1.471568, adv: 0.391124, recon: 0.030539, id: 0.030626] time: 0:02:12.825509 \n",
            "[Epoch 22/200] [Batch 16/48] [D loss: 0.225942, acc:  61%] [G loss: 1.437927, adv: 0.405496, recon: 0.027794, id: 0.026057] time: 0:02:12.941503 \n",
            "[Epoch 22/200] [Batch 17/48] [D loss: 0.224131, acc:  61%] [G loss: 1.467008, adv: 0.408629, recon: 0.028846, id: 0.029413] time: 0:02:13.056001 \n",
            "[Epoch 22/200] [Batch 18/48] [D loss: 0.230539, acc:  59%] [G loss: 1.477838, adv: 0.399055, recon: 0.030299, id: 0.028041] time: 0:02:13.166567 \n",
            "[Epoch 22/200] [Batch 19/48] [D loss: 0.220791, acc:  62%] [G loss: 1.505170, adv: 0.416248, recon: 0.029877, id: 0.029093] time: 0:02:13.284624 \n",
            "[Epoch 22/200] [Batch 20/48] [D loss: 0.233286, acc:  58%] [G loss: 1.416706, adv: 0.396061, recon: 0.027611, id: 0.029728] time: 0:02:13.395456 \n",
            "[Epoch 22/200] [Batch 21/48] [D loss: 0.250790, acc:  51%] [G loss: 1.467752, adv: 0.385670, recon: 0.030886, id: 0.030850] time: 0:02:13.507456 \n",
            "[Epoch 22/200] [Batch 22/48] [D loss: 0.224617, acc:  60%] [G loss: 1.480842, adv: 0.412809, recon: 0.029050, id: 0.027551] time: 0:02:13.620605 \n",
            "[Epoch 22/200] [Batch 23/48] [D loss: 0.228804, acc:  60%] [G loss: 1.503726, adv: 0.401825, recon: 0.031176, id: 0.027754] time: 0:02:13.733865 \n",
            "[Epoch 22/200] [Batch 24/48] [D loss: 0.223441, acc:  60%] [G loss: 1.472729, adv: 0.404398, recon: 0.029477, id: 0.027062] time: 0:02:13.848419 \n",
            "[Epoch 22/200] [Batch 25/48] [D loss: 0.237280, acc:  56%] [G loss: 1.460479, adv: 0.399916, recon: 0.029356, id: 0.027556] time: 0:02:13.963172 \n",
            "[Epoch 22/200] [Batch 26/48] [D loss: 0.239173, acc:  56%] [G loss: 1.448858, adv: 0.387032, recon: 0.029871, id: 0.029800] time: 0:02:14.075953 \n",
            "[Epoch 22/200] [Batch 27/48] [D loss: 0.223653, acc:  61%] [G loss: 1.475183, adv: 0.408091, recon: 0.029247, id: 0.028893] time: 0:02:14.188729 \n",
            "[Epoch 22/200] [Batch 28/48] [D loss: 0.230237, acc:  60%] [G loss: 1.476302, adv: 0.405852, recon: 0.029517, id: 0.027139] time: 0:02:14.305881 \n",
            "[Epoch 22/200] [Batch 29/48] [D loss: 0.221023, acc:  62%] [G loss: 1.494011, adv: 0.417330, recon: 0.029195, id: 0.027447] time: 0:02:14.418559 \n",
            "[Epoch 22/200] [Batch 30/48] [D loss: 0.229856, acc:  60%] [G loss: 1.472011, adv: 0.396848, recon: 0.030144, id: 0.028012] time: 0:02:14.531866 \n",
            "[Epoch 22/200] [Batch 31/48] [D loss: 0.236222, acc:  57%] [G loss: 1.445496, adv: 0.394219, recon: 0.029218, id: 0.027015] time: 0:02:14.646832 \n",
            "[Epoch 22/200] [Batch 32/48] [D loss: 0.223154, acc:  61%] [G loss: 1.503311, adv: 0.408493, recon: 0.030486, id: 0.029929] time: 0:02:14.760108 \n",
            "[Epoch 22/200] [Batch 33/48] [D loss: 0.231882, acc:  58%] [G loss: 1.493165, adv: 0.407880, recon: 0.030057, id: 0.028935] time: 0:02:14.871446 \n",
            "[Epoch 22/200] [Batch 34/48] [D loss: 0.225167, acc:  60%] [G loss: 1.464278, adv: 0.410227, recon: 0.028473, id: 0.027812] time: 0:02:14.982540 \n",
            "[Epoch 22/200] [Batch 35/48] [D loss: 0.234357, acc:  57%] [G loss: 1.439552, adv: 0.395114, recon: 0.028776, id: 0.027232] time: 0:02:15.093731 \n",
            "[Epoch 22/200] [Batch 36/48] [D loss: 0.244156, acc:  54%] [G loss: 1.485450, adv: 0.395128, recon: 0.030829, id: 0.029784] time: 0:02:15.206440 \n",
            "[Epoch 22/200] [Batch 37/48] [D loss: 0.224202, acc:  61%] [G loss: 1.490794, adv: 0.415455, recon: 0.029259, id: 0.027952] time: 0:02:15.324458 \n",
            "[Epoch 22/200] [Batch 38/48] [D loss: 0.237959, acc:  55%] [G loss: 1.436079, adv: 0.386576, recon: 0.029453, id: 0.027508] time: 0:02:15.443274 \n",
            "[Epoch 22/200] [Batch 39/48] [D loss: 0.232285, acc:  59%] [G loss: 1.466411, adv: 0.396594, recon: 0.030006, id: 0.028419] time: 0:02:15.557741 \n",
            "[Epoch 22/200] [Batch 40/48] [D loss: 0.234748, acc:  57%] [G loss: 1.495990, adv: 0.405884, recon: 0.030424, id: 0.028112] time: 0:02:15.672502 \n",
            "[Epoch 22/200] [Batch 41/48] [D loss: 0.231302, acc:  59%] [G loss: 1.435668, adv: 0.393071, recon: 0.028913, id: 0.026953] time: 0:02:15.784113 \n",
            "[Epoch 22/200] [Batch 42/48] [D loss: 0.229224, acc:  59%] [G loss: 1.458401, adv: 0.404771, recon: 0.028769, id: 0.028839] time: 0:02:15.894209 \n",
            "[Epoch 22/200] [Batch 43/48] [D loss: 0.231548, acc:  58%] [G loss: 1.470149, adv: 0.402070, recon: 0.029545, id: 0.028627] time: 0:02:16.007428 \n",
            "[Epoch 22/200] [Batch 44/48] [D loss: 0.218862, acc:  63%] [G loss: 1.483682, adv: 0.413606, recon: 0.029096, id: 0.029561] time: 0:02:16.117449 \n",
            "[Epoch 22/200] [Batch 45/48] [D loss: 0.229448, acc:  58%] [G loss: 1.463604, adv: 0.402155, recon: 0.029334, id: 0.027556] time: 0:02:16.228430 \n",
            "[Epoch 22/200] [Batch 46/48] [D loss: 0.238707, acc:  56%] [G loss: 1.461979, adv: 0.391979, recon: 0.030114, id: 0.027925] time: 0:02:16.349816 \n",
            "[Epoch 22/200] [Batch 47/48] [D loss: 0.232359, acc:  57%] [G loss: 1.426707, adv: 0.397197, recon: 0.028200, id: 0.025083] time: 0:02:16.462221 \n",
            "[Epoch 23/200] [Batch 0/48] [D loss: 0.239191, acc:  56%] [G loss: 1.482588, adv: 0.395765, recon: 0.030724, id: 0.030070] time: 0:02:16.574917 \n",
            "[Epoch 23/200] [Batch 1/48] [D loss: 0.227783, acc:  59%] [G loss: 1.459467, adv: 0.403455, recon: 0.029037, id: 0.028464] time: 0:02:16.686029 \n",
            "[Epoch 23/200] [Batch 2/48] [D loss: 0.233609, acc:  57%] [G loss: 1.475639, adv: 0.393727, recon: 0.030692, id: 0.028874] time: 0:02:16.796935 \n",
            "[Epoch 23/200] [Batch 3/48] [D loss: 0.231370, acc:  58%] [G loss: 1.497925, adv: 0.406740, recon: 0.030629, id: 0.026950] time: 0:02:16.907875 \n",
            "[Epoch 23/200] [Batch 4/48] [D loss: 0.230756, acc:  59%] [G loss: 1.519432, adv: 0.400840, recon: 0.031947, id: 0.029869] time: 0:02:17.018987 \n",
            "[Epoch 23/200] [Batch 5/48] [D loss: 0.232859, acc:  59%] [G loss: 1.482185, adv: 0.404656, recon: 0.029967, id: 0.026815] time: 0:02:17.131925 \n",
            "[Epoch 23/200] [Batch 6/48] [D loss: 0.238582, acc:  55%] [G loss: 1.478592, adv: 0.395702, recon: 0.030453, id: 0.029715] time: 0:02:17.246784 \n",
            "[Epoch 23/200] [Batch 7/48] [D loss: 0.240934, acc:  55%] [G loss: 1.450219, adv: 0.389700, recon: 0.029785, id: 0.029487] time: 0:02:17.367004 \n",
            "[Epoch 23/200] [Batch 8/48] [D loss: 0.221143, acc:  62%] [G loss: 1.469175, adv: 0.406431, recon: 0.029078, id: 0.027985] time: 0:02:17.478986 \n",
            "[Epoch 23/200] [Batch 9/48] [D loss: 0.237398, acc:  56%] [G loss: 1.468114, adv: 0.393485, recon: 0.030418, id: 0.027105] time: 0:02:17.591614 \n",
            "[Epoch 23/200] [Batch 10/48] [D loss: 0.236102, acc:  57%] [G loss: 1.480495, adv: 0.396865, recon: 0.030688, id: 0.027584] time: 0:02:17.706554 \n",
            "[Epoch 23/200] [Batch 11/48] [D loss: 0.230282, acc:  59%] [G loss: 1.498798, adv: 0.405067, recon: 0.030698, id: 0.028317] time: 0:02:17.816572 \n",
            "[Epoch 23/200] [Batch 12/48] [D loss: 0.241437, acc:  55%] [G loss: 1.482244, adv: 0.398715, recon: 0.030345, id: 0.030559] time: 0:02:17.928549 \n",
            "[Epoch 23/200] [Batch 13/48] [D loss: 0.234804, acc:  56%] [G loss: 1.450845, adv: 0.396643, recon: 0.029152, id: 0.027850] time: 0:02:18.040957 \n",
            "[Epoch 23/200] [Batch 14/48] [D loss: 0.220834, acc:  61%] [G loss: 1.468402, adv: 0.410997, recon: 0.028745, id: 0.026212] time: 0:02:18.153825 \n",
            "[Epoch 23/200] [Batch 15/48] [D loss: 0.235578, acc:  57%] [G loss: 1.474042, adv: 0.395612, recon: 0.030227, id: 0.030706] time: 0:02:18.265471 \n",
            "[Epoch 23/200] [Batch 16/48] [D loss: 0.223777, acc:  62%] [G loss: 1.452451, adv: 0.411477, recon: 0.027968, id: 0.025815] time: 0:02:18.383216 \n",
            "[Epoch 23/200] [Batch 17/48] [D loss: 0.223261, acc:  62%] [G loss: 1.488493, adv: 0.409282, recon: 0.029850, id: 0.029801] time: 0:02:18.496742 \n",
            "[Epoch 23/200] [Batch 18/48] [D loss: 0.227486, acc:  60%] [G loss: 1.489591, adv: 0.403115, recon: 0.030521, id: 0.027651] time: 0:02:18.607344 \n",
            "[Epoch 23/200] [Batch 19/48] [D loss: 0.219410, acc:  62%] [G loss: 1.496269, adv: 0.414712, recon: 0.029613, id: 0.029462] time: 0:02:18.718740 \n",
            "[Epoch 23/200] [Batch 20/48] [D loss: 0.231962, acc:  58%] [G loss: 1.414170, adv: 0.395090, recon: 0.027621, id: 0.029459] time: 0:02:18.831482 \n",
            "[Epoch 23/200] [Batch 21/48] [D loss: 0.249467, acc:  52%] [G loss: 1.465445, adv: 0.385052, recon: 0.030872, id: 0.030557] time: 0:02:18.945136 \n",
            "[Epoch 23/200] [Batch 22/48] [D loss: 0.222906, acc:  61%] [G loss: 1.484707, adv: 0.414659, recon: 0.029088, id: 0.027485] time: 0:02:19.058636 \n",
            "[Epoch 23/200] [Batch 23/48] [D loss: 0.226388, acc:  61%] [G loss: 1.510788, adv: 0.406188, recon: 0.031132, id: 0.027556] time: 0:02:19.171489 \n",
            "[Epoch 23/200] [Batch 24/48] [D loss: 0.222372, acc:  60%] [G loss: 1.473730, adv: 0.403959, recon: 0.029588, id: 0.027062] time: 0:02:19.281720 \n",
            "[Epoch 23/200] [Batch 25/48] [D loss: 0.235107, acc:  57%] [G loss: 1.471853, adv: 0.404090, recon: 0.029532, id: 0.027528] time: 0:02:19.399556 \n",
            "[Epoch 23/200] [Batch 26/48] [D loss: 0.237635, acc:  57%] [G loss: 1.460133, adv: 0.390489, recon: 0.030112, id: 0.029958] time: 0:02:19.512640 \n",
            "[Epoch 23/200] [Batch 27/48] [D loss: 0.221549, acc:  62%] [G loss: 1.483110, adv: 0.411670, recon: 0.029311, id: 0.028848] time: 0:02:19.626889 \n",
            "[Epoch 23/200] [Batch 28/48] [D loss: 0.228503, acc:  60%] [G loss: 1.473591, adv: 0.407162, recon: 0.029277, id: 0.027112] time: 0:02:19.742222 \n",
            "[Epoch 23/200] [Batch 29/48] [D loss: 0.218772, acc:  63%] [G loss: 1.494072, adv: 0.422120, recon: 0.028734, id: 0.027473] time: 0:02:19.853960 \n",
            "[Epoch 23/200] [Batch 30/48] [D loss: 0.227853, acc:  61%] [G loss: 1.452873, adv: 0.398817, recon: 0.029023, id: 0.027985] time: 0:02:19.965734 \n",
            "[Epoch 23/200] [Batch 31/48] [D loss: 0.236504, acc:  57%] [G loss: 1.424999, adv: 0.391527, recon: 0.028453, id: 0.027325] time: 0:02:20.076669 \n",
            "[Epoch 23/200] [Batch 32/48] [D loss: 0.220178, acc:  62%] [G loss: 1.489959, adv: 0.412502, recon: 0.029454, id: 0.029886] time: 0:02:20.187633 \n",
            "[Epoch 23/200] [Batch 33/48] [D loss: 0.230787, acc:  59%] [G loss: 1.490096, adv: 0.411033, recon: 0.029579, id: 0.028954] time: 0:02:20.300814 \n",
            "[Epoch 23/200] [Batch 34/48] [D loss: 0.222890, acc:  61%] [G loss: 1.461882, adv: 0.414259, recon: 0.027990, id: 0.027576] time: 0:02:20.417160 \n",
            "[Epoch 23/200] [Batch 35/48] [D loss: 0.233053, acc:  57%] [G loss: 1.434033, adv: 0.396769, recon: 0.028331, id: 0.027614] time: 0:02:20.531439 \n",
            "[Epoch 23/200] [Batch 36/48] [D loss: 0.241341, acc:  56%] [G loss: 1.490134, adv: 0.398954, recon: 0.030752, id: 0.029306] time: 0:02:20.646040 \n",
            "[Epoch 23/200] [Batch 37/48] [D loss: 0.223459, acc:  61%] [G loss: 1.500681, adv: 0.416314, recon: 0.029623, id: 0.028654] time: 0:02:20.757457 \n",
            "[Epoch 23/200] [Batch 38/48] [D loss: 0.235929, acc:  56%] [G loss: 1.428053, adv: 0.385964, recon: 0.029198, id: 0.026408] time: 0:02:20.870592 \n",
            "[Epoch 23/200] [Batch 39/48] [D loss: 0.231192, acc:  59%] [G loss: 1.455494, adv: 0.393527, recon: 0.029739, id: 0.029162] time: 0:02:20.981380 \n",
            "[Epoch 23/200] [Batch 40/48] [D loss: 0.230100, acc:  59%] [G loss: 1.474873, adv: 0.408413, recon: 0.029219, id: 0.027312] time: 0:02:21.093518 \n",
            "[Epoch 23/200] [Batch 41/48] [D loss: 0.231615, acc:  58%] [G loss: 1.433619, adv: 0.390761, recon: 0.029012, id: 0.027303] time: 0:02:21.205780 \n",
            "[Epoch 23/200] [Batch 42/48] [D loss: 0.225502, acc:  61%] [G loss: 1.461393, adv: 0.413018, recon: 0.028145, id: 0.028507] time: 0:02:21.319661 \n",
            "[Epoch 23/200] [Batch 43/48] [D loss: 0.232535, acc:  58%] [G loss: 1.469492, adv: 0.398288, recon: 0.029846, id: 0.029398] time: 0:02:21.443908 \n",
            "[Epoch 23/200] [Batch 44/48] [D loss: 0.215392, acc:  64%] [G loss: 1.491698, adv: 0.413187, recon: 0.029607, id: 0.028997] time: 0:02:21.558088 \n",
            "[Epoch 23/200] [Batch 45/48] [D loss: 0.226360, acc:  60%] [G loss: 1.476619, adv: 0.405672, recon: 0.029597, id: 0.028485] time: 0:02:21.671265 \n",
            "[Epoch 23/200] [Batch 46/48] [D loss: 0.233439, acc:  58%] [G loss: 1.469706, adv: 0.399908, recon: 0.029775, id: 0.027072] time: 0:02:21.783703 \n",
            "[Epoch 23/200] [Batch 47/48] [D loss: 0.231512, acc:  57%] [G loss: 1.422918, adv: 0.397021, recon: 0.027984, id: 0.026139] time: 0:02:21.896231 \n",
            "[Epoch 24/200] [Batch 0/48] [D loss: 0.233946, acc:  58%] [G loss: 1.488151, adv: 0.402429, recon: 0.030432, id: 0.029084] time: 0:02:22.009132 \n",
            "[Epoch 24/200] [Batch 1/48] [D loss: 0.229437, acc:  59%] [G loss: 1.465848, adv: 0.401800, recon: 0.029465, id: 0.028996] time: 0:02:22.120551 \n",
            "[Epoch 24/200] [Batch 2/48] [D loss: 0.228994, acc:  59%] [G loss: 1.437440, adv: 0.398315, recon: 0.028443, id: 0.027941] time: 0:02:22.232572 \n",
            "[Epoch 24/200] [Batch 3/48] [D loss: 0.232041, acc:  58%] [G loss: 1.461334, adv: 0.404998, recon: 0.028939, id: 0.027626] time: 0:02:22.346791 \n",
            "[Epoch 24/200] [Batch 4/48] [D loss: 0.227904, acc:  61%] [G loss: 1.491727, adv: 0.404216, recon: 0.030329, id: 0.028620] time: 0:02:22.462624 \n",
            "[Epoch 24/200] [Batch 5/48] [D loss: 0.230748, acc:  59%] [G loss: 1.473231, adv: 0.409306, recon: 0.029055, id: 0.027301] time: 0:02:22.575104 \n",
            "[Epoch 24/200] [Batch 6/48] [D loss: 0.236827, acc:  56%] [G loss: 1.479347, adv: 0.396633, recon: 0.030437, id: 0.029360] time: 0:02:22.687491 \n",
            "[Epoch 24/200] [Batch 7/48] [D loss: 0.239844, acc:  55%] [G loss: 1.460721, adv: 0.389192, recon: 0.030351, id: 0.029831] time: 0:02:22.799693 \n",
            "[Epoch 24/200] [Batch 8/48] [D loss: 0.217533, acc:  63%] [G loss: 1.505497, adv: 0.411917, recon: 0.030362, id: 0.028503] time: 0:02:22.913278 \n",
            "[Epoch 24/200] [Batch 9/48] [D loss: 0.237773, acc:  56%] [G loss: 1.490412, adv: 0.392272, recon: 0.031649, id: 0.027477] time: 0:02:23.026766 \n",
            "[Epoch 24/200] [Batch 10/48] [D loss: 0.234364, acc:  57%] [G loss: 1.475120, adv: 0.396766, recon: 0.030454, id: 0.027704] time: 0:02:23.141573 \n",
            "[Epoch 24/200] [Batch 11/48] [D loss: 0.230469, acc:  59%] [G loss: 1.465136, adv: 0.400097, recon: 0.029552, id: 0.028103] time: 0:02:23.254005 \n",
            "[Epoch 24/200] [Batch 12/48] [D loss: 0.240175, acc:  55%] [G loss: 1.474397, adv: 0.399198, recon: 0.029913, id: 0.030713] time: 0:02:23.365093 \n",
            "[Epoch 24/200] [Batch 13/48] [D loss: 0.234206, acc:  56%] [G loss: 1.447412, adv: 0.397018, recon: 0.028981, id: 0.027669] time: 0:02:23.482101 \n",
            "[Epoch 24/200] [Batch 14/48] [D loss: 0.220414, acc:  61%] [G loss: 1.468186, adv: 0.410845, recon: 0.028758, id: 0.026265] time: 0:02:23.592923 \n",
            "[Epoch 24/200] [Batch 15/48] [D loss: 0.235557, acc:  57%] [G loss: 1.470267, adv: 0.394019, recon: 0.030242, id: 0.030451] time: 0:02:23.704079 \n",
            "[Epoch 24/200] [Batch 16/48] [D loss: 0.223564, acc:  62%] [G loss: 1.433571, adv: 0.409490, recon: 0.027228, id: 0.026132] time: 0:02:23.817345 \n",
            "[Epoch 24/200] [Batch 17/48] [D loss: 0.223164, acc:  61%] [G loss: 1.462476, adv: 0.408477, recon: 0.028684, id: 0.029331] time: 0:02:23.933829 \n",
            "[Epoch 24/200] [Batch 18/48] [D loss: 0.228006, acc:  60%] [G loss: 1.468247, adv: 0.403590, recon: 0.029415, id: 0.028021] time: 0:02:24.047076 \n",
            "[Epoch 24/200] [Batch 19/48] [D loss: 0.220488, acc:  62%] [G loss: 1.500213, adv: 0.415174, recon: 0.029775, id: 0.029195] time: 0:02:24.160290 \n",
            "[Epoch 24/200] [Batch 20/48] [D loss: 0.230148, acc:  59%] [G loss: 1.418794, adv: 0.400037, recon: 0.027386, id: 0.029562] time: 0:02:24.270720 \n",
            "[Epoch 24/200] [Batch 21/48] [D loss: 0.248182, acc:  52%] [G loss: 1.476408, adv: 0.392749, recon: 0.030640, id: 0.030881] time: 0:02:24.383243 \n",
            "[Epoch 24/200] [Batch 22/48] [D loss: 0.221665, acc:  62%] [G loss: 1.482456, adv: 0.419325, recon: 0.028542, id: 0.027744] time: 0:02:24.502073 \n",
            "[Epoch 24/200] [Batch 23/48] [D loss: 0.225902, acc:  61%] [G loss: 1.515211, adv: 0.408108, recon: 0.031171, id: 0.027899] time: 0:02:24.617410 \n",
            "[Epoch 24/200] [Batch 24/48] [D loss: 0.223094, acc:  60%] [G loss: 1.484988, adv: 0.406622, recon: 0.029926, id: 0.026817] time: 0:02:24.729683 \n",
            "[Epoch 24/200] [Batch 25/48] [D loss: 0.233451, acc:  58%] [G loss: 1.476290, adv: 0.409427, recon: 0.029230, id: 0.027862] time: 0:02:24.843031 \n",
            "[Epoch 24/200] [Batch 26/48] [D loss: 0.235027, acc:  58%] [G loss: 1.452877, adv: 0.395338, recon: 0.029323, id: 0.029742] time: 0:02:24.957921 \n",
            "[Epoch 24/200] [Batch 27/48] [D loss: 0.220398, acc:  62%] [G loss: 1.477235, adv: 0.413826, recon: 0.028812, id: 0.028905] time: 0:02:25.070911 \n",
            "[Epoch 24/200] [Batch 28/48] [D loss: 0.228488, acc:  60%] [G loss: 1.457380, adv: 0.403425, recon: 0.028895, id: 0.026645] time: 0:02:25.184485 \n",
            "[Epoch 24/200] [Batch 29/48] [D loss: 0.219155, acc:  63%] [G loss: 1.491376, adv: 0.420308, recon: 0.028780, id: 0.027640] time: 0:02:25.295737 \n",
            "[Epoch 24/200] [Batch 30/48] [D loss: 0.226943, acc:  61%] [G loss: 1.479167, adv: 0.403176, recon: 0.029947, id: 0.027647] time: 0:02:25.407724 \n",
            "[Epoch 24/200] [Batch 31/48] [D loss: 0.236434, acc:  57%] [G loss: 1.448428, adv: 0.394097, recon: 0.029356, id: 0.028107] time: 0:02:25.522904 \n",
            "[Epoch 24/200] [Batch 32/48] [D loss: 0.218926, acc:  63%] [G loss: 1.511859, adv: 0.414065, recon: 0.030407, id: 0.030027] time: 0:02:25.634093 \n",
            "[Epoch 24/200] [Batch 33/48] [D loss: 0.231631, acc:  58%] [G loss: 1.514781, adv: 0.410388, recon: 0.030878, id: 0.029076] time: 0:02:25.746706 \n",
            "[Epoch 24/200] [Batch 34/48] [D loss: 0.222759, acc:  61%] [G loss: 1.473688, adv: 0.415363, recon: 0.028512, id: 0.027412] time: 0:02:25.861268 \n",
            "[Epoch 24/200] [Batch 35/48] [D loss: 0.235262, acc:  56%] [G loss: 1.457972, adv: 0.394567, recon: 0.029760, id: 0.027554] time: 0:02:25.974792 \n",
            "[Epoch 24/200] [Batch 36/48] [D loss: 0.237892, acc:  57%] [G loss: 1.514999, adv: 0.407377, recon: 0.031204, id: 0.029051] time: 0:02:26.087255 \n",
            "[Epoch 24/200] [Batch 37/48] [D loss: 0.221875, acc:  62%] [G loss: 1.498343, adv: 0.417679, recon: 0.029410, id: 0.028497] time: 0:02:26.199056 \n",
            "[Epoch 24/200] [Batch 38/48] [D loss: 0.232671, acc:  57%] [G loss: 1.430204, adv: 0.391913, recon: 0.028722, id: 0.026540] time: 0:02:26.312438 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f9d20781a0c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCycleGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;31m#gan.save_Model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-f9d20781a0c9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    249\u001b[0m                                                         [valid, valid,\n\u001b[1;32m    250\u001b[0m                                                         \u001b[0mimgs_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_B\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                                                         imgs_A, imgs_B])\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLMKO5CFuKOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQjWLdkMjWUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan = CycleGAN()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BItkXKknhYaR",
        "colab_type": "code",
        "outputId": "62a43d11-8b8a-43c6-e091-ac0fa91da89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "# !pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "\n",
        "modelA2B = load_model('DomainA2B.model')\n",
        "modelB2A = load_model('DomainB2A.model')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b0f1630509bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# !pip install git+https://www.github.com/keras-team/keras-contrib.git\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"InstanceNormalization\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInstanceNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodelA2B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DomainA2B.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodelB2A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DomainB2A.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object does not support item assignment"
          ]
        }
      ]
    }
  ]
}