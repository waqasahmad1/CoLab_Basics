{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waqasahmad1/CoLab_Basics/blob/master/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmDevT9zI9Co",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-CX35q7I7-7",
        "colab_type": "code",
        "outputId": "1ac60da6-6db1-4bbd-e207-d7d810687bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "DCGAN on MNIST using Keras\n",
        "Author: Rowel Atienza\n",
        "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
        "Dependencies: tensorflow 1.0 and keras 2.0\n",
        "Usage: python3 dcgan_mnist.py\n",
        "'''\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Reshape\n",
        "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
        "from keras.layers import LeakyReLU, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ElapsedTimer(object):\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "    def elapsed(self,sec):\n",
        "        if sec < 60:\n",
        "            return str(sec) + \" sec\"\n",
        "        elif sec < (60 * 60):\n",
        "            return str(sec / 60) + \" min\"\n",
        "        else:\n",
        "            return str(sec / (60 * 60)) + \" hr\"\n",
        "    def elapsed_time(self):\n",
        "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n",
        "\n",
        "class DCGAN(object):\n",
        "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.channel = channel\n",
        "        self.D = None   # discriminator\n",
        "        self.G = None   # generator\n",
        "        self.AM = None  # adversarial model\n",
        "        self.DM = None  # discriminator model\n",
        "\n",
        "    # (Wâˆ’F+2P)/S+1\n",
        "    def discriminator(self):\n",
        "        if self.D:\n",
        "            return self.D\n",
        "        self.D = Sequential()\n",
        "        depth = 64\n",
        "        dropout = 0.4\n",
        "        # In: 28 x 28 x 1, depth = 1\n",
        "        # Out: 14 x 14 x 1, depth=64\n",
        "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
        "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
        "            padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=0.2))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        # Out: 1-dim probability\n",
        "        self.D.add(Flatten())\n",
        "        self.D.add(Dense(1))\n",
        "        self.D.add(Activation('sigmoid'))\n",
        "        self.D.summary()\n",
        "        return self.D\n",
        "\n",
        "    def generator(self):\n",
        "        if self.G:\n",
        "            return self.G\n",
        "        self.G = Sequential()\n",
        "        dropout = 0.4\n",
        "        depth = 64+64+64+64\n",
        "        dim = 7\n",
        "        # In: 100\n",
        "        # Out: dim x dim x depth\n",
        "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "        self.G.add(Reshape((dim, dim, depth)))\n",
        "        self.G.add(Dropout(dropout))\n",
        "\n",
        "        # In: dim x dim x depth\n",
        "        # Out: 2*dim x 2*dim x depth/2\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=0.9))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
        "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
        "        self.G.add(Activation('sigmoid'))\n",
        "        self.G.summary()\n",
        "        return self.G\n",
        "\n",
        "    def discriminator_model(self):\n",
        "        if self.DM:\n",
        "            return self.DM\n",
        "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
        "        self.DM = Sequential()\n",
        "        self.DM.add(self.discriminator())\n",
        "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "            metrics=['accuracy'])\n",
        "        return self.DM\n",
        "\n",
        "    def adversarial_model(self):\n",
        "        if self.AM:\n",
        "            return self.AM\n",
        "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
        "        self.AM = Sequential()\n",
        "        self.AM.add(self.generator())\n",
        "        self.AM.add(self.discriminator())\n",
        "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "            metrics=['accuracy'])\n",
        "        return self.AM\n",
        "\n",
        "class MNIST_DCGAN(object):\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channel = 1\n",
        "\n",
        "        self.x_train = input_data.read_data_sets(\"mnist\",\\\n",
        "        \tone_hot=True).train.images\n",
        "        self.x_train = self.x_train.reshape(-1, self.img_rows,\\\n",
        "        \tself.img_cols, 1).astype(np.float32)\n",
        "\n",
        "        self.DCGAN = DCGAN()\n",
        "        self.discriminator =  self.DCGAN.discriminator_model()\n",
        "        self.adversarial = self.DCGAN.adversarial_model()\n",
        "        self.generator = self.DCGAN.generator()\n",
        "\n",
        "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
        "        noise_input = None\n",
        "        if save_interval>0:\n",
        "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "        for i in range(train_steps):\n",
        "            images_train = self.x_train[np.random.randint(0,\n",
        "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            images_fake = self.generator.predict(noise)\n",
        "            x = np.concatenate((images_train, images_fake))\n",
        "            y = np.ones([2*batch_size, 1])\n",
        "            y[batch_size:, :] = 0\n",
        "            d_loss = self.discriminator.train_on_batch(x, y)\n",
        "\n",
        "            y = np.ones([batch_size, 1])\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
        "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
        "            print(log_mesg)\n",
        "            if save_interval>0:\n",
        "                if (i+1)%save_interval==0:\n",
        "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
        "                        noise=noise_input, step=(i+1))\n",
        "\n",
        "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
        "        filename = 'mnist.png'\n",
        "        if fake:\n",
        "            if noise is None:\n",
        "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
        "            else:\n",
        "                filename = \"mnist_%d.png\" % step\n",
        "            images = self.generator.predict(noise)\n",
        "        else:\n",
        "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
        "            images = self.x_train[i, :, :, :]\n",
        "\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            image = images[i, :, :, :]\n",
        "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
        "            plt.imshow(image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        if save2file:\n",
        "            plt.savefig(filename)\n",
        "            plt.close('all')\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    mnist_dcgan = MNIST_DCGAN()\n",
        "    timer = ElapsedTimer()\n",
        "    mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=500)\n",
        "    timer.elapsed_time()\n",
        "    mnist_dcgan.plot_images(fake=True)\n",
        "    mnist_dcgan.plot_images(fake=False, save2file=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0718 06:36:55.354182 140469315291008 deprecation.py:323] From <ipython-input-1-2cedcea166ec>:140: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0718 06:36:55.355684 140469315291008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0718 06:36:55.357041 140469315291008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0718 06:37:00.560569 140469315291008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting mnist/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0718 06:37:00.882209 140469315291008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0718 06:37:00.887888 140469315291008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0718 06:37:01.044996 140469315291008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting mnist/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0718 06:37:01.353834 140469315291008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0718 06:37:01.355521 140469315291008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0718 06:37:01.357604 140469315291008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0718 06:37:01.387689 140469315291008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0718 06:37:01.397292 140469315291008 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 8193      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 4,311,553\n",
            "Trainable params: 4,311,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0718 06:37:01.757041 140469315291008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0718 06:37:01.764948 140469315291008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0718 06:37:01.771699 140469315291008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0718 06:37:01.954674 140469315291008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "W0718 06:37:04.893121 140469315291008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 12544)             1266944   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 12544)             50176     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,394,241\n",
            "Trainable params: 2,368,705\n",
            "Non-trainable params: 25,536\n",
            "_________________________________________________________________\n",
            "0: [D loss: 0.694476, acc: 0.429688]  [A loss: 1.016221, acc: 0.000000]\n",
            "1: [D loss: 0.661801, acc: 0.570312]  [A loss: 0.562958, acc: 1.000000]\n",
            "2: [D loss: 1.546414, acc: 0.500000]  [A loss: 0.738994, acc: 0.023438]\n",
            "3: [D loss: 0.614243, acc: 0.787109]  [A loss: 0.726203, acc: 0.191406]\n",
            "4: [D loss: 0.559077, acc: 0.894531]  [A loss: 0.647935, acc: 0.812500]\n",
            "5: [D loss: 0.456317, acc: 0.957031]  [A loss: 0.508176, acc: 0.972656]\n",
            "6: [D loss: 0.362287, acc: 0.933594]  [A loss: 0.436984, acc: 0.968750]\n",
            "7: [D loss: 0.938770, acc: 0.498047]  [A loss: 1.153674, acc: 0.003906]\n",
            "8: [D loss: 0.983146, acc: 0.429688]  [A loss: 1.041586, acc: 0.000000]\n",
            "9: [D loss: 0.727811, acc: 0.458984]  [A loss: 0.993025, acc: 0.000000]\n",
            "10: [D loss: 0.733585, acc: 0.417969]  [A loss: 0.982890, acc: 0.000000]\n",
            "11: [D loss: 0.723697, acc: 0.441406]  [A loss: 0.983204, acc: 0.000000]\n",
            "12: [D loss: 0.719617, acc: 0.437500]  [A loss: 0.969813, acc: 0.000000]\n",
            "13: [D loss: 0.714802, acc: 0.429688]  [A loss: 0.956111, acc: 0.000000]\n",
            "14: [D loss: 0.693884, acc: 0.462891]  [A loss: 0.925905, acc: 0.000000]\n",
            "15: [D loss: 0.672567, acc: 0.492188]  [A loss: 0.927719, acc: 0.000000]\n",
            "16: [D loss: 0.664983, acc: 0.490234]  [A loss: 0.935395, acc: 0.000000]\n",
            "17: [D loss: 0.649709, acc: 0.494141]  [A loss: 0.916886, acc: 0.000000]\n",
            "18: [D loss: 0.634325, acc: 0.500000]  [A loss: 0.955688, acc: 0.000000]\n",
            "19: [D loss: 0.604255, acc: 0.500000]  [A loss: 0.838315, acc: 0.031250]\n",
            "20: [D loss: 0.652458, acc: 0.500000]  [A loss: 1.116341, acc: 0.000000]\n",
            "21: [D loss: 0.550510, acc: 0.925781]  [A loss: 0.604148, acc: 0.902344]\n",
            "22: [D loss: 0.873315, acc: 0.500000]  [A loss: 1.251875, acc: 0.000000]\n",
            "23: [D loss: 0.587395, acc: 0.960938]  [A loss: 0.834318, acc: 0.007812]\n",
            "24: [D loss: 0.546326, acc: 0.521484]  [A loss: 0.691073, acc: 0.539062]\n",
            "25: [D loss: 0.656949, acc: 0.500000]  [A loss: 0.891176, acc: 0.000000]\n",
            "26: [D loss: 0.545226, acc: 0.523438]  [A loss: 0.799011, acc: 0.097656]\n",
            "27: [D loss: 0.598545, acc: 0.500000]  [A loss: 0.944013, acc: 0.003906]\n",
            "28: [D loss: 0.528278, acc: 0.562500]  [A loss: 0.845399, acc: 0.058594]\n",
            "29: [D loss: 0.590294, acc: 0.501953]  [A loss: 1.062263, acc: 0.000000]\n",
            "30: [D loss: 0.496804, acc: 0.787109]  [A loss: 0.739142, acc: 0.339844]\n",
            "31: [D loss: 0.724205, acc: 0.500000]  [A loss: 1.321660, acc: 0.000000]\n",
            "32: [D loss: 0.560586, acc: 0.931641]  [A loss: 0.744181, acc: 0.339844]\n",
            "33: [D loss: 0.603184, acc: 0.503906]  [A loss: 0.945700, acc: 0.003906]\n",
            "34: [D loss: 0.543556, acc: 0.625000]  [A loss: 0.860823, acc: 0.085938]\n",
            "35: [D loss: 0.569893, acc: 0.562500]  [A loss: 0.995760, acc: 0.011719]\n",
            "36: [D loss: 0.526367, acc: 0.662109]  [A loss: 0.905827, acc: 0.046875]\n",
            "37: [D loss: 0.567738, acc: 0.568359]  [A loss: 1.113822, acc: 0.000000]\n",
            "38: [D loss: 0.501139, acc: 0.816406]  [A loss: 0.825086, acc: 0.214844]\n",
            "39: [D loss: 0.614848, acc: 0.527344]  [A loss: 1.414727, acc: 0.000000]\n",
            "40: [D loss: 0.534255, acc: 0.888672]  [A loss: 0.701109, acc: 0.511719]\n",
            "41: [D loss: 0.613378, acc: 0.527344]  [A loss: 1.245472, acc: 0.000000]\n",
            "42: [D loss: 0.484556, acc: 0.947266]  [A loss: 0.778721, acc: 0.308594]\n",
            "43: [D loss: 0.553037, acc: 0.570312]  [A loss: 1.230675, acc: 0.000000]\n",
            "44: [D loss: 0.453951, acc: 0.933594]  [A loss: 0.785195, acc: 0.324219]\n",
            "45: [D loss: 0.541201, acc: 0.582031]  [A loss: 1.321195, acc: 0.000000]\n",
            "46: [D loss: 0.440676, acc: 0.937500]  [A loss: 0.778950, acc: 0.347656]\n",
            "47: [D loss: 0.561545, acc: 0.572266]  [A loss: 1.380872, acc: 0.000000]\n",
            "48: [D loss: 0.439065, acc: 0.919922]  [A loss: 0.736926, acc: 0.375000]\n",
            "49: [D loss: 0.541228, acc: 0.617188]  [A loss: 1.299603, acc: 0.000000]\n",
            "50: [D loss: 0.397397, acc: 0.941406]  [A loss: 0.810608, acc: 0.324219]\n",
            "51: [D loss: 0.478302, acc: 0.664062]  [A loss: 1.338531, acc: 0.000000]\n",
            "52: [D loss: 0.379401, acc: 0.947266]  [A loss: 0.796597, acc: 0.328125]\n",
            "53: [D loss: 0.509813, acc: 0.638672]  [A loss: 1.440782, acc: 0.000000]\n",
            "54: [D loss: 0.366312, acc: 0.941406]  [A loss: 0.778114, acc: 0.363281]\n",
            "55: [D loss: 0.484591, acc: 0.660156]  [A loss: 1.424892, acc: 0.000000]\n",
            "56: [D loss: 0.379209, acc: 0.929688]  [A loss: 0.758689, acc: 0.410156]\n",
            "57: [D loss: 0.507290, acc: 0.628906]  [A loss: 1.473082, acc: 0.000000]\n",
            "58: [D loss: 0.407036, acc: 0.900391]  [A loss: 0.693883, acc: 0.507812]\n",
            "59: [D loss: 0.559676, acc: 0.607422]  [A loss: 1.444221, acc: 0.000000]\n",
            "60: [D loss: 0.374768, acc: 0.931641]  [A loss: 0.869046, acc: 0.218750]\n",
            "61: [D loss: 0.458190, acc: 0.681641]  [A loss: 1.233698, acc: 0.003906]\n",
            "62: [D loss: 0.406022, acc: 0.833984]  [A loss: 0.956979, acc: 0.125000]\n",
            "63: [D loss: 0.447659, acc: 0.710938]  [A loss: 1.362922, acc: 0.000000]\n",
            "64: [D loss: 0.395052, acc: 0.871094]  [A loss: 0.982728, acc: 0.105469]\n",
            "65: [D loss: 0.482839, acc: 0.656250]  [A loss: 1.551238, acc: 0.000000]\n",
            "66: [D loss: 0.413430, acc: 0.912109]  [A loss: 0.721533, acc: 0.542969]\n",
            "67: [D loss: 0.688172, acc: 0.527344]  [A loss: 1.699907, acc: 0.000000]\n",
            "68: [D loss: 0.469351, acc: 0.865234]  [A loss: 0.740209, acc: 0.406250]\n",
            "69: [D loss: 0.564760, acc: 0.554688]  [A loss: 1.290613, acc: 0.000000]\n",
            "70: [D loss: 0.412341, acc: 0.914062]  [A loss: 0.918014, acc: 0.128906]\n",
            "71: [D loss: 0.470276, acc: 0.646484]  [A loss: 1.293717, acc: 0.000000]\n",
            "72: [D loss: 0.431837, acc: 0.847656]  [A loss: 0.941482, acc: 0.113281]\n",
            "73: [D loss: 0.516760, acc: 0.603516]  [A loss: 1.520202, acc: 0.000000]\n",
            "74: [D loss: 0.461679, acc: 0.875000]  [A loss: 0.777070, acc: 0.394531]\n",
            "75: [D loss: 0.629841, acc: 0.519531]  [A loss: 1.634235, acc: 0.000000]\n",
            "76: [D loss: 0.489166, acc: 0.861328]  [A loss: 0.726531, acc: 0.429688]\n",
            "77: [D loss: 0.672221, acc: 0.505859]  [A loss: 1.458629, acc: 0.000000]\n",
            "78: [D loss: 0.471622, acc: 0.898438]  [A loss: 0.774267, acc: 0.363281]\n",
            "79: [D loss: 0.631315, acc: 0.515625]  [A loss: 1.376924, acc: 0.000000]\n",
            "80: [D loss: 0.472473, acc: 0.896484]  [A loss: 0.835788, acc: 0.238281]\n",
            "81: [D loss: 0.613503, acc: 0.521484]  [A loss: 1.426435, acc: 0.000000]\n",
            "82: [D loss: 0.492408, acc: 0.884766]  [A loss: 0.828792, acc: 0.214844]\n",
            "83: [D loss: 0.628251, acc: 0.515625]  [A loss: 1.429515, acc: 0.000000]\n",
            "84: [D loss: 0.487886, acc: 0.876953]  [A loss: 0.819183, acc: 0.273438]\n",
            "85: [D loss: 0.677208, acc: 0.513672]  [A loss: 1.540281, acc: 0.000000]\n",
            "86: [D loss: 0.514266, acc: 0.871094]  [A loss: 0.820289, acc: 0.242188]\n",
            "87: [D loss: 0.662156, acc: 0.507812]  [A loss: 1.413157, acc: 0.000000]\n",
            "88: [D loss: 0.511033, acc: 0.859375]  [A loss: 0.830356, acc: 0.242188]\n",
            "89: [D loss: 0.655610, acc: 0.519531]  [A loss: 1.446715, acc: 0.000000]\n",
            "90: [D loss: 0.520495, acc: 0.847656]  [A loss: 0.860563, acc: 0.226562]\n",
            "91: [D loss: 0.672950, acc: 0.507812]  [A loss: 1.459798, acc: 0.000000]\n",
            "92: [D loss: 0.524994, acc: 0.835938]  [A loss: 0.835772, acc: 0.226562]\n",
            "93: [D loss: 0.672500, acc: 0.523438]  [A loss: 1.484125, acc: 0.000000]\n",
            "94: [D loss: 0.537920, acc: 0.804688]  [A loss: 0.849676, acc: 0.207031]\n",
            "95: [D loss: 0.671462, acc: 0.513672]  [A loss: 1.482700, acc: 0.000000]\n",
            "96: [D loss: 0.520536, acc: 0.859375]  [A loss: 0.846763, acc: 0.246094]\n",
            "97: [D loss: 0.683606, acc: 0.519531]  [A loss: 1.551203, acc: 0.000000]\n",
            "98: [D loss: 0.538431, acc: 0.841797]  [A loss: 0.755480, acc: 0.363281]\n",
            "99: [D loss: 0.723775, acc: 0.507812]  [A loss: 1.539511, acc: 0.000000]\n",
            "100: [D loss: 0.545723, acc: 0.806641]  [A loss: 0.772896, acc: 0.351562]\n",
            "101: [D loss: 0.695437, acc: 0.511719]  [A loss: 1.452464, acc: 0.000000]\n",
            "102: [D loss: 0.536112, acc: 0.849609]  [A loss: 0.845310, acc: 0.210938]\n",
            "103: [D loss: 0.679598, acc: 0.511719]  [A loss: 1.494763, acc: 0.000000]\n",
            "104: [D loss: 0.535047, acc: 0.845703]  [A loss: 0.829785, acc: 0.269531]\n",
            "105: [D loss: 0.683836, acc: 0.513672]  [A loss: 1.514791, acc: 0.000000]\n",
            "106: [D loss: 0.541688, acc: 0.843750]  [A loss: 0.801807, acc: 0.300781]\n",
            "107: [D loss: 0.691777, acc: 0.513672]  [A loss: 1.518111, acc: 0.000000]\n",
            "108: [D loss: 0.539746, acc: 0.851562]  [A loss: 0.821224, acc: 0.281250]\n",
            "109: [D loss: 0.666523, acc: 0.515625]  [A loss: 1.512963, acc: 0.000000]\n",
            "110: [D loss: 0.525869, acc: 0.853516]  [A loss: 0.803233, acc: 0.308594]\n",
            "111: [D loss: 0.670101, acc: 0.525391]  [A loss: 1.541172, acc: 0.000000]\n",
            "112: [D loss: 0.524074, acc: 0.845703]  [A loss: 0.784571, acc: 0.347656]\n",
            "113: [D loss: 0.675954, acc: 0.515625]  [A loss: 1.572696, acc: 0.000000]\n",
            "114: [D loss: 0.527314, acc: 0.851562]  [A loss: 0.809980, acc: 0.296875]\n",
            "115: [D loss: 0.672527, acc: 0.513672]  [A loss: 1.593421, acc: 0.000000]\n",
            "116: [D loss: 0.520066, acc: 0.847656]  [A loss: 0.828487, acc: 0.289062]\n",
            "117: [D loss: 0.651722, acc: 0.523438]  [A loss: 1.535605, acc: 0.000000]\n",
            "118: [D loss: 0.530108, acc: 0.820312]  [A loss: 0.844945, acc: 0.273438]\n",
            "119: [D loss: 0.672330, acc: 0.517578]  [A loss: 1.690632, acc: 0.000000]\n",
            "120: [D loss: 0.524258, acc: 0.855469]  [A loss: 0.761410, acc: 0.410156]\n",
            "121: [D loss: 0.703803, acc: 0.521484]  [A loss: 1.660688, acc: 0.000000]\n",
            "122: [D loss: 0.542556, acc: 0.808594]  [A loss: 0.718529, acc: 0.468750]\n",
            "123: [D loss: 0.716794, acc: 0.505859]  [A loss: 1.596024, acc: 0.000000]\n",
            "124: [D loss: 0.530565, acc: 0.849609]  [A loss: 0.796391, acc: 0.304688]\n",
            "125: [D loss: 0.690399, acc: 0.505859]  [A loss: 1.550220, acc: 0.000000]\n",
            "126: [D loss: 0.541872, acc: 0.812500]  [A loss: 0.885316, acc: 0.171875]\n",
            "127: [D loss: 0.650853, acc: 0.521484]  [A loss: 1.614048, acc: 0.000000]\n",
            "128: [D loss: 0.530555, acc: 0.837891]  [A loss: 0.840786, acc: 0.246094]\n",
            "129: [D loss: 0.685944, acc: 0.507812]  [A loss: 1.736090, acc: 0.000000]\n",
            "130: [D loss: 0.546242, acc: 0.781250]  [A loss: 0.682274, acc: 0.570312]\n",
            "131: [D loss: 0.739825, acc: 0.501953]  [A loss: 1.644744, acc: 0.000000]\n",
            "132: [D loss: 0.551732, acc: 0.791016]  [A loss: 0.788404, acc: 0.339844]\n",
            "133: [D loss: 0.690880, acc: 0.503906]  [A loss: 1.613827, acc: 0.000000]\n",
            "134: [D loss: 0.536269, acc: 0.832031]  [A loss: 0.803990, acc: 0.304688]\n",
            "135: [D loss: 0.643325, acc: 0.529297]  [A loss: 1.563465, acc: 0.000000]\n",
            "136: [D loss: 0.525130, acc: 0.851562]  [A loss: 0.823879, acc: 0.261719]\n",
            "137: [D loss: 0.670154, acc: 0.509766]  [A loss: 1.740704, acc: 0.000000]\n",
            "138: [D loss: 0.536224, acc: 0.798828]  [A loss: 0.687171, acc: 0.558594]\n",
            "139: [D loss: 0.706942, acc: 0.501953]  [A loss: 1.647376, acc: 0.000000]\n",
            "140: [D loss: 0.539449, acc: 0.822266]  [A loss: 0.748845, acc: 0.406250]\n",
            "141: [D loss: 0.707089, acc: 0.500000]  [A loss: 1.641866, acc: 0.000000]\n",
            "142: [D loss: 0.530121, acc: 0.847656]  [A loss: 0.800861, acc: 0.347656]\n",
            "143: [D loss: 0.670667, acc: 0.523438]  [A loss: 1.649661, acc: 0.000000]\n",
            "144: [D loss: 0.535274, acc: 0.830078]  [A loss: 0.796775, acc: 0.285156]\n",
            "145: [D loss: 0.676347, acc: 0.517578]  [A loss: 1.703431, acc: 0.000000]\n",
            "146: [D loss: 0.539724, acc: 0.820312]  [A loss: 0.813654, acc: 0.261719]\n",
            "147: [D loss: 0.669643, acc: 0.515625]  [A loss: 1.699083, acc: 0.000000]\n",
            "148: [D loss: 0.526842, acc: 0.832031]  [A loss: 0.729060, acc: 0.414062]\n",
            "149: [D loss: 0.750619, acc: 0.500000]  [A loss: 1.750275, acc: 0.000000]\n",
            "150: [D loss: 0.563887, acc: 0.761719]  [A loss: 0.668715, acc: 0.554688]\n",
            "151: [D loss: 0.742144, acc: 0.507812]  [A loss: 1.614317, acc: 0.000000]\n",
            "152: [D loss: 0.556810, acc: 0.783203]  [A loss: 0.774162, acc: 0.363281]\n",
            "153: [D loss: 0.702464, acc: 0.505859]  [A loss: 1.582880, acc: 0.000000]\n",
            "154: [D loss: 0.540527, acc: 0.818359]  [A loss: 0.817489, acc: 0.320312]\n",
            "155: [D loss: 0.680389, acc: 0.519531]  [A loss: 1.594825, acc: 0.000000]\n",
            "156: [D loss: 0.563835, acc: 0.769531]  [A loss: 0.796807, acc: 0.343750]\n",
            "157: [D loss: 0.732919, acc: 0.511719]  [A loss: 1.788265, acc: 0.000000]\n",
            "158: [D loss: 0.568585, acc: 0.751953]  [A loss: 0.666148, acc: 0.578125]\n",
            "159: [D loss: 0.738686, acc: 0.505859]  [A loss: 1.688690, acc: 0.000000]\n",
            "160: [D loss: 0.574729, acc: 0.755859]  [A loss: 0.707402, acc: 0.519531]\n",
            "161: [D loss: 0.755317, acc: 0.501953]  [A loss: 1.710468, acc: 0.000000]\n",
            "162: [D loss: 0.570755, acc: 0.751953]  [A loss: 0.813164, acc: 0.320312]\n",
            "163: [D loss: 0.703465, acc: 0.523438]  [A loss: 1.560974, acc: 0.000000]\n",
            "164: [D loss: 0.571955, acc: 0.763672]  [A loss: 0.781842, acc: 0.363281]\n",
            "165: [D loss: 0.710845, acc: 0.525391]  [A loss: 1.673455, acc: 0.000000]\n",
            "166: [D loss: 0.576470, acc: 0.748047]  [A loss: 0.727624, acc: 0.453125]\n",
            "167: [D loss: 0.726150, acc: 0.505859]  [A loss: 1.664882, acc: 0.000000]\n",
            "168: [D loss: 0.580390, acc: 0.728516]  [A loss: 0.680420, acc: 0.542969]\n",
            "169: [D loss: 0.745263, acc: 0.513672]  [A loss: 1.638800, acc: 0.003906]\n",
            "170: [D loss: 0.573095, acc: 0.748047]  [A loss: 0.734786, acc: 0.453125]\n",
            "171: [D loss: 0.718341, acc: 0.509766]  [A loss: 1.626893, acc: 0.000000]\n",
            "172: [D loss: 0.584529, acc: 0.748047]  [A loss: 0.720739, acc: 0.480469]\n",
            "173: [D loss: 0.716362, acc: 0.511719]  [A loss: 1.513912, acc: 0.000000]\n",
            "174: [D loss: 0.610509, acc: 0.722656]  [A loss: 0.788345, acc: 0.339844]\n",
            "175: [D loss: 0.709315, acc: 0.509766]  [A loss: 1.536890, acc: 0.000000]\n",
            "176: [D loss: 0.583217, acc: 0.763672]  [A loss: 0.840581, acc: 0.250000]\n",
            "177: [D loss: 0.693737, acc: 0.527344]  [A loss: 1.461587, acc: 0.003906]\n",
            "178: [D loss: 0.600045, acc: 0.748047]  [A loss: 0.764103, acc: 0.382812]\n",
            "179: [D loss: 0.696113, acc: 0.509766]  [A loss: 1.598203, acc: 0.000000]\n",
            "180: [D loss: 0.580037, acc: 0.755859]  [A loss: 0.777615, acc: 0.421875]\n",
            "181: [D loss: 0.690830, acc: 0.521484]  [A loss: 1.515821, acc: 0.000000]\n",
            "182: [D loss: 0.585283, acc: 0.736328]  [A loss: 0.692077, acc: 0.511719]\n",
            "183: [D loss: 0.746161, acc: 0.509766]  [A loss: 1.648911, acc: 0.000000]\n",
            "184: [D loss: 0.587755, acc: 0.734375]  [A loss: 0.681997, acc: 0.558594]\n",
            "185: [D loss: 0.733045, acc: 0.515625]  [A loss: 1.537174, acc: 0.000000]\n",
            "186: [D loss: 0.607072, acc: 0.710938]  [A loss: 0.745727, acc: 0.402344]\n",
            "187: [D loss: 0.714355, acc: 0.515625]  [A loss: 1.574183, acc: 0.000000]\n",
            "188: [D loss: 0.587813, acc: 0.738281]  [A loss: 0.707280, acc: 0.507812]\n",
            "189: [D loss: 0.702476, acc: 0.521484]  [A loss: 1.531094, acc: 0.000000]\n",
            "190: [D loss: 0.608114, acc: 0.703125]  [A loss: 0.756451, acc: 0.363281]\n",
            "191: [D loss: 0.705438, acc: 0.521484]  [A loss: 1.583981, acc: 0.000000]\n",
            "192: [D loss: 0.610230, acc: 0.699219]  [A loss: 0.785588, acc: 0.367188]\n",
            "193: [D loss: 0.703264, acc: 0.519531]  [A loss: 1.652510, acc: 0.000000]\n",
            "194: [D loss: 0.602010, acc: 0.707031]  [A loss: 0.687039, acc: 0.566406]\n",
            "195: [D loss: 0.729664, acc: 0.519531]  [A loss: 1.602481, acc: 0.000000]\n",
            "196: [D loss: 0.592068, acc: 0.708984]  [A loss: 0.762406, acc: 0.386719]\n",
            "197: [D loss: 0.708021, acc: 0.521484]  [A loss: 1.564375, acc: 0.000000]\n",
            "198: [D loss: 0.599777, acc: 0.703125]  [A loss: 0.742377, acc: 0.390625]\n",
            "199: [D loss: 0.696382, acc: 0.513672]  [A loss: 1.487018, acc: 0.000000]\n",
            "200: [D loss: 0.600151, acc: 0.707031]  [A loss: 0.753092, acc: 0.414062]\n",
            "201: [D loss: 0.710736, acc: 0.531250]  [A loss: 1.504564, acc: 0.000000]\n",
            "202: [D loss: 0.598270, acc: 0.705078]  [A loss: 0.741426, acc: 0.441406]\n",
            "203: [D loss: 0.703205, acc: 0.533203]  [A loss: 1.604653, acc: 0.000000]\n",
            "204: [D loss: 0.608351, acc: 0.689453]  [A loss: 0.791723, acc: 0.367188]\n",
            "205: [D loss: 0.688549, acc: 0.531250]  [A loss: 1.513742, acc: 0.000000]\n",
            "206: [D loss: 0.594367, acc: 0.736328]  [A loss: 0.765916, acc: 0.363281]\n",
            "207: [D loss: 0.731166, acc: 0.527344]  [A loss: 1.622870, acc: 0.000000]\n",
            "208: [D loss: 0.619360, acc: 0.669922]  [A loss: 0.625689, acc: 0.644531]\n",
            "209: [D loss: 0.747403, acc: 0.515625]  [A loss: 1.586771, acc: 0.000000]\n",
            "210: [D loss: 0.624349, acc: 0.646484]  [A loss: 0.641167, acc: 0.640625]\n",
            "211: [D loss: 0.750225, acc: 0.513672]  [A loss: 1.363739, acc: 0.000000]\n",
            "212: [D loss: 0.617458, acc: 0.679688]  [A loss: 0.806894, acc: 0.296875]\n",
            "213: [D loss: 0.694027, acc: 0.527344]  [A loss: 1.389730, acc: 0.000000]\n",
            "214: [D loss: 0.645185, acc: 0.628906]  [A loss: 0.892986, acc: 0.207031]\n",
            "215: [D loss: 0.666928, acc: 0.564453]  [A loss: 1.190247, acc: 0.023438]\n",
            "216: [D loss: 0.623431, acc: 0.656250]  [A loss: 1.007878, acc: 0.089844]\n",
            "217: [D loss: 0.635572, acc: 0.597656]  [A loss: 1.265028, acc: 0.015625]\n",
            "218: [D loss: 0.616015, acc: 0.691406]  [A loss: 1.011317, acc: 0.101562]\n",
            "219: [D loss: 0.636413, acc: 0.587891]  [A loss: 1.438042, acc: 0.000000]\n",
            "220: [D loss: 0.623012, acc: 0.679688]  [A loss: 0.733638, acc: 0.421875]\n",
            "221: [D loss: 0.718846, acc: 0.511719]  [A loss: 1.829887, acc: 0.000000]\n",
            "222: [D loss: 0.671312, acc: 0.576172]  [A loss: 0.577109, acc: 0.734375]\n",
            "223: [D loss: 0.785341, acc: 0.501953]  [A loss: 1.489174, acc: 0.000000]\n",
            "224: [D loss: 0.652505, acc: 0.582031]  [A loss: 0.729988, acc: 0.492188]\n",
            "225: [D loss: 0.704139, acc: 0.525391]  [A loss: 1.331491, acc: 0.003906]\n",
            "226: [D loss: 0.633092, acc: 0.660156]  [A loss: 0.830279, acc: 0.261719]\n",
            "227: [D loss: 0.696983, acc: 0.525391]  [A loss: 1.321878, acc: 0.007812]\n",
            "228: [D loss: 0.623434, acc: 0.699219]  [A loss: 0.765839, acc: 0.367188]\n",
            "229: [D loss: 0.693203, acc: 0.527344]  [A loss: 1.407266, acc: 0.000000]\n",
            "230: [D loss: 0.621094, acc: 0.689453]  [A loss: 0.715149, acc: 0.476562]\n",
            "231: [D loss: 0.718310, acc: 0.515625]  [A loss: 1.538530, acc: 0.000000]\n",
            "232: [D loss: 0.642779, acc: 0.609375]  [A loss: 0.652697, acc: 0.628906]\n",
            "233: [D loss: 0.743927, acc: 0.501953]  [A loss: 1.420703, acc: 0.000000]\n",
            "234: [D loss: 0.630101, acc: 0.662109]  [A loss: 0.696565, acc: 0.507812]\n",
            "235: [D loss: 0.708835, acc: 0.519531]  [A loss: 1.323839, acc: 0.003906]\n",
            "236: [D loss: 0.633427, acc: 0.644531]  [A loss: 0.730455, acc: 0.457031]\n",
            "237: [D loss: 0.702197, acc: 0.525391]  [A loss: 1.318313, acc: 0.000000]\n",
            "238: [D loss: 0.638462, acc: 0.636719]  [A loss: 0.716440, acc: 0.476562]\n",
            "239: [D loss: 0.720259, acc: 0.513672]  [A loss: 1.310345, acc: 0.003906]\n",
            "240: [D loss: 0.644189, acc: 0.642578]  [A loss: 0.724857, acc: 0.472656]\n",
            "241: [D loss: 0.704034, acc: 0.525391]  [A loss: 1.366720, acc: 0.000000]\n",
            "242: [D loss: 0.639689, acc: 0.642578]  [A loss: 0.716439, acc: 0.488281]\n",
            "243: [D loss: 0.721551, acc: 0.505859]  [A loss: 1.314084, acc: 0.003906]\n",
            "244: [D loss: 0.649377, acc: 0.628906]  [A loss: 0.706090, acc: 0.496094]\n",
            "245: [D loss: 0.716254, acc: 0.521484]  [A loss: 1.328785, acc: 0.000000]\n",
            "246: [D loss: 0.663613, acc: 0.605469]  [A loss: 0.671599, acc: 0.593750]\n",
            "247: [D loss: 0.724643, acc: 0.513672]  [A loss: 1.297927, acc: 0.007812]\n",
            "248: [D loss: 0.635702, acc: 0.648438]  [A loss: 0.715857, acc: 0.460938]\n",
            "249: [D loss: 0.735088, acc: 0.509766]  [A loss: 1.267813, acc: 0.007812]\n",
            "250: [D loss: 0.655907, acc: 0.625000]  [A loss: 0.748428, acc: 0.406250]\n",
            "251: [D loss: 0.703926, acc: 0.533203]  [A loss: 1.250175, acc: 0.015625]\n",
            "252: [D loss: 0.649833, acc: 0.625000]  [A loss: 0.718005, acc: 0.476562]\n",
            "253: [D loss: 0.693047, acc: 0.537109]  [A loss: 1.318581, acc: 0.007812]\n",
            "254: [D loss: 0.674184, acc: 0.568359]  [A loss: 0.771137, acc: 0.359375]\n",
            "255: [D loss: 0.702926, acc: 0.523438]  [A loss: 1.210890, acc: 0.015625]\n",
            "256: [D loss: 0.658624, acc: 0.595703]  [A loss: 0.730788, acc: 0.472656]\n",
            "257: [D loss: 0.723834, acc: 0.507812]  [A loss: 1.341797, acc: 0.003906]\n",
            "258: [D loss: 0.670120, acc: 0.587891]  [A loss: 0.622766, acc: 0.714844]\n",
            "259: [D loss: 0.739580, acc: 0.505859]  [A loss: 1.352519, acc: 0.000000]\n",
            "260: [D loss: 0.659506, acc: 0.597656]  [A loss: 0.652602, acc: 0.628906]\n",
            "261: [D loss: 0.719197, acc: 0.517578]  [A loss: 1.199659, acc: 0.000000]\n",
            "262: [D loss: 0.657566, acc: 0.625000]  [A loss: 0.706854, acc: 0.464844]\n",
            "263: [D loss: 0.716911, acc: 0.523438]  [A loss: 1.207813, acc: 0.003906]\n",
            "264: [D loss: 0.656253, acc: 0.623047]  [A loss: 0.730819, acc: 0.425781]\n",
            "265: [D loss: 0.713260, acc: 0.515625]  [A loss: 1.152388, acc: 0.003906]\n",
            "266: [D loss: 0.659548, acc: 0.597656]  [A loss: 0.791812, acc: 0.312500]\n",
            "267: [D loss: 0.688771, acc: 0.523438]  [A loss: 1.121982, acc: 0.011719]\n",
            "268: [D loss: 0.652464, acc: 0.666016]  [A loss: 0.820441, acc: 0.253906]\n",
            "269: [D loss: 0.703374, acc: 0.529297]  [A loss: 1.237538, acc: 0.000000]\n",
            "270: [D loss: 0.662215, acc: 0.605469]  [A loss: 0.728155, acc: 0.468750]\n",
            "271: [D loss: 0.734058, acc: 0.513672]  [A loss: 1.326206, acc: 0.000000]\n",
            "272: [D loss: 0.657663, acc: 0.609375]  [A loss: 0.676734, acc: 0.550781]\n",
            "273: [D loss: 0.733449, acc: 0.513672]  [A loss: 1.313178, acc: 0.000000]\n",
            "274: [D loss: 0.660427, acc: 0.587891]  [A loss: 0.640514, acc: 0.648438]\n",
            "275: [D loss: 0.725535, acc: 0.505859]  [A loss: 1.203813, acc: 0.000000]\n",
            "276: [D loss: 0.658071, acc: 0.607422]  [A loss: 0.750490, acc: 0.398438]\n",
            "277: [D loss: 0.677203, acc: 0.544922]  [A loss: 1.123859, acc: 0.003906]\n",
            "278: [D loss: 0.667558, acc: 0.628906]  [A loss: 0.864445, acc: 0.191406]\n",
            "279: [D loss: 0.682664, acc: 0.546875]  [A loss: 1.061814, acc: 0.023438]\n",
            "280: [D loss: 0.669869, acc: 0.603516]  [A loss: 0.858714, acc: 0.160156]\n",
            "281: [D loss: 0.671405, acc: 0.558594]  [A loss: 1.162349, acc: 0.011719]\n",
            "282: [D loss: 0.654281, acc: 0.626953]  [A loss: 0.799570, acc: 0.277344]\n",
            "283: [D loss: 0.713230, acc: 0.511719]  [A loss: 1.287224, acc: 0.007812]\n",
            "284: [D loss: 0.673437, acc: 0.572266]  [A loss: 0.725549, acc: 0.445312]\n",
            "285: [D loss: 0.708195, acc: 0.511719]  [A loss: 1.246102, acc: 0.003906]\n",
            "286: [D loss: 0.661457, acc: 0.607422]  [A loss: 0.667351, acc: 0.570312]\n",
            "287: [D loss: 0.732627, acc: 0.501953]  [A loss: 1.285387, acc: 0.000000]\n",
            "288: [D loss: 0.671053, acc: 0.566406]  [A loss: 0.627734, acc: 0.691406]\n",
            "289: [D loss: 0.722804, acc: 0.513672]  [A loss: 1.217732, acc: 0.003906]\n",
            "290: [D loss: 0.666342, acc: 0.582031]  [A loss: 0.653676, acc: 0.632812]\n",
            "291: [D loss: 0.724247, acc: 0.509766]  [A loss: 1.158298, acc: 0.003906]\n",
            "292: [D loss: 0.668997, acc: 0.625000]  [A loss: 0.731060, acc: 0.398438]\n",
            "293: [D loss: 0.700400, acc: 0.527344]  [A loss: 1.061104, acc: 0.023438]\n",
            "294: [D loss: 0.666975, acc: 0.599609]  [A loss: 0.810583, acc: 0.242188]\n",
            "295: [D loss: 0.673427, acc: 0.574219]  [A loss: 1.034292, acc: 0.011719]\n",
            "296: [D loss: 0.669482, acc: 0.595703]  [A loss: 0.825377, acc: 0.242188]\n",
            "297: [D loss: 0.682929, acc: 0.546875]  [A loss: 1.109503, acc: 0.003906]\n",
            "298: [D loss: 0.668858, acc: 0.591797]  [A loss: 0.780341, acc: 0.296875]\n",
            "299: [D loss: 0.682137, acc: 0.546875]  [A loss: 1.122685, acc: 0.003906]\n",
            "300: [D loss: 0.660896, acc: 0.615234]  [A loss: 0.774661, acc: 0.304688]\n",
            "301: [D loss: 0.707471, acc: 0.513672]  [A loss: 1.211045, acc: 0.007812]\n",
            "302: [D loss: 0.651069, acc: 0.644531]  [A loss: 0.723061, acc: 0.445312]\n",
            "303: [D loss: 0.714732, acc: 0.519531]  [A loss: 1.277696, acc: 0.000000]\n",
            "304: [D loss: 0.665503, acc: 0.595703]  [A loss: 0.657555, acc: 0.625000]\n",
            "305: [D loss: 0.724950, acc: 0.501953]  [A loss: 1.244074, acc: 0.000000]\n",
            "306: [D loss: 0.667773, acc: 0.603516]  [A loss: 0.659240, acc: 0.593750]\n",
            "307: [D loss: 0.718004, acc: 0.509766]  [A loss: 1.140984, acc: 0.000000]\n",
            "308: [D loss: 0.685953, acc: 0.568359]  [A loss: 0.744990, acc: 0.386719]\n",
            "309: [D loss: 0.694592, acc: 0.531250]  [A loss: 1.066288, acc: 0.019531]\n",
            "310: [D loss: 0.670862, acc: 0.583984]  [A loss: 0.840283, acc: 0.222656]\n",
            "311: [D loss: 0.682628, acc: 0.558594]  [A loss: 1.008096, acc: 0.023438]\n",
            "312: [D loss: 0.661317, acc: 0.626953]  [A loss: 0.827823, acc: 0.183594]\n",
            "313: [D loss: 0.686307, acc: 0.548828]  [A loss: 1.050993, acc: 0.023438]\n",
            "314: [D loss: 0.673137, acc: 0.597656]  [A loss: 0.773652, acc: 0.300781]\n",
            "315: [D loss: 0.691672, acc: 0.529297]  [A loss: 1.188776, acc: 0.007812]\n",
            "316: [D loss: 0.689971, acc: 0.525391]  [A loss: 0.723235, acc: 0.425781]\n",
            "317: [D loss: 0.699966, acc: 0.531250]  [A loss: 1.179844, acc: 0.003906]\n",
            "318: [D loss: 0.672436, acc: 0.591797]  [A loss: 0.700921, acc: 0.484375]\n",
            "319: [D loss: 0.706059, acc: 0.513672]  [A loss: 1.158418, acc: 0.000000]\n",
            "320: [D loss: 0.662302, acc: 0.605469]  [A loss: 0.725726, acc: 0.429688]\n",
            "321: [D loss: 0.694765, acc: 0.546875]  [A loss: 1.121227, acc: 0.019531]\n",
            "322: [D loss: 0.676437, acc: 0.548828]  [A loss: 0.774292, acc: 0.308594]\n",
            "323: [D loss: 0.700681, acc: 0.519531]  [A loss: 1.134730, acc: 0.000000]\n",
            "324: [D loss: 0.660164, acc: 0.593750]  [A loss: 0.692359, acc: 0.519531]\n",
            "325: [D loss: 0.697709, acc: 0.513672]  [A loss: 1.112559, acc: 0.007812]\n",
            "326: [D loss: 0.673626, acc: 0.580078]  [A loss: 0.706044, acc: 0.476562]\n",
            "327: [D loss: 0.696520, acc: 0.519531]  [A loss: 1.097460, acc: 0.007812]\n",
            "328: [D loss: 0.665629, acc: 0.611328]  [A loss: 0.762683, acc: 0.332031]\n",
            "329: [D loss: 0.695057, acc: 0.525391]  [A loss: 1.077886, acc: 0.015625]\n",
            "330: [D loss: 0.665336, acc: 0.613281]  [A loss: 0.776170, acc: 0.324219]\n",
            "331: [D loss: 0.686620, acc: 0.515625]  [A loss: 1.023678, acc: 0.011719]\n",
            "332: [D loss: 0.662496, acc: 0.625000]  [A loss: 0.795741, acc: 0.242188]\n",
            "333: [D loss: 0.674116, acc: 0.544922]  [A loss: 1.069040, acc: 0.003906]\n",
            "334: [D loss: 0.655979, acc: 0.638672]  [A loss: 0.785584, acc: 0.308594]\n",
            "335: [D loss: 0.697555, acc: 0.525391]  [A loss: 1.113483, acc: 0.003906]\n",
            "336: [D loss: 0.670287, acc: 0.587891]  [A loss: 0.786398, acc: 0.304688]\n",
            "337: [D loss: 0.685213, acc: 0.533203]  [A loss: 1.115938, acc: 0.007812]\n",
            "338: [D loss: 0.667815, acc: 0.597656]  [A loss: 0.739323, acc: 0.425781]\n",
            "339: [D loss: 0.701180, acc: 0.515625]  [A loss: 1.203945, acc: 0.000000]\n",
            "340: [D loss: 0.675532, acc: 0.580078]  [A loss: 0.632922, acc: 0.671875]\n",
            "341: [D loss: 0.732492, acc: 0.507812]  [A loss: 1.185853, acc: 0.000000]\n",
            "342: [D loss: 0.676406, acc: 0.550781]  [A loss: 0.662738, acc: 0.621094]\n",
            "343: [D loss: 0.711281, acc: 0.505859]  [A loss: 0.977728, acc: 0.031250]\n",
            "344: [D loss: 0.656940, acc: 0.628906]  [A loss: 0.768588, acc: 0.316406]\n",
            "345: [D loss: 0.684125, acc: 0.537109]  [A loss: 0.991025, acc: 0.011719]\n",
            "346: [D loss: 0.666919, acc: 0.615234]  [A loss: 0.772161, acc: 0.281250]\n",
            "347: [D loss: 0.677533, acc: 0.550781]  [A loss: 0.962914, acc: 0.042969]\n",
            "348: [D loss: 0.657243, acc: 0.628906]  [A loss: 0.850691, acc: 0.144531]\n",
            "349: [D loss: 0.668264, acc: 0.574219]  [A loss: 0.985798, acc: 0.046875]\n",
            "350: [D loss: 0.659627, acc: 0.595703]  [A loss: 0.887788, acc: 0.113281]\n",
            "351: [D loss: 0.658435, acc: 0.601562]  [A loss: 1.016335, acc: 0.031250]\n",
            "352: [D loss: 0.658083, acc: 0.623047]  [A loss: 0.835387, acc: 0.226562]\n",
            "353: [D loss: 0.671338, acc: 0.556641]  [A loss: 1.126227, acc: 0.011719]\n",
            "354: [D loss: 0.671039, acc: 0.613281]  [A loss: 0.759656, acc: 0.347656]\n",
            "355: [D loss: 0.701488, acc: 0.519531]  [A loss: 1.224848, acc: 0.011719]\n",
            "356: [D loss: 0.657754, acc: 0.591797]  [A loss: 0.656772, acc: 0.609375]\n",
            "357: [D loss: 0.728603, acc: 0.507812]  [A loss: 1.268354, acc: 0.003906]\n",
            "358: [D loss: 0.673940, acc: 0.554688]  [A loss: 0.671221, acc: 0.550781]\n",
            "359: [D loss: 0.700421, acc: 0.513672]  [A loss: 1.071051, acc: 0.027344]\n",
            "360: [D loss: 0.662451, acc: 0.595703]  [A loss: 0.742373, acc: 0.390625]\n",
            "361: [D loss: 0.686721, acc: 0.537109]  [A loss: 1.030145, acc: 0.019531]\n",
            "362: [D loss: 0.668210, acc: 0.597656]  [A loss: 0.808198, acc: 0.222656]\n",
            "363: [D loss: 0.675626, acc: 0.566406]  [A loss: 0.973300, acc: 0.066406]\n",
            "364: [D loss: 0.652709, acc: 0.615234]  [A loss: 0.845902, acc: 0.199219]\n",
            "365: [D loss: 0.683670, acc: 0.572266]  [A loss: 1.039307, acc: 0.007812]\n",
            "366: [D loss: 0.657133, acc: 0.632812]  [A loss: 0.753228, acc: 0.343750]\n",
            "367: [D loss: 0.698129, acc: 0.521484]  [A loss: 1.092384, acc: 0.007812]\n",
            "368: [D loss: 0.673754, acc: 0.580078]  [A loss: 0.718851, acc: 0.468750]\n",
            "369: [D loss: 0.684638, acc: 0.529297]  [A loss: 1.120600, acc: 0.015625]\n",
            "370: [D loss: 0.659083, acc: 0.613281]  [A loss: 0.718479, acc: 0.445312]\n",
            "371: [D loss: 0.691177, acc: 0.546875]  [A loss: 1.124372, acc: 0.003906]\n",
            "372: [D loss: 0.667832, acc: 0.583984]  [A loss: 0.704318, acc: 0.503906]\n",
            "373: [D loss: 0.712530, acc: 0.509766]  [A loss: 1.105599, acc: 0.019531]\n",
            "374: [D loss: 0.667158, acc: 0.595703]  [A loss: 0.726807, acc: 0.441406]\n",
            "375: [D loss: 0.695181, acc: 0.535156]  [A loss: 1.036329, acc: 0.023438]\n",
            "376: [D loss: 0.679933, acc: 0.564453]  [A loss: 0.775450, acc: 0.324219]\n",
            "377: [D loss: 0.669961, acc: 0.546875]  [A loss: 0.946539, acc: 0.089844]\n",
            "378: [D loss: 0.668741, acc: 0.599609]  [A loss: 0.860115, acc: 0.179688]\n",
            "379: [D loss: 0.680911, acc: 0.554688]  [A loss: 0.937040, acc: 0.066406]\n",
            "380: [D loss: 0.673055, acc: 0.587891]  [A loss: 0.875910, acc: 0.167969]\n",
            "381: [D loss: 0.665719, acc: 0.593750]  [A loss: 0.998452, acc: 0.039062]\n",
            "382: [D loss: 0.665450, acc: 0.611328]  [A loss: 0.870164, acc: 0.156250]\n",
            "383: [D loss: 0.674149, acc: 0.580078]  [A loss: 1.034019, acc: 0.031250]\n",
            "384: [D loss: 0.667363, acc: 0.578125]  [A loss: 0.831298, acc: 0.199219]\n",
            "385: [D loss: 0.675808, acc: 0.566406]  [A loss: 1.183091, acc: 0.003906]\n",
            "386: [D loss: 0.673165, acc: 0.550781]  [A loss: 0.683887, acc: 0.554688]\n",
            "387: [D loss: 0.706614, acc: 0.531250]  [A loss: 1.263943, acc: 0.003906]\n",
            "388: [D loss: 0.666239, acc: 0.574219]  [A loss: 0.628391, acc: 0.671875]\n",
            "389: [D loss: 0.750287, acc: 0.507812]  [A loss: 1.066636, acc: 0.031250]\n",
            "390: [D loss: 0.664963, acc: 0.580078]  [A loss: 0.745236, acc: 0.347656]\n",
            "391: [D loss: 0.684677, acc: 0.529297]  [A loss: 0.913440, acc: 0.101562]\n",
            "392: [D loss: 0.662408, acc: 0.570312]  [A loss: 0.855288, acc: 0.195312]\n",
            "393: [D loss: 0.673953, acc: 0.591797]  [A loss: 0.883768, acc: 0.113281]\n",
            "394: [D loss: 0.679532, acc: 0.574219]  [A loss: 0.895691, acc: 0.121094]\n",
            "395: [D loss: 0.664072, acc: 0.587891]  [A loss: 0.906114, acc: 0.117188]\n",
            "396: [D loss: 0.685398, acc: 0.544922]  [A loss: 0.961412, acc: 0.066406]\n",
            "397: [D loss: 0.654458, acc: 0.636719]  [A loss: 0.872204, acc: 0.148438]\n",
            "398: [D loss: 0.683699, acc: 0.568359]  [A loss: 1.030422, acc: 0.023438]\n",
            "399: [D loss: 0.678102, acc: 0.585938]  [A loss: 0.832050, acc: 0.222656]\n",
            "400: [D loss: 0.669854, acc: 0.580078]  [A loss: 1.097741, acc: 0.031250]\n",
            "401: [D loss: 0.644740, acc: 0.662109]  [A loss: 0.819761, acc: 0.289062]\n",
            "402: [D loss: 0.686196, acc: 0.546875]  [A loss: 1.108321, acc: 0.023438]\n",
            "403: [D loss: 0.680758, acc: 0.558594]  [A loss: 0.707770, acc: 0.488281]\n",
            "404: [D loss: 0.708085, acc: 0.509766]  [A loss: 1.278808, acc: 0.000000]\n",
            "405: [D loss: 0.681641, acc: 0.544922]  [A loss: 0.617934, acc: 0.750000]\n",
            "406: [D loss: 0.717752, acc: 0.511719]  [A loss: 1.088039, acc: 0.007812]\n",
            "407: [D loss: 0.676429, acc: 0.568359]  [A loss: 0.745787, acc: 0.382812]\n",
            "408: [D loss: 0.700796, acc: 0.511719]  [A loss: 0.970221, acc: 0.042969]\n",
            "409: [D loss: 0.664637, acc: 0.615234]  [A loss: 0.809000, acc: 0.195312]\n",
            "410: [D loss: 0.678893, acc: 0.582031]  [A loss: 0.980134, acc: 0.035156]\n",
            "411: [D loss: 0.671296, acc: 0.585938]  [A loss: 0.775903, acc: 0.285156]\n",
            "412: [D loss: 0.675430, acc: 0.564453]  [A loss: 0.954622, acc: 0.085938]\n",
            "413: [D loss: 0.662889, acc: 0.609375]  [A loss: 0.814200, acc: 0.226562]\n",
            "414: [D loss: 0.676120, acc: 0.576172]  [A loss: 1.005785, acc: 0.050781]\n",
            "415: [D loss: 0.662588, acc: 0.628906]  [A loss: 0.808163, acc: 0.242188]\n",
            "416: [D loss: 0.678864, acc: 0.550781]  [A loss: 1.060796, acc: 0.019531]\n",
            "417: [D loss: 0.655071, acc: 0.621094]  [A loss: 0.732967, acc: 0.402344]\n",
            "418: [D loss: 0.682594, acc: 0.546875]  [A loss: 1.158193, acc: 0.003906]\n",
            "419: [D loss: 0.667551, acc: 0.562500]  [A loss: 0.653609, acc: 0.625000]\n",
            "420: [D loss: 0.720924, acc: 0.513672]  [A loss: 1.158901, acc: 0.007812]\n",
            "421: [D loss: 0.665717, acc: 0.582031]  [A loss: 0.694363, acc: 0.527344]\n",
            "422: [D loss: 0.711541, acc: 0.527344]  [A loss: 1.071205, acc: 0.007812]\n",
            "423: [D loss: 0.675231, acc: 0.589844]  [A loss: 0.748801, acc: 0.320312]\n",
            "424: [D loss: 0.683991, acc: 0.513672]  [A loss: 0.960941, acc: 0.066406]\n",
            "425: [D loss: 0.671899, acc: 0.552734]  [A loss: 0.846547, acc: 0.167969]\n",
            "426: [D loss: 0.676705, acc: 0.541016]  [A loss: 0.894992, acc: 0.109375]\n",
            "427: [D loss: 0.680373, acc: 0.554688]  [A loss: 0.890994, acc: 0.121094]\n",
            "428: [D loss: 0.669583, acc: 0.582031]  [A loss: 0.914301, acc: 0.066406]\n",
            "429: [D loss: 0.666064, acc: 0.595703]  [A loss: 0.877722, acc: 0.109375]\n",
            "430: [D loss: 0.674521, acc: 0.578125]  [A loss: 0.896066, acc: 0.121094]\n",
            "431: [D loss: 0.652650, acc: 0.617188]  [A loss: 0.869412, acc: 0.171875]\n",
            "432: [D loss: 0.690000, acc: 0.541016]  [A loss: 1.006555, acc: 0.058594]\n",
            "433: [D loss: 0.694864, acc: 0.521484]  [A loss: 0.913586, acc: 0.097656]\n",
            "434: [D loss: 0.666085, acc: 0.589844]  [A loss: 0.935470, acc: 0.105469]\n",
            "435: [D loss: 0.664627, acc: 0.595703]  [A loss: 0.991504, acc: 0.058594]\n",
            "436: [D loss: 0.658653, acc: 0.621094]  [A loss: 0.853255, acc: 0.187500]\n",
            "437: [D loss: 0.664478, acc: 0.589844]  [A loss: 1.086223, acc: 0.015625]\n",
            "438: [D loss: 0.663248, acc: 0.582031]  [A loss: 0.681641, acc: 0.558594]\n",
            "439: [D loss: 0.699288, acc: 0.533203]  [A loss: 1.263081, acc: 0.007812]\n",
            "440: [D loss: 0.687562, acc: 0.560547]  [A loss: 0.645616, acc: 0.632812]\n",
            "441: [D loss: 0.720500, acc: 0.515625]  [A loss: 1.115579, acc: 0.003906]\n",
            "442: [D loss: 0.666466, acc: 0.589844]  [A loss: 0.739374, acc: 0.378906]\n",
            "443: [D loss: 0.682146, acc: 0.533203]  [A loss: 0.943042, acc: 0.070312]\n",
            "444: [D loss: 0.653649, acc: 0.634766]  [A loss: 0.771395, acc: 0.332031]\n",
            "445: [D loss: 0.672711, acc: 0.574219]  [A loss: 1.024877, acc: 0.011719]\n",
            "446: [D loss: 0.663283, acc: 0.603516]  [A loss: 0.775261, acc: 0.308594]\n",
            "447: [D loss: 0.674516, acc: 0.566406]  [A loss: 0.991545, acc: 0.054688]\n",
            "448: [D loss: 0.663134, acc: 0.587891]  [A loss: 0.765191, acc: 0.343750]\n",
            "449: [D loss: 0.689876, acc: 0.548828]  [A loss: 1.084590, acc: 0.011719]\n",
            "450: [D loss: 0.658649, acc: 0.582031]  [A loss: 0.747225, acc: 0.406250]\n",
            "451: [D loss: 0.703837, acc: 0.511719]  [A loss: 1.061407, acc: 0.019531]\n",
            "452: [D loss: 0.662698, acc: 0.607422]  [A loss: 0.734466, acc: 0.425781]\n",
            "453: [D loss: 0.694070, acc: 0.537109]  [A loss: 1.027384, acc: 0.039062]\n",
            "454: [D loss: 0.662728, acc: 0.580078]  [A loss: 0.781479, acc: 0.304688]\n",
            "455: [D loss: 0.688558, acc: 0.521484]  [A loss: 1.005057, acc: 0.035156]\n",
            "456: [D loss: 0.656836, acc: 0.617188]  [A loss: 0.756613, acc: 0.367188]\n",
            "457: [D loss: 0.692782, acc: 0.544922]  [A loss: 1.008397, acc: 0.050781]\n",
            "458: [D loss: 0.653776, acc: 0.642578]  [A loss: 0.777771, acc: 0.304688]\n",
            "459: [D loss: 0.684688, acc: 0.566406]  [A loss: 1.002442, acc: 0.042969]\n",
            "460: [D loss: 0.658689, acc: 0.630859]  [A loss: 0.794125, acc: 0.253906]\n",
            "461: [D loss: 0.684898, acc: 0.558594]  [A loss: 1.039063, acc: 0.046875]\n",
            "462: [D loss: 0.670029, acc: 0.572266]  [A loss: 0.784935, acc: 0.316406]\n",
            "463: [D loss: 0.709661, acc: 0.521484]  [A loss: 0.995160, acc: 0.035156]\n",
            "464: [D loss: 0.661821, acc: 0.609375]  [A loss: 0.769673, acc: 0.316406]\n",
            "465: [D loss: 0.673220, acc: 0.566406]  [A loss: 1.040489, acc: 0.039062]\n",
            "466: [D loss: 0.659518, acc: 0.601562]  [A loss: 0.760055, acc: 0.367188]\n",
            "467: [D loss: 0.671798, acc: 0.564453]  [A loss: 1.066952, acc: 0.023438]\n",
            "468: [D loss: 0.659877, acc: 0.611328]  [A loss: 0.777745, acc: 0.332031]\n",
            "469: [D loss: 0.673162, acc: 0.550781]  [A loss: 1.092863, acc: 0.007812]\n",
            "470: [D loss: 0.666026, acc: 0.585938]  [A loss: 0.730857, acc: 0.421875]\n",
            "471: [D loss: 0.696048, acc: 0.513672]  [A loss: 1.143960, acc: 0.019531]\n",
            "472: [D loss: 0.648720, acc: 0.605469]  [A loss: 0.700505, acc: 0.511719]\n",
            "473: [D loss: 0.691324, acc: 0.511719]  [A loss: 1.077354, acc: 0.015625]\n",
            "474: [D loss: 0.656086, acc: 0.607422]  [A loss: 0.763272, acc: 0.343750]\n",
            "475: [D loss: 0.694910, acc: 0.539062]  [A loss: 1.084356, acc: 0.007812]\n",
            "476: [D loss: 0.668303, acc: 0.593750]  [A loss: 0.763291, acc: 0.359375]\n",
            "477: [D loss: 0.684553, acc: 0.550781]  [A loss: 0.959297, acc: 0.058594]\n",
            "478: [D loss: 0.661286, acc: 0.615234]  [A loss: 0.829599, acc: 0.230469]\n",
            "479: [D loss: 0.662222, acc: 0.583984]  [A loss: 0.932852, acc: 0.101562]\n",
            "480: [D loss: 0.670461, acc: 0.572266]  [A loss: 0.883231, acc: 0.144531]\n",
            "481: [D loss: 0.665465, acc: 0.580078]  [A loss: 0.990108, acc: 0.031250]\n",
            "482: [D loss: 0.667643, acc: 0.591797]  [A loss: 0.851272, acc: 0.199219]\n",
            "483: [D loss: 0.670216, acc: 0.572266]  [A loss: 0.999751, acc: 0.042969]\n",
            "484: [D loss: 0.663143, acc: 0.597656]  [A loss: 0.788250, acc: 0.316406]\n",
            "485: [D loss: 0.670661, acc: 0.570312]  [A loss: 1.131090, acc: 0.011719]\n",
            "486: [D loss: 0.664989, acc: 0.589844]  [A loss: 0.694005, acc: 0.535156]\n",
            "487: [D loss: 0.695093, acc: 0.527344]  [A loss: 1.155280, acc: 0.011719]\n",
            "488: [D loss: 0.675068, acc: 0.556641]  [A loss: 0.694390, acc: 0.527344]\n",
            "489: [D loss: 0.712517, acc: 0.517578]  [A loss: 1.104767, acc: 0.011719]\n",
            "490: [D loss: 0.656229, acc: 0.621094]  [A loss: 0.711217, acc: 0.472656]\n",
            "491: [D loss: 0.691305, acc: 0.537109]  [A loss: 1.019242, acc: 0.031250]\n",
            "492: [D loss: 0.664253, acc: 0.613281]  [A loss: 0.751562, acc: 0.386719]\n",
            "493: [D loss: 0.690960, acc: 0.531250]  [A loss: 1.005937, acc: 0.066406]\n",
            "494: [D loss: 0.670768, acc: 0.587891]  [A loss: 0.796470, acc: 0.269531]\n",
            "495: [D loss: 0.688088, acc: 0.558594]  [A loss: 0.909202, acc: 0.105469]\n",
            "496: [D loss: 0.655873, acc: 0.617188]  [A loss: 0.896048, acc: 0.132812]\n",
            "497: [D loss: 0.664192, acc: 0.603516]  [A loss: 0.937549, acc: 0.109375]\n",
            "498: [D loss: 0.652122, acc: 0.636719]  [A loss: 0.879054, acc: 0.117188]\n",
            "499: [D loss: 0.660893, acc: 0.621094]  [A loss: 0.951089, acc: 0.089844]\n",
            "500: [D loss: 0.649439, acc: 0.640625]  [A loss: 0.837956, acc: 0.199219]\n",
            "501: [D loss: 0.671357, acc: 0.593750]  [A loss: 1.159734, acc: 0.015625]\n",
            "502: [D loss: 0.661859, acc: 0.601562]  [A loss: 0.697079, acc: 0.542969]\n",
            "503: [D loss: 0.707124, acc: 0.541016]  [A loss: 1.129074, acc: 0.007812]\n",
            "504: [D loss: 0.663544, acc: 0.597656]  [A loss: 0.715600, acc: 0.503906]\n",
            "505: [D loss: 0.691529, acc: 0.542969]  [A loss: 1.076928, acc: 0.039062]\n",
            "506: [D loss: 0.662883, acc: 0.595703]  [A loss: 0.764125, acc: 0.355469]\n",
            "507: [D loss: 0.692619, acc: 0.533203]  [A loss: 1.012054, acc: 0.035156]\n",
            "508: [D loss: 0.668025, acc: 0.585938]  [A loss: 0.824343, acc: 0.222656]\n",
            "509: [D loss: 0.694625, acc: 0.552734]  [A loss: 1.057663, acc: 0.031250]\n",
            "510: [D loss: 0.663764, acc: 0.603516]  [A loss: 0.791068, acc: 0.281250]\n",
            "511: [D loss: 0.673250, acc: 0.578125]  [A loss: 1.011620, acc: 0.058594]\n",
            "512: [D loss: 0.668315, acc: 0.580078]  [A loss: 0.793901, acc: 0.292969]\n",
            "513: [D loss: 0.676482, acc: 0.578125]  [A loss: 1.000196, acc: 0.027344]\n",
            "514: [D loss: 0.671511, acc: 0.597656]  [A loss: 0.881373, acc: 0.191406]\n",
            "515: [D loss: 0.684367, acc: 0.568359]  [A loss: 0.974008, acc: 0.054688]\n",
            "516: [D loss: 0.673395, acc: 0.591797]  [A loss: 0.831769, acc: 0.214844]\n",
            "517: [D loss: 0.683932, acc: 0.554688]  [A loss: 0.984768, acc: 0.097656]\n",
            "518: [D loss: 0.680102, acc: 0.568359]  [A loss: 0.795760, acc: 0.292969]\n",
            "519: [D loss: 0.682071, acc: 0.570312]  [A loss: 0.978338, acc: 0.085938]\n",
            "520: [D loss: 0.667390, acc: 0.593750]  [A loss: 0.829692, acc: 0.214844]\n",
            "521: [D loss: 0.667352, acc: 0.595703]  [A loss: 0.957492, acc: 0.046875]\n",
            "522: [D loss: 0.667862, acc: 0.580078]  [A loss: 0.872688, acc: 0.179688]\n",
            "523: [D loss: 0.684066, acc: 0.560547]  [A loss: 1.067124, acc: 0.035156]\n",
            "524: [D loss: 0.677079, acc: 0.583984]  [A loss: 0.792279, acc: 0.296875]\n",
            "525: [D loss: 0.673099, acc: 0.570312]  [A loss: 1.084658, acc: 0.035156]\n",
            "526: [D loss: 0.691857, acc: 0.537109]  [A loss: 0.738425, acc: 0.425781]\n",
            "527: [D loss: 0.714825, acc: 0.523438]  [A loss: 1.102597, acc: 0.011719]\n",
            "528: [D loss: 0.663804, acc: 0.605469]  [A loss: 0.677243, acc: 0.550781]\n",
            "529: [D loss: 0.696425, acc: 0.542969]  [A loss: 1.086290, acc: 0.011719]\n",
            "530: [D loss: 0.677364, acc: 0.560547]  [A loss: 0.704617, acc: 0.507812]\n",
            "531: [D loss: 0.698918, acc: 0.541016]  [A loss: 1.006545, acc: 0.039062]\n",
            "532: [D loss: 0.662193, acc: 0.585938]  [A loss: 0.784646, acc: 0.261719]\n",
            "533: [D loss: 0.695754, acc: 0.529297]  [A loss: 0.956233, acc: 0.117188]\n",
            "534: [D loss: 0.668727, acc: 0.617188]  [A loss: 0.835174, acc: 0.207031]\n",
            "535: [D loss: 0.672820, acc: 0.597656]  [A loss: 0.922278, acc: 0.097656]\n",
            "536: [D loss: 0.668167, acc: 0.587891]  [A loss: 0.866602, acc: 0.152344]\n",
            "537: [D loss: 0.675307, acc: 0.548828]  [A loss: 0.958695, acc: 0.097656]\n",
            "538: [D loss: 0.640225, acc: 0.636719]  [A loss: 0.812337, acc: 0.265625]\n",
            "539: [D loss: 0.681857, acc: 0.548828]  [A loss: 1.055173, acc: 0.035156]\n",
            "540: [D loss: 0.668007, acc: 0.615234]  [A loss: 0.753815, acc: 0.359375]\n",
            "541: [D loss: 0.691276, acc: 0.535156]  [A loss: 1.069068, acc: 0.011719]\n",
            "542: [D loss: 0.660018, acc: 0.623047]  [A loss: 0.713446, acc: 0.472656]\n",
            "543: [D loss: 0.685977, acc: 0.529297]  [A loss: 1.096040, acc: 0.027344]\n",
            "544: [D loss: 0.660239, acc: 0.583984]  [A loss: 0.711739, acc: 0.488281]\n",
            "545: [D loss: 0.686281, acc: 0.554688]  [A loss: 0.970150, acc: 0.058594]\n",
            "546: [D loss: 0.676907, acc: 0.570312]  [A loss: 0.850602, acc: 0.167969]\n",
            "547: [D loss: 0.663207, acc: 0.582031]  [A loss: 0.904244, acc: 0.121094]\n",
            "548: [D loss: 0.669747, acc: 0.599609]  [A loss: 0.851846, acc: 0.195312]\n",
            "549: [D loss: 0.660181, acc: 0.589844]  [A loss: 0.927092, acc: 0.085938]\n",
            "550: [D loss: 0.660159, acc: 0.626953]  [A loss: 0.933299, acc: 0.089844]\n",
            "551: [D loss: 0.653677, acc: 0.626953]  [A loss: 0.945863, acc: 0.117188]\n",
            "552: [D loss: 0.681698, acc: 0.548828]  [A loss: 0.937947, acc: 0.093750]\n",
            "553: [D loss: 0.687596, acc: 0.550781]  [A loss: 0.885474, acc: 0.144531]\n",
            "554: [D loss: 0.667952, acc: 0.582031]  [A loss: 0.958389, acc: 0.066406]\n",
            "555: [D loss: 0.662377, acc: 0.619141]  [A loss: 0.819770, acc: 0.253906]\n",
            "556: [D loss: 0.676994, acc: 0.576172]  [A loss: 1.074155, acc: 0.031250]\n",
            "557: [D loss: 0.660960, acc: 0.607422]  [A loss: 0.745920, acc: 0.398438]\n",
            "558: [D loss: 0.688388, acc: 0.544922]  [A loss: 1.148956, acc: 0.015625]\n",
            "559: [D loss: 0.669018, acc: 0.595703]  [A loss: 0.696046, acc: 0.511719]\n",
            "560: [D loss: 0.709829, acc: 0.533203]  [A loss: 1.106339, acc: 0.011719]\n",
            "561: [D loss: 0.663732, acc: 0.591797]  [A loss: 0.745537, acc: 0.371094]\n",
            "562: [D loss: 0.689926, acc: 0.542969]  [A loss: 0.984620, acc: 0.082031]\n",
            "563: [D loss: 0.655981, acc: 0.621094]  [A loss: 0.799754, acc: 0.289062]\n",
            "564: [D loss: 0.660526, acc: 0.591797]  [A loss: 0.996282, acc: 0.046875]\n",
            "565: [D loss: 0.656366, acc: 0.613281]  [A loss: 0.832496, acc: 0.207031]\n",
            "566: [D loss: 0.678080, acc: 0.541016]  [A loss: 0.965007, acc: 0.093750]\n",
            "567: [D loss: 0.663626, acc: 0.595703]  [A loss: 0.857504, acc: 0.167969]\n",
            "568: [D loss: 0.675200, acc: 0.585938]  [A loss: 0.941462, acc: 0.070312]\n",
            "569: [D loss: 0.655243, acc: 0.619141]  [A loss: 0.873205, acc: 0.152344]\n",
            "570: [D loss: 0.681072, acc: 0.560547]  [A loss: 0.998898, acc: 0.058594]\n",
            "571: [D loss: 0.671104, acc: 0.566406]  [A loss: 0.836328, acc: 0.238281]\n",
            "572: [D loss: 0.677174, acc: 0.568359]  [A loss: 1.007483, acc: 0.046875]\n",
            "573: [D loss: 0.645504, acc: 0.634766]  [A loss: 0.784528, acc: 0.371094]\n",
            "574: [D loss: 0.695759, acc: 0.539062]  [A loss: 1.114681, acc: 0.023438]\n",
            "575: [D loss: 0.668476, acc: 0.601562]  [A loss: 0.691181, acc: 0.511719]\n",
            "576: [D loss: 0.720896, acc: 0.527344]  [A loss: 1.219005, acc: 0.007812]\n",
            "577: [D loss: 0.667400, acc: 0.574219]  [A loss: 0.675455, acc: 0.527344]\n",
            "578: [D loss: 0.707303, acc: 0.527344]  [A loss: 0.987067, acc: 0.054688]\n",
            "579: [D loss: 0.660024, acc: 0.617188]  [A loss: 0.768916, acc: 0.312500]\n",
            "580: [D loss: 0.684709, acc: 0.554688]  [A loss: 0.952276, acc: 0.082031]\n",
            "581: [D loss: 0.678465, acc: 0.562500]  [A loss: 0.816561, acc: 0.269531]\n",
            "582: [D loss: 0.687255, acc: 0.556641]  [A loss: 0.905920, acc: 0.121094]\n",
            "583: [D loss: 0.652639, acc: 0.644531]  [A loss: 0.792318, acc: 0.328125]\n",
            "584: [D loss: 0.691407, acc: 0.531250]  [A loss: 1.023260, acc: 0.042969]\n",
            "585: [D loss: 0.655481, acc: 0.619141]  [A loss: 0.768364, acc: 0.277344]\n",
            "586: [D loss: 0.690172, acc: 0.535156]  [A loss: 0.965848, acc: 0.085938]\n",
            "587: [D loss: 0.661030, acc: 0.615234]  [A loss: 0.811314, acc: 0.226562]\n",
            "588: [D loss: 0.683345, acc: 0.552734]  [A loss: 0.965055, acc: 0.082031]\n",
            "589: [D loss: 0.669962, acc: 0.578125]  [A loss: 0.795281, acc: 0.277344]\n",
            "590: [D loss: 0.669423, acc: 0.566406]  [A loss: 1.031951, acc: 0.062500]\n",
            "591: [D loss: 0.659786, acc: 0.607422]  [A loss: 0.752952, acc: 0.390625]\n",
            "592: [D loss: 0.696101, acc: 0.542969]  [A loss: 1.132235, acc: 0.003906]\n",
            "593: [D loss: 0.664496, acc: 0.589844]  [A loss: 0.724909, acc: 0.457031]\n",
            "594: [D loss: 0.703333, acc: 0.517578]  [A loss: 1.062464, acc: 0.019531]\n",
            "595: [D loss: 0.661295, acc: 0.597656]  [A loss: 0.745548, acc: 0.414062]\n",
            "596: [D loss: 0.693255, acc: 0.533203]  [A loss: 0.999106, acc: 0.054688]\n",
            "597: [D loss: 0.664418, acc: 0.615234]  [A loss: 0.781042, acc: 0.308594]\n",
            "598: [D loss: 0.673249, acc: 0.580078]  [A loss: 0.934589, acc: 0.101562]\n",
            "599: [D loss: 0.659330, acc: 0.591797]  [A loss: 0.811147, acc: 0.250000]\n",
            "600: [D loss: 0.664377, acc: 0.566406]  [A loss: 0.974069, acc: 0.085938]\n",
            "601: [D loss: 0.682146, acc: 0.570312]  [A loss: 0.816263, acc: 0.257812]\n",
            "602: [D loss: 0.695626, acc: 0.552734]  [A loss: 0.997822, acc: 0.039062]\n",
            "603: [D loss: 0.665698, acc: 0.607422]  [A loss: 0.823618, acc: 0.222656]\n",
            "604: [D loss: 0.687328, acc: 0.548828]  [A loss: 0.975390, acc: 0.070312]\n",
            "605: [D loss: 0.661967, acc: 0.611328]  [A loss: 0.848242, acc: 0.207031]\n",
            "606: [D loss: 0.669434, acc: 0.566406]  [A loss: 0.960632, acc: 0.066406]\n",
            "607: [D loss: 0.648820, acc: 0.634766]  [A loss: 0.819493, acc: 0.207031]\n",
            "608: [D loss: 0.670383, acc: 0.572266]  [A loss: 0.978042, acc: 0.062500]\n",
            "609: [D loss: 0.651069, acc: 0.656250]  [A loss: 0.821458, acc: 0.269531]\n",
            "610: [D loss: 0.677597, acc: 0.548828]  [A loss: 1.023105, acc: 0.042969]\n",
            "611: [D loss: 0.666413, acc: 0.589844]  [A loss: 0.802922, acc: 0.265625]\n",
            "612: [D loss: 0.682860, acc: 0.585938]  [A loss: 1.015304, acc: 0.054688]\n",
            "613: [D loss: 0.657898, acc: 0.611328]  [A loss: 0.791131, acc: 0.324219]\n",
            "614: [D loss: 0.675005, acc: 0.562500]  [A loss: 1.073188, acc: 0.031250]\n",
            "615: [D loss: 0.663606, acc: 0.593750]  [A loss: 0.730272, acc: 0.488281]\n",
            "616: [D loss: 0.688618, acc: 0.535156]  [A loss: 1.144949, acc: 0.019531]\n",
            "617: [D loss: 0.657397, acc: 0.609375]  [A loss: 0.687663, acc: 0.527344]\n",
            "618: [D loss: 0.715038, acc: 0.529297]  [A loss: 1.109864, acc: 0.035156]\n",
            "619: [D loss: 0.659749, acc: 0.630859]  [A loss: 0.727869, acc: 0.425781]\n",
            "620: [D loss: 0.701601, acc: 0.521484]  [A loss: 1.002637, acc: 0.050781]\n",
            "621: [D loss: 0.663952, acc: 0.607422]  [A loss: 0.800668, acc: 0.289062]\n",
            "622: [D loss: 0.675494, acc: 0.556641]  [A loss: 0.984559, acc: 0.066406]\n",
            "623: [D loss: 0.665760, acc: 0.599609]  [A loss: 0.852296, acc: 0.214844]\n",
            "624: [D loss: 0.681269, acc: 0.587891]  [A loss: 0.924295, acc: 0.109375]\n",
            "625: [D loss: 0.664375, acc: 0.599609]  [A loss: 0.876460, acc: 0.167969]\n",
            "626: [D loss: 0.673274, acc: 0.578125]  [A loss: 0.894878, acc: 0.187500]\n",
            "627: [D loss: 0.661131, acc: 0.583984]  [A loss: 0.878599, acc: 0.148438]\n",
            "628: [D loss: 0.661910, acc: 0.601562]  [A loss: 1.028116, acc: 0.054688]\n",
            "629: [D loss: 0.662277, acc: 0.626953]  [A loss: 0.805634, acc: 0.253906]\n",
            "630: [D loss: 0.682380, acc: 0.562500]  [A loss: 1.071116, acc: 0.027344]\n",
            "631: [D loss: 0.656956, acc: 0.617188]  [A loss: 0.813481, acc: 0.242188]\n",
            "632: [D loss: 0.689097, acc: 0.535156]  [A loss: 1.047211, acc: 0.039062]\n",
            "633: [D loss: 0.653871, acc: 0.630859]  [A loss: 0.803273, acc: 0.281250]\n",
            "634: [D loss: 0.697407, acc: 0.519531]  [A loss: 1.017221, acc: 0.035156]\n",
            "635: [D loss: 0.667743, acc: 0.599609]  [A loss: 0.816804, acc: 0.265625]\n",
            "636: [D loss: 0.676820, acc: 0.585938]  [A loss: 1.001236, acc: 0.078125]\n",
            "637: [D loss: 0.670374, acc: 0.591797]  [A loss: 0.847386, acc: 0.195312]\n",
            "638: [D loss: 0.663287, acc: 0.587891]  [A loss: 0.976353, acc: 0.093750]\n",
            "639: [D loss: 0.668708, acc: 0.611328]  [A loss: 0.862697, acc: 0.183594]\n",
            "640: [D loss: 0.671895, acc: 0.589844]  [A loss: 1.018527, acc: 0.042969]\n",
            "641: [D loss: 0.649455, acc: 0.617188]  [A loss: 0.806573, acc: 0.273438]\n",
            "642: [D loss: 0.682366, acc: 0.544922]  [A loss: 1.084387, acc: 0.023438]\n",
            "643: [D loss: 0.655487, acc: 0.628906]  [A loss: 0.759246, acc: 0.398438]\n",
            "644: [D loss: 0.695172, acc: 0.535156]  [A loss: 1.124836, acc: 0.011719]\n",
            "645: [D loss: 0.659615, acc: 0.589844]  [A loss: 0.730614, acc: 0.445312]\n",
            "646: [D loss: 0.692328, acc: 0.533203]  [A loss: 1.003695, acc: 0.058594]\n",
            "647: [D loss: 0.651354, acc: 0.611328]  [A loss: 0.738112, acc: 0.433594]\n",
            "648: [D loss: 0.681092, acc: 0.544922]  [A loss: 1.050321, acc: 0.054688]\n",
            "649: [D loss: 0.671359, acc: 0.564453]  [A loss: 0.831716, acc: 0.222656]\n",
            "650: [D loss: 0.671420, acc: 0.556641]  [A loss: 0.937621, acc: 0.113281]\n",
            "651: [D loss: 0.655158, acc: 0.609375]  [A loss: 0.846445, acc: 0.214844]\n",
            "652: [D loss: 0.671272, acc: 0.570312]  [A loss: 0.986015, acc: 0.074219]\n",
            "653: [D loss: 0.653296, acc: 0.628906]  [A loss: 0.771848, acc: 0.339844]\n",
            "654: [D loss: 0.684405, acc: 0.562500]  [A loss: 1.087804, acc: 0.031250]\n",
            "655: [D loss: 0.661758, acc: 0.628906]  [A loss: 0.778814, acc: 0.363281]\n",
            "656: [D loss: 0.685711, acc: 0.513672]  [A loss: 1.059692, acc: 0.035156]\n",
            "657: [D loss: 0.662147, acc: 0.617188]  [A loss: 0.783561, acc: 0.281250]\n",
            "658: [D loss: 0.698423, acc: 0.531250]  [A loss: 1.069843, acc: 0.058594]\n",
            "659: [D loss: 0.669860, acc: 0.585938]  [A loss: 0.797389, acc: 0.296875]\n",
            "660: [D loss: 0.670443, acc: 0.562500]  [A loss: 1.008746, acc: 0.070312]\n",
            "661: [D loss: 0.666360, acc: 0.591797]  [A loss: 0.847416, acc: 0.218750]\n",
            "662: [D loss: 0.673846, acc: 0.546875]  [A loss: 1.036182, acc: 0.046875]\n",
            "663: [D loss: 0.661046, acc: 0.609375]  [A loss: 0.809360, acc: 0.296875]\n",
            "664: [D loss: 0.657066, acc: 0.599609]  [A loss: 1.007341, acc: 0.078125]\n",
            "665: [D loss: 0.642233, acc: 0.640625]  [A loss: 0.889062, acc: 0.187500]\n",
            "666: [D loss: 0.688980, acc: 0.578125]  [A loss: 1.137457, acc: 0.011719]\n",
            "667: [D loss: 0.668735, acc: 0.583984]  [A loss: 0.717733, acc: 0.472656]\n",
            "668: [D loss: 0.714581, acc: 0.525391]  [A loss: 1.100501, acc: 0.003906]\n",
            "669: [D loss: 0.667609, acc: 0.599609]  [A loss: 0.742462, acc: 0.421875]\n",
            "670: [D loss: 0.682882, acc: 0.566406]  [A loss: 1.000567, acc: 0.062500]\n",
            "671: [D loss: 0.656662, acc: 0.609375]  [A loss: 0.822732, acc: 0.234375]\n",
            "672: [D loss: 0.666965, acc: 0.580078]  [A loss: 0.962071, acc: 0.089844]\n",
            "673: [D loss: 0.654226, acc: 0.615234]  [A loss: 0.827100, acc: 0.246094]\n",
            "674: [D loss: 0.695879, acc: 0.554688]  [A loss: 1.005890, acc: 0.058594]\n",
            "675: [D loss: 0.653578, acc: 0.605469]  [A loss: 0.760111, acc: 0.386719]\n",
            "676: [D loss: 0.689810, acc: 0.550781]  [A loss: 1.036721, acc: 0.058594]\n",
            "677: [D loss: 0.654556, acc: 0.623047]  [A loss: 0.781441, acc: 0.339844]\n",
            "678: [D loss: 0.677679, acc: 0.564453]  [A loss: 1.006472, acc: 0.050781]\n",
            "679: [D loss: 0.662702, acc: 0.615234]  [A loss: 0.826892, acc: 0.269531]\n",
            "680: [D loss: 0.665274, acc: 0.570312]  [A loss: 0.976501, acc: 0.066406]\n",
            "681: [D loss: 0.645974, acc: 0.632812]  [A loss: 0.828005, acc: 0.250000]\n",
            "682: [D loss: 0.680265, acc: 0.550781]  [A loss: 1.030534, acc: 0.039062]\n",
            "683: [D loss: 0.654165, acc: 0.611328]  [A loss: 0.749000, acc: 0.433594]\n",
            "684: [D loss: 0.692625, acc: 0.558594]  [A loss: 1.117971, acc: 0.035156]\n",
            "685: [D loss: 0.665609, acc: 0.583984]  [A loss: 0.717348, acc: 0.500000]\n",
            "686: [D loss: 0.692355, acc: 0.552734]  [A loss: 0.985781, acc: 0.078125]\n",
            "687: [D loss: 0.663171, acc: 0.593750]  [A loss: 0.865972, acc: 0.191406]\n",
            "688: [D loss: 0.649698, acc: 0.611328]  [A loss: 0.914074, acc: 0.167969]\n",
            "689: [D loss: 0.660085, acc: 0.595703]  [A loss: 0.945599, acc: 0.097656]\n",
            "690: [D loss: 0.663802, acc: 0.607422]  [A loss: 0.906995, acc: 0.156250]\n",
            "691: [D loss: 0.663918, acc: 0.599609]  [A loss: 0.947774, acc: 0.121094]\n",
            "692: [D loss: 0.686619, acc: 0.558594]  [A loss: 1.006245, acc: 0.082031]\n",
            "693: [D loss: 0.669471, acc: 0.574219]  [A loss: 0.813493, acc: 0.273438]\n",
            "694: [D loss: 0.689656, acc: 0.535156]  [A loss: 1.064191, acc: 0.058594]\n",
            "695: [D loss: 0.672558, acc: 0.585938]  [A loss: 0.790585, acc: 0.308594]\n",
            "696: [D loss: 0.676283, acc: 0.582031]  [A loss: 1.079945, acc: 0.042969]\n",
            "697: [D loss: 0.688802, acc: 0.564453]  [A loss: 0.763445, acc: 0.355469]\n",
            "698: [D loss: 0.686335, acc: 0.564453]  [A loss: 1.038318, acc: 0.031250]\n",
            "699: [D loss: 0.652369, acc: 0.634766]  [A loss: 0.777804, acc: 0.343750]\n",
            "700: [D loss: 0.660355, acc: 0.580078]  [A loss: 0.999739, acc: 0.062500]\n",
            "701: [D loss: 0.645846, acc: 0.634766]  [A loss: 0.815767, acc: 0.250000]\n",
            "702: [D loss: 0.692924, acc: 0.556641]  [A loss: 1.055493, acc: 0.039062]\n",
            "703: [D loss: 0.681985, acc: 0.542969]  [A loss: 0.816566, acc: 0.300781]\n",
            "704: [D loss: 0.670134, acc: 0.576172]  [A loss: 1.032803, acc: 0.042969]\n",
            "705: [D loss: 0.672637, acc: 0.564453]  [A loss: 0.924562, acc: 0.105469]\n",
            "706: [D loss: 0.666289, acc: 0.556641]  [A loss: 0.970804, acc: 0.117188]\n",
            "707: [D loss: 0.671616, acc: 0.589844]  [A loss: 0.871813, acc: 0.167969]\n",
            "708: [D loss: 0.663897, acc: 0.593750]  [A loss: 0.959552, acc: 0.078125]\n",
            "709: [D loss: 0.685545, acc: 0.556641]  [A loss: 0.870607, acc: 0.199219]\n",
            "710: [D loss: 0.708925, acc: 0.552734]  [A loss: 1.033060, acc: 0.035156]\n",
            "711: [D loss: 0.660239, acc: 0.626953]  [A loss: 0.775872, acc: 0.292969]\n",
            "712: [D loss: 0.674297, acc: 0.548828]  [A loss: 1.107432, acc: 0.039062]\n",
            "713: [D loss: 0.669985, acc: 0.572266]  [A loss: 0.748153, acc: 0.402344]\n",
            "714: [D loss: 0.698789, acc: 0.527344]  [A loss: 1.051485, acc: 0.058594]\n",
            "715: [D loss: 0.652824, acc: 0.595703]  [A loss: 0.828712, acc: 0.230469]\n",
            "716: [D loss: 0.669724, acc: 0.570312]  [A loss: 0.939919, acc: 0.097656]\n",
            "717: [D loss: 0.668735, acc: 0.562500]  [A loss: 0.866976, acc: 0.156250]\n",
            "718: [D loss: 0.654278, acc: 0.601562]  [A loss: 0.908860, acc: 0.125000]\n",
            "719: [D loss: 0.651700, acc: 0.621094]  [A loss: 0.941309, acc: 0.109375]\n",
            "720: [D loss: 0.669426, acc: 0.570312]  [A loss: 0.932939, acc: 0.136719]\n",
            "721: [D loss: 0.667404, acc: 0.587891]  [A loss: 0.856391, acc: 0.214844]\n",
            "722: [D loss: 0.677605, acc: 0.558594]  [A loss: 1.039847, acc: 0.070312]\n",
            "723: [D loss: 0.653709, acc: 0.613281]  [A loss: 0.809295, acc: 0.250000]\n",
            "724: [D loss: 0.707537, acc: 0.529297]  [A loss: 1.069533, acc: 0.046875]\n",
            "725: [D loss: 0.679993, acc: 0.548828]  [A loss: 0.751158, acc: 0.402344]\n",
            "726: [D loss: 0.692619, acc: 0.542969]  [A loss: 1.047267, acc: 0.042969]\n",
            "727: [D loss: 0.658332, acc: 0.626953]  [A loss: 0.808829, acc: 0.269531]\n",
            "728: [D loss: 0.706730, acc: 0.523438]  [A loss: 1.012873, acc: 0.058594]\n",
            "729: [D loss: 0.658048, acc: 0.619141]  [A loss: 0.809290, acc: 0.304688]\n",
            "730: [D loss: 0.670361, acc: 0.552734]  [A loss: 1.068117, acc: 0.027344]\n",
            "731: [D loss: 0.661911, acc: 0.576172]  [A loss: 0.773526, acc: 0.363281]\n",
            "732: [D loss: 0.687776, acc: 0.546875]  [A loss: 0.989030, acc: 0.078125]\n",
            "733: [D loss: 0.644130, acc: 0.644531]  [A loss: 0.826033, acc: 0.289062]\n",
            "734: [D loss: 0.694469, acc: 0.523438]  [A loss: 1.037231, acc: 0.050781]\n",
            "735: [D loss: 0.658351, acc: 0.619141]  [A loss: 0.803304, acc: 0.277344]\n",
            "736: [D loss: 0.681730, acc: 0.548828]  [A loss: 1.034241, acc: 0.031250]\n",
            "737: [D loss: 0.671157, acc: 0.576172]  [A loss: 0.782897, acc: 0.347656]\n",
            "738: [D loss: 0.668566, acc: 0.589844]  [A loss: 1.024545, acc: 0.093750]\n",
            "739: [D loss: 0.660157, acc: 0.587891]  [A loss: 0.807448, acc: 0.261719]\n",
            "740: [D loss: 0.681738, acc: 0.591797]  [A loss: 1.020445, acc: 0.042969]\n",
            "741: [D loss: 0.649983, acc: 0.599609]  [A loss: 0.804985, acc: 0.269531]\n",
            "742: [D loss: 0.657576, acc: 0.582031]  [A loss: 1.027632, acc: 0.074219]\n",
            "743: [D loss: 0.641062, acc: 0.652344]  [A loss: 0.835731, acc: 0.257812]\n",
            "744: [D loss: 0.726815, acc: 0.517578]  [A loss: 1.164422, acc: 0.023438]\n",
            "745: [D loss: 0.692663, acc: 0.521484]  [A loss: 0.750549, acc: 0.398438]\n",
            "746: [D loss: 0.684716, acc: 0.550781]  [A loss: 1.039955, acc: 0.050781]\n",
            "747: [D loss: 0.646200, acc: 0.642578]  [A loss: 0.786496, acc: 0.292969]\n",
            "748: [D loss: 0.685227, acc: 0.527344]  [A loss: 1.030588, acc: 0.058594]\n",
            "749: [D loss: 0.664301, acc: 0.587891]  [A loss: 0.834023, acc: 0.195312]\n",
            "750: [D loss: 0.684693, acc: 0.539062]  [A loss: 0.980746, acc: 0.078125]\n",
            "751: [D loss: 0.655501, acc: 0.625000]  [A loss: 0.845415, acc: 0.230469]\n",
            "752: [D loss: 0.683497, acc: 0.582031]  [A loss: 1.009023, acc: 0.054688]\n",
            "753: [D loss: 0.653625, acc: 0.628906]  [A loss: 0.811734, acc: 0.261719]\n",
            "754: [D loss: 0.674813, acc: 0.541016]  [A loss: 0.995645, acc: 0.054688]\n",
            "755: [D loss: 0.671603, acc: 0.595703]  [A loss: 0.827342, acc: 0.218750]\n",
            "756: [D loss: 0.672397, acc: 0.542969]  [A loss: 0.898605, acc: 0.144531]\n",
            "757: [D loss: 0.662072, acc: 0.609375]  [A loss: 0.917851, acc: 0.144531]\n",
            "758: [D loss: 0.660175, acc: 0.603516]  [A loss: 0.913823, acc: 0.132812]\n",
            "759: [D loss: 0.661071, acc: 0.609375]  [A loss: 0.980820, acc: 0.070312]\n",
            "760: [D loss: 0.662936, acc: 0.605469]  [A loss: 0.902121, acc: 0.148438]\n",
            "761: [D loss: 0.657486, acc: 0.605469]  [A loss: 0.958312, acc: 0.082031]\n",
            "762: [D loss: 0.651499, acc: 0.605469]  [A loss: 0.919784, acc: 0.144531]\n",
            "763: [D loss: 0.664639, acc: 0.603516]  [A loss: 0.996151, acc: 0.054688]\n",
            "764: [D loss: 0.649750, acc: 0.609375]  [A loss: 0.933091, acc: 0.121094]\n",
            "765: [D loss: 0.648524, acc: 0.621094]  [A loss: 1.034224, acc: 0.089844]\n",
            "766: [D loss: 0.651017, acc: 0.634766]  [A loss: 0.963056, acc: 0.125000]\n",
            "767: [D loss: 0.674182, acc: 0.568359]  [A loss: 1.056174, acc: 0.066406]\n",
            "768: [D loss: 0.667109, acc: 0.599609]  [A loss: 0.821892, acc: 0.277344]\n",
            "769: [D loss: 0.681431, acc: 0.548828]  [A loss: 1.082942, acc: 0.035156]\n",
            "770: [D loss: 0.670641, acc: 0.572266]  [A loss: 0.791098, acc: 0.296875]\n",
            "771: [D loss: 0.682371, acc: 0.546875]  [A loss: 1.175045, acc: 0.007812]\n",
            "772: [D loss: 0.671574, acc: 0.593750]  [A loss: 0.657153, acc: 0.601562]\n",
            "773: [D loss: 0.708173, acc: 0.527344]  [A loss: 1.107166, acc: 0.011719]\n",
            "774: [D loss: 0.659714, acc: 0.597656]  [A loss: 0.766674, acc: 0.339844]\n",
            "775: [D loss: 0.705190, acc: 0.535156]  [A loss: 1.038655, acc: 0.058594]\n",
            "776: [D loss: 0.681811, acc: 0.552734]  [A loss: 0.802398, acc: 0.304688]\n",
            "777: [D loss: 0.675426, acc: 0.574219]  [A loss: 0.910903, acc: 0.117188]\n",
            "778: [D loss: 0.654616, acc: 0.613281]  [A loss: 0.876359, acc: 0.156250]\n",
            "779: [D loss: 0.677241, acc: 0.566406]  [A loss: 0.928674, acc: 0.070312]\n",
            "780: [D loss: 0.654365, acc: 0.611328]  [A loss: 0.877513, acc: 0.179688]\n",
            "781: [D loss: 0.667069, acc: 0.570312]  [A loss: 0.937805, acc: 0.113281]\n",
            "782: [D loss: 0.688885, acc: 0.529297]  [A loss: 0.975370, acc: 0.085938]\n",
            "783: [D loss: 0.664212, acc: 0.582031]  [A loss: 0.833379, acc: 0.222656]\n",
            "784: [D loss: 0.657061, acc: 0.582031]  [A loss: 0.969328, acc: 0.132812]\n",
            "785: [D loss: 0.675702, acc: 0.560547]  [A loss: 0.817755, acc: 0.250000]\n",
            "786: [D loss: 0.692648, acc: 0.548828]  [A loss: 1.003879, acc: 0.046875]\n",
            "787: [D loss: 0.667624, acc: 0.585938]  [A loss: 0.839995, acc: 0.222656]\n",
            "788: [D loss: 0.659122, acc: 0.572266]  [A loss: 1.064760, acc: 0.039062]\n",
            "789: [D loss: 0.686896, acc: 0.564453]  [A loss: 0.802658, acc: 0.289062]\n",
            "790: [D loss: 0.669877, acc: 0.546875]  [A loss: 0.982201, acc: 0.082031]\n",
            "791: [D loss: 0.653785, acc: 0.619141]  [A loss: 0.930602, acc: 0.093750]\n",
            "792: [D loss: 0.688296, acc: 0.525391]  [A loss: 0.975500, acc: 0.101562]\n",
            "793: [D loss: 0.661022, acc: 0.619141]  [A loss: 0.815487, acc: 0.273438]\n",
            "794: [D loss: 0.666474, acc: 0.558594]  [A loss: 1.047781, acc: 0.042969]\n",
            "795: [D loss: 0.668692, acc: 0.597656]  [A loss: 0.698607, acc: 0.484375]\n",
            "796: [D loss: 0.710009, acc: 0.531250]  [A loss: 1.060011, acc: 0.035156]\n",
            "797: [D loss: 0.671500, acc: 0.570312]  [A loss: 0.728104, acc: 0.457031]\n",
            "798: [D loss: 0.702964, acc: 0.525391]  [A loss: 0.986593, acc: 0.074219]\n",
            "799: [D loss: 0.659083, acc: 0.593750]  [A loss: 0.784946, acc: 0.355469]\n",
            "800: [D loss: 0.677420, acc: 0.570312]  [A loss: 0.940429, acc: 0.097656]\n",
            "801: [D loss: 0.674076, acc: 0.576172]  [A loss: 0.854522, acc: 0.199219]\n",
            "802: [D loss: 0.678026, acc: 0.548828]  [A loss: 0.935285, acc: 0.097656]\n",
            "803: [D loss: 0.671929, acc: 0.550781]  [A loss: 0.944760, acc: 0.121094]\n",
            "804: [D loss: 0.670430, acc: 0.599609]  [A loss: 0.904033, acc: 0.121094]\n",
            "805: [D loss: 0.669801, acc: 0.601562]  [A loss: 0.890685, acc: 0.148438]\n",
            "806: [D loss: 0.687522, acc: 0.535156]  [A loss: 1.008931, acc: 0.066406]\n",
            "807: [D loss: 0.669211, acc: 0.603516]  [A loss: 0.825363, acc: 0.242188]\n",
            "808: [D loss: 0.671207, acc: 0.564453]  [A loss: 0.985215, acc: 0.070312]\n",
            "809: [D loss: 0.664336, acc: 0.578125]  [A loss: 0.787006, acc: 0.292969]\n",
            "810: [D loss: 0.687322, acc: 0.546875]  [A loss: 1.096236, acc: 0.019531]\n",
            "811: [D loss: 0.666102, acc: 0.599609]  [A loss: 0.708152, acc: 0.503906]\n",
            "812: [D loss: 0.699213, acc: 0.519531]  [A loss: 1.064137, acc: 0.031250]\n",
            "813: [D loss: 0.665137, acc: 0.580078]  [A loss: 0.752095, acc: 0.371094]\n",
            "814: [D loss: 0.679640, acc: 0.572266]  [A loss: 0.990116, acc: 0.070312]\n",
            "815: [D loss: 0.668677, acc: 0.566406]  [A loss: 0.844788, acc: 0.257812]\n",
            "816: [D loss: 0.665550, acc: 0.583984]  [A loss: 0.954104, acc: 0.078125]\n",
            "817: [D loss: 0.658138, acc: 0.607422]  [A loss: 0.816844, acc: 0.265625]\n",
            "818: [D loss: 0.680693, acc: 0.542969]  [A loss: 1.006634, acc: 0.066406]\n",
            "819: [D loss: 0.671608, acc: 0.566406]  [A loss: 0.770248, acc: 0.324219]\n",
            "820: [D loss: 0.709368, acc: 0.521484]  [A loss: 1.051321, acc: 0.015625]\n",
            "821: [D loss: 0.668699, acc: 0.593750]  [A loss: 0.807991, acc: 0.250000]\n",
            "822: [D loss: 0.681866, acc: 0.564453]  [A loss: 0.980998, acc: 0.082031]\n",
            "823: [D loss: 0.655379, acc: 0.634766]  [A loss: 0.813466, acc: 0.300781]\n",
            "824: [D loss: 0.672601, acc: 0.554688]  [A loss: 0.989347, acc: 0.066406]\n",
            "825: [D loss: 0.659228, acc: 0.607422]  [A loss: 0.834955, acc: 0.246094]\n",
            "826: [D loss: 0.656220, acc: 0.613281]  [A loss: 0.954611, acc: 0.125000]\n",
            "827: [D loss: 0.671866, acc: 0.585938]  [A loss: 0.846280, acc: 0.195312]\n",
            "828: [D loss: 0.661489, acc: 0.607422]  [A loss: 0.927159, acc: 0.097656]\n",
            "829: [D loss: 0.668002, acc: 0.578125]  [A loss: 0.823796, acc: 0.265625]\n",
            "830: [D loss: 0.665642, acc: 0.593750]  [A loss: 0.989059, acc: 0.089844]\n",
            "831: [D loss: 0.654162, acc: 0.611328]  [A loss: 0.817535, acc: 0.257812]\n",
            "832: [D loss: 0.693072, acc: 0.544922]  [A loss: 1.047133, acc: 0.031250]\n",
            "833: [D loss: 0.676099, acc: 0.578125]  [A loss: 0.784050, acc: 0.304688]\n",
            "834: [D loss: 0.673097, acc: 0.585938]  [A loss: 1.056622, acc: 0.039062]\n",
            "835: [D loss: 0.665936, acc: 0.582031]  [A loss: 0.757224, acc: 0.386719]\n",
            "836: [D loss: 0.698113, acc: 0.546875]  [A loss: 1.107382, acc: 0.011719]\n",
            "837: [D loss: 0.675442, acc: 0.568359]  [A loss: 0.679378, acc: 0.554688]\n",
            "838: [D loss: 0.697021, acc: 0.539062]  [A loss: 0.965014, acc: 0.066406]\n",
            "839: [D loss: 0.655971, acc: 0.632812]  [A loss: 0.838147, acc: 0.222656]\n",
            "840: [D loss: 0.678816, acc: 0.580078]  [A loss: 0.997407, acc: 0.085938]\n",
            "841: [D loss: 0.674273, acc: 0.583984]  [A loss: 0.795299, acc: 0.324219]\n",
            "842: [D loss: 0.674623, acc: 0.570312]  [A loss: 0.942778, acc: 0.101562]\n",
            "843: [D loss: 0.676011, acc: 0.589844]  [A loss: 0.847359, acc: 0.207031]\n",
            "844: [D loss: 0.653225, acc: 0.583984]  [A loss: 0.889202, acc: 0.183594]\n",
            "845: [D loss: 0.665504, acc: 0.585938]  [A loss: 0.879710, acc: 0.214844]\n",
            "846: [D loss: 0.674884, acc: 0.589844]  [A loss: 0.910166, acc: 0.148438]\n",
            "847: [D loss: 0.654415, acc: 0.626953]  [A loss: 0.873298, acc: 0.167969]\n",
            "848: [D loss: 0.661540, acc: 0.601562]  [A loss: 0.935497, acc: 0.144531]\n",
            "849: [D loss: 0.664728, acc: 0.587891]  [A loss: 0.867581, acc: 0.203125]\n",
            "850: [D loss: 0.702529, acc: 0.537109]  [A loss: 1.055433, acc: 0.050781]\n",
            "851: [D loss: 0.669152, acc: 0.574219]  [A loss: 0.786802, acc: 0.312500]\n",
            "852: [D loss: 0.684138, acc: 0.558594]  [A loss: 0.998450, acc: 0.058594]\n",
            "853: [D loss: 0.659534, acc: 0.615234]  [A loss: 0.772063, acc: 0.335938]\n",
            "854: [D loss: 0.694801, acc: 0.533203]  [A loss: 1.053443, acc: 0.035156]\n",
            "855: [D loss: 0.665791, acc: 0.617188]  [A loss: 0.754541, acc: 0.382812]\n",
            "856: [D loss: 0.685802, acc: 0.568359]  [A loss: 1.045065, acc: 0.058594]\n",
            "857: [D loss: 0.667435, acc: 0.597656]  [A loss: 0.711801, acc: 0.457031]\n",
            "858: [D loss: 0.724082, acc: 0.527344]  [A loss: 1.008341, acc: 0.042969]\n",
            "859: [D loss: 0.678437, acc: 0.585938]  [A loss: 0.749087, acc: 0.394531]\n",
            "860: [D loss: 0.670040, acc: 0.576172]  [A loss: 0.907668, acc: 0.121094]\n",
            "861: [D loss: 0.661154, acc: 0.589844]  [A loss: 0.815541, acc: 0.257812]\n",
            "862: [D loss: 0.672198, acc: 0.582031]  [A loss: 0.950102, acc: 0.105469]\n",
            "863: [D loss: 0.664001, acc: 0.621094]  [A loss: 0.816950, acc: 0.222656]\n",
            "864: [D loss: 0.660558, acc: 0.587891]  [A loss: 0.949595, acc: 0.093750]\n",
            "865: [D loss: 0.664351, acc: 0.582031]  [A loss: 0.851706, acc: 0.242188]\n",
            "866: [D loss: 0.671643, acc: 0.595703]  [A loss: 0.951761, acc: 0.093750]\n",
            "867: [D loss: 0.671747, acc: 0.535156]  [A loss: 0.869986, acc: 0.199219]\n",
            "868: [D loss: 0.673317, acc: 0.570312]  [A loss: 0.885049, acc: 0.179688]\n",
            "869: [D loss: 0.657665, acc: 0.615234]  [A loss: 0.868695, acc: 0.207031]\n",
            "870: [D loss: 0.683547, acc: 0.527344]  [A loss: 0.878754, acc: 0.191406]\n",
            "871: [D loss: 0.681841, acc: 0.574219]  [A loss: 0.922085, acc: 0.121094]\n",
            "872: [D loss: 0.674364, acc: 0.568359]  [A loss: 0.912425, acc: 0.144531]\n",
            "873: [D loss: 0.664600, acc: 0.570312]  [A loss: 0.949426, acc: 0.085938]\n",
            "874: [D loss: 0.670814, acc: 0.582031]  [A loss: 0.869081, acc: 0.210938]\n",
            "875: [D loss: 0.675944, acc: 0.560547]  [A loss: 0.935188, acc: 0.078125]\n",
            "876: [D loss: 0.652999, acc: 0.619141]  [A loss: 0.848655, acc: 0.207031]\n",
            "877: [D loss: 0.686403, acc: 0.554688]  [A loss: 0.958146, acc: 0.101562]\n",
            "878: [D loss: 0.652962, acc: 0.632812]  [A loss: 0.855937, acc: 0.230469]\n",
            "879: [D loss: 0.679944, acc: 0.562500]  [A loss: 1.081324, acc: 0.027344]\n",
            "880: [D loss: 0.655871, acc: 0.638672]  [A loss: 0.717871, acc: 0.468750]\n",
            "881: [D loss: 0.723984, acc: 0.525391]  [A loss: 1.133584, acc: 0.023438]\n",
            "882: [D loss: 0.688045, acc: 0.533203]  [A loss: 0.725541, acc: 0.437500]\n",
            "883: [D loss: 0.706876, acc: 0.523438]  [A loss: 0.976740, acc: 0.070312]\n",
            "884: [D loss: 0.664495, acc: 0.595703]  [A loss: 0.853164, acc: 0.203125]\n",
            "885: [D loss: 0.690521, acc: 0.548828]  [A loss: 0.972983, acc: 0.085938]\n",
            "886: [D loss: 0.661795, acc: 0.599609]  [A loss: 0.792284, acc: 0.304688]\n",
            "887: [D loss: 0.681593, acc: 0.562500]  [A loss: 0.977798, acc: 0.070312]\n",
            "888: [D loss: 0.666294, acc: 0.619141]  [A loss: 0.823760, acc: 0.230469]\n",
            "889: [D loss: 0.678655, acc: 0.541016]  [A loss: 0.941540, acc: 0.097656]\n",
            "890: [D loss: 0.662366, acc: 0.589844]  [A loss: 0.813332, acc: 0.261719]\n",
            "891: [D loss: 0.673269, acc: 0.583984]  [A loss: 1.005524, acc: 0.058594]\n",
            "892: [D loss: 0.671148, acc: 0.599609]  [A loss: 0.817744, acc: 0.261719]\n",
            "893: [D loss: 0.691271, acc: 0.517578]  [A loss: 0.981907, acc: 0.062500]\n",
            "894: [D loss: 0.673201, acc: 0.603516]  [A loss: 0.793475, acc: 0.332031]\n",
            "895: [D loss: 0.673115, acc: 0.570312]  [A loss: 1.001382, acc: 0.070312]\n",
            "896: [D loss: 0.666506, acc: 0.572266]  [A loss: 0.838507, acc: 0.234375]\n",
            "897: [D loss: 0.669227, acc: 0.603516]  [A loss: 0.953833, acc: 0.097656]\n",
            "898: [D loss: 0.654865, acc: 0.619141]  [A loss: 0.789705, acc: 0.304688]\n",
            "899: [D loss: 0.676146, acc: 0.566406]  [A loss: 1.019115, acc: 0.058594]\n",
            "900: [D loss: 0.666074, acc: 0.583984]  [A loss: 0.767955, acc: 0.335938]\n",
            "901: [D loss: 0.688736, acc: 0.546875]  [A loss: 0.981055, acc: 0.082031]\n",
            "902: [D loss: 0.665538, acc: 0.568359]  [A loss: 0.791773, acc: 0.304688]\n",
            "903: [D loss: 0.678402, acc: 0.548828]  [A loss: 0.947197, acc: 0.093750]\n",
            "904: [D loss: 0.658815, acc: 0.609375]  [A loss: 0.855948, acc: 0.210938]\n",
            "905: [D loss: 0.665439, acc: 0.552734]  [A loss: 0.906297, acc: 0.121094]\n",
            "906: [D loss: 0.671005, acc: 0.585938]  [A loss: 0.866294, acc: 0.187500]\n",
            "907: [D loss: 0.671855, acc: 0.572266]  [A loss: 0.894358, acc: 0.144531]\n",
            "908: [D loss: 0.697123, acc: 0.523438]  [A loss: 1.010577, acc: 0.062500]\n",
            "909: [D loss: 0.674669, acc: 0.574219]  [A loss: 0.777277, acc: 0.312500]\n",
            "910: [D loss: 0.694874, acc: 0.550781]  [A loss: 0.984139, acc: 0.050781]\n",
            "911: [D loss: 0.678875, acc: 0.531250]  [A loss: 0.799906, acc: 0.261719]\n",
            "912: [D loss: 0.674139, acc: 0.576172]  [A loss: 0.943143, acc: 0.097656]\n",
            "913: [D loss: 0.662809, acc: 0.601562]  [A loss: 0.873524, acc: 0.148438]\n",
            "914: [D loss: 0.673939, acc: 0.593750]  [A loss: 0.925029, acc: 0.097656]\n",
            "915: [D loss: 0.669674, acc: 0.593750]  [A loss: 0.827991, acc: 0.234375]\n",
            "916: [D loss: 0.667761, acc: 0.583984]  [A loss: 0.939863, acc: 0.140625]\n",
            "917: [D loss: 0.661031, acc: 0.599609]  [A loss: 0.796648, acc: 0.304688]\n",
            "918: [D loss: 0.688542, acc: 0.546875]  [A loss: 1.011595, acc: 0.070312]\n",
            "919: [D loss: 0.650654, acc: 0.640625]  [A loss: 0.764489, acc: 0.359375]\n",
            "920: [D loss: 0.704610, acc: 0.541016]  [A loss: 1.043297, acc: 0.039062]\n",
            "921: [D loss: 0.648171, acc: 0.613281]  [A loss: 0.738566, acc: 0.421875]\n",
            "922: [D loss: 0.692295, acc: 0.533203]  [A loss: 1.032708, acc: 0.050781]\n",
            "923: [D loss: 0.647596, acc: 0.623047]  [A loss: 0.770015, acc: 0.382812]\n",
            "924: [D loss: 0.691094, acc: 0.542969]  [A loss: 1.044347, acc: 0.023438]\n",
            "925: [D loss: 0.670435, acc: 0.578125]  [A loss: 0.716564, acc: 0.460938]\n",
            "926: [D loss: 0.693283, acc: 0.529297]  [A loss: 0.974612, acc: 0.101562]\n",
            "927: [D loss: 0.653021, acc: 0.607422]  [A loss: 0.785878, acc: 0.316406]\n",
            "928: [D loss: 0.684756, acc: 0.535156]  [A loss: 0.940250, acc: 0.097656]\n",
            "929: [D loss: 0.667405, acc: 0.615234]  [A loss: 0.805735, acc: 0.277344]\n",
            "930: [D loss: 0.679935, acc: 0.574219]  [A loss: 0.950605, acc: 0.117188]\n",
            "931: [D loss: 0.678565, acc: 0.595703]  [A loss: 0.793706, acc: 0.332031]\n",
            "932: [D loss: 0.688094, acc: 0.550781]  [A loss: 0.906337, acc: 0.144531]\n",
            "933: [D loss: 0.660967, acc: 0.595703]  [A loss: 0.833353, acc: 0.269531]\n",
            "934: [D loss: 0.681278, acc: 0.558594]  [A loss: 0.896135, acc: 0.175781]\n",
            "935: [D loss: 0.681947, acc: 0.568359]  [A loss: 0.895595, acc: 0.164062]\n",
            "936: [D loss: 0.671563, acc: 0.566406]  [A loss: 0.897432, acc: 0.164062]\n",
            "937: [D loss: 0.676677, acc: 0.544922]  [A loss: 0.881883, acc: 0.195312]\n",
            "938: [D loss: 0.693086, acc: 0.556641]  [A loss: 0.908170, acc: 0.125000]\n",
            "939: [D loss: 0.665073, acc: 0.593750]  [A loss: 0.854687, acc: 0.179688]\n",
            "940: [D loss: 0.657656, acc: 0.578125]  [A loss: 0.955882, acc: 0.109375]\n",
            "941: [D loss: 0.675425, acc: 0.552734]  [A loss: 0.933570, acc: 0.121094]\n",
            "942: [D loss: 0.669185, acc: 0.583984]  [A loss: 0.856871, acc: 0.199219]\n",
            "943: [D loss: 0.693703, acc: 0.568359]  [A loss: 0.982296, acc: 0.078125]\n",
            "944: [D loss: 0.667736, acc: 0.583984]  [A loss: 0.807060, acc: 0.296875]\n",
            "945: [D loss: 0.712088, acc: 0.527344]  [A loss: 1.048237, acc: 0.050781]\n",
            "946: [D loss: 0.702704, acc: 0.539062]  [A loss: 0.844960, acc: 0.222656]\n",
            "947: [D loss: 0.697499, acc: 0.564453]  [A loss: 0.916633, acc: 0.156250]\n",
            "948: [D loss: 0.663453, acc: 0.626953]  [A loss: 0.796585, acc: 0.273438]\n",
            "949: [D loss: 0.702740, acc: 0.519531]  [A loss: 1.017689, acc: 0.042969]\n",
            "950: [D loss: 0.674182, acc: 0.593750]  [A loss: 0.787820, acc: 0.312500]\n",
            "951: [D loss: 0.686796, acc: 0.550781]  [A loss: 0.977424, acc: 0.082031]\n",
            "952: [D loss: 0.677816, acc: 0.558594]  [A loss: 0.759281, acc: 0.355469]\n",
            "953: [D loss: 0.682395, acc: 0.560547]  [A loss: 1.005087, acc: 0.066406]\n",
            "954: [D loss: 0.675948, acc: 0.562500]  [A loss: 0.764269, acc: 0.367188]\n",
            "955: [D loss: 0.685984, acc: 0.521484]  [A loss: 0.955859, acc: 0.093750]\n",
            "956: [D loss: 0.662165, acc: 0.611328]  [A loss: 0.795685, acc: 0.289062]\n",
            "957: [D loss: 0.680122, acc: 0.587891]  [A loss: 0.929612, acc: 0.117188]\n",
            "958: [D loss: 0.676824, acc: 0.568359]  [A loss: 0.827200, acc: 0.257812]\n",
            "959: [D loss: 0.694191, acc: 0.539062]  [A loss: 0.907590, acc: 0.121094]\n",
            "960: [D loss: 0.675135, acc: 0.576172]  [A loss: 0.862541, acc: 0.156250]\n",
            "961: [D loss: 0.691781, acc: 0.523438]  [A loss: 0.883384, acc: 0.183594]\n",
            "962: [D loss: 0.676125, acc: 0.554688]  [A loss: 0.813127, acc: 0.265625]\n",
            "963: [D loss: 0.681953, acc: 0.564453]  [A loss: 0.918077, acc: 0.117188]\n",
            "964: [D loss: 0.666516, acc: 0.599609]  [A loss: 0.845881, acc: 0.214844]\n",
            "965: [D loss: 0.678897, acc: 0.562500]  [A loss: 0.909195, acc: 0.136719]\n",
            "966: [D loss: 0.664760, acc: 0.619141]  [A loss: 0.876699, acc: 0.207031]\n",
            "967: [D loss: 0.667662, acc: 0.591797]  [A loss: 0.906694, acc: 0.152344]\n",
            "968: [D loss: 0.666457, acc: 0.576172]  [A loss: 0.772845, acc: 0.343750]\n",
            "969: [D loss: 0.676600, acc: 0.566406]  [A loss: 0.968841, acc: 0.101562]\n",
            "970: [D loss: 0.675888, acc: 0.564453]  [A loss: 0.881660, acc: 0.171875]\n",
            "971: [D loss: 0.673903, acc: 0.560547]  [A loss: 0.937522, acc: 0.128906]\n",
            "972: [D loss: 0.677993, acc: 0.578125]  [A loss: 0.831152, acc: 0.226562]\n",
            "973: [D loss: 0.674233, acc: 0.568359]  [A loss: 0.919147, acc: 0.078125]\n",
            "974: [D loss: 0.662282, acc: 0.601562]  [A loss: 0.796525, acc: 0.316406]\n",
            "975: [D loss: 0.677014, acc: 0.556641]  [A loss: 1.041917, acc: 0.066406]\n",
            "976: [D loss: 0.663228, acc: 0.591797]  [A loss: 0.780374, acc: 0.335938]\n",
            "977: [D loss: 0.668604, acc: 0.578125]  [A loss: 0.963582, acc: 0.117188]\n",
            "978: [D loss: 0.673706, acc: 0.576172]  [A loss: 0.792657, acc: 0.355469]\n",
            "979: [D loss: 0.691651, acc: 0.527344]  [A loss: 1.065271, acc: 0.054688]\n",
            "980: [D loss: 0.679811, acc: 0.568359]  [A loss: 0.676809, acc: 0.566406]\n",
            "981: [D loss: 0.715389, acc: 0.515625]  [A loss: 1.035139, acc: 0.031250]\n",
            "982: [D loss: 0.651812, acc: 0.615234]  [A loss: 0.772656, acc: 0.347656]\n",
            "983: [D loss: 0.683380, acc: 0.552734]  [A loss: 1.000537, acc: 0.070312]\n",
            "984: [D loss: 0.675809, acc: 0.583984]  [A loss: 0.771252, acc: 0.335938]\n",
            "985: [D loss: 0.692842, acc: 0.544922]  [A loss: 0.962513, acc: 0.109375]\n",
            "986: [D loss: 0.688626, acc: 0.533203]  [A loss: 0.751919, acc: 0.382812]\n",
            "987: [D loss: 0.675881, acc: 0.583984]  [A loss: 0.928232, acc: 0.082031]\n",
            "988: [D loss: 0.648341, acc: 0.632812]  [A loss: 0.768578, acc: 0.375000]\n",
            "989: [D loss: 0.709488, acc: 0.523438]  [A loss: 0.966312, acc: 0.070312]\n",
            "990: [D loss: 0.674336, acc: 0.585938]  [A loss: 0.780909, acc: 0.351562]\n",
            "991: [D loss: 0.667840, acc: 0.583984]  [A loss: 0.917187, acc: 0.140625]\n",
            "992: [D loss: 0.669425, acc: 0.562500]  [A loss: 0.865139, acc: 0.195312]\n",
            "993: [D loss: 0.675334, acc: 0.583984]  [A loss: 0.901163, acc: 0.140625]\n",
            "994: [D loss: 0.676491, acc: 0.572266]  [A loss: 0.848434, acc: 0.175781]\n",
            "995: [D loss: 0.655687, acc: 0.613281]  [A loss: 0.873714, acc: 0.160156]\n",
            "996: [D loss: 0.671291, acc: 0.601562]  [A loss: 0.931605, acc: 0.132812]\n",
            "997: [D loss: 0.674907, acc: 0.566406]  [A loss: 0.873718, acc: 0.187500]\n",
            "998: [D loss: 0.680163, acc: 0.562500]  [A loss: 0.911958, acc: 0.136719]\n",
            "999: [D loss: 0.669397, acc: 0.566406]  [A loss: 0.849549, acc: 0.210938]\n",
            "Elapsed: 4.249002540111542 min \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAALICAYAAABiqwZ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3WmYXWWVN+5dGSoDCRkgkAABEQgS\nAgZURCJIWrQbxUZtWhxaBUTEAQe6HXltR2xBxQHHlkFsbCTYoKiINtpMjUQZEkSmMEkCmSCEzHO9\nH/q6/v/nZa3dnOJUnapTdd8ff9d+9nmAp04t9lVrr46urq4KAAD4H0P6egMAANCfKJABAKCgQAYA\ngIICGQAACgpkAAAoKJABAKCgQAYAgMKwVn7YkCFDwkuXvYeZZ6urq6uj1Z/Z0dHhwNJjnGHanTNM\nu6s7w54gAwBAQYEMAAAFBTIAABQUyAAAUGhpk56GPAAA+jtPkAEAoKBABgCAggIZAAAKCmQAACgo\nkAEAoNDSt1gAANA3hg8fnuajRo0K2apVq3p7O/2aJ8gAAFBQIAMAQEGBDAAABQUyAAAUNOkBAAwC\nr3/969P8uuuuC5kmPQAA4P+jQAYAgIICGQAACgpkAAAoaNJrkWHD8n/Vo0ePDtlg/8N4es/znve8\nkJ1xxhkhu/XWW0M2YsSI9J6veMUrQvbc5z43ZPvtt1+6fuPGjWkOQGOGDh0astWrV4es7nt8r732\n6vE9tTtPkAEAoKBABgCAggIZAAAKCmQAACho0usF2223XcjOO++89NrPfOYzIevrJr2Ojo6QZU1X\nDz/8cLp+69atPb0leshxxx0Xsle/+tUhyxo2ssa9qqqqffbZJ2S77757yNasWZOuHz58eJpDq7zu\nda8L2fvf//6QLVmyJF1/1VVXhWzOnDkh05BKb/n0pz8dslGjRoVs06ZN6fq63+eDmSfIAABQUCAD\nAEBBgQwAAAUFMgAAFBTIAABQ6Ojq6mrdh3V0tO7DWmTMmDEhu/POO0M2ZcqUdP3zn//8kN1zzz3N\nb6wJhx56aMi+9KUvhez4449P1z/22GM9vqdMV1dXfN1GL2v3M5yNNp81a1bIbrzxxpDVdT+PGzcu\nZEuXLg1Z3bj1CRMmhGzlypXptQONM9x7srfxLF++PL124sSJIduwYUPD68eOHRuy7O0sH/7wh0P2\n3e9+N71nu3CG+4fs925Wdzz++OPp+kmTJvX4ntpF3Rn2BBkAAAoKZAAAKCiQAQCgoEAGAICCJr1u\nGDlyZMj+/Oc/hywby1zn85//fMg++clPdm9jz9JOO+2U5vPnzw/Z9ttvH7LDDz88XX/bbbc1t7EG\naQ7pH7JmqGwkb915e8UrXhGya665pvmNtQFnuPf86U9/CtmMGTPSa7MG1L333jtkq1atStffcsst\nDa3PTJ06Nc0XLVrU0Pq+5gz3D9kZzhpF77777nT99OnTe3xP7UKTHgAANECBDAAABQUyAAAUFMgA\nAFDIR1sNcnVNE1/96ldDtuuuuzb1We9///tDdt5554XsL3/5S1Of09nZGbI5c+ak12YTdTZv3hyy\nrGmRwWfo0KENZXWOOeaYkA2WJj16Rjb9M2s62rZtW7p+3333DdnChQtDljWkVlVVXXfddSFrtEmv\nrmkqa4xuZVM97aVuUunTzZs3r5d3MnB4ggwAAAUFMgAAFBTIAABQUCADAEBBgQwAAIVB/xaLIUPi\n/yO8853vTK992cteFrK5c+eGLBtHmq2tqvxNEDfeeGPIPvvZz6br//M//zNkkydPDtnXvva1kB1y\nyCHpPTPr168P2dKlSxtez8A1ceLEkI0ZM6bh9dOmTQtZ9rYAHfxUVVWNHz8+ZNdee23Isu/2LVu2\npPdcvnx5Q59d96aAbNT0SSedFLLsXI8aNSq9Z/a7IfseZnDJznVV1b9h5en+67/+qye3M6B5ggwA\nAAUFMgAAFBTIAABQUCADAEBh0DfpjRs3LmTZiM+qqqqbb745ZKeddlrIFi9eHLIXvehF6T0vueSS\nkGWjns8999x0fda4NHz48JB1Z/Rvds/sn+nxxx9v+J4MXGPHjg1Z1sy0devWdH3WYKVJj1133TXN\nL7744pB1dnY2dM+6JrunnnoqZBs3bgzZunXr0vULFiwIWfb9uOOOO4Ysa+quqvqfF2jGL37xi77e\nQtvwBBkAAAoKZAAAKCiQAQCgoEAGAIDCoG/SGz16dMjmzJmTXptNYVq4cGHIsuaKP/zhD+k93/3u\nd4fsk5/8ZMgOPPDAdH3WdNKdhrxM1gyVNVJlTSwMPitXrgxZNvHr0UcfTdd/+9vfDtm2bdua3xht\nY8SIESGra1LLvjOXLFkSsqwBu+57eKeddgpZ9rshy6qqqlavXh2yrKn7la98ZchWrFiR3rNuYhqD\nW7O/3/3ebpyfQAAAKCiQAQCgoEAGAICCAhkAAAqDqklv1KhRIZswYULI7r777nR91ojR6LSjbDJY\nVVXV5s2bQ3bPPfeErG4C1JQpU0I2ceLEkDU6aaqq8qaRbOJf1rTI4LNmzZqQPfzwwyG799570/VZ\nQx+DS9Y4lDXe/W/502XNozvvvHN6bZZfeeWVIbv//vvT9Vmj6THHHBOy2bNnh+yxxx5L7+n7lUw2\nKbeq8ub6rO7IGmLJeYIMAAAFBTIAABQUyAAAUFAgAwBAQYEMAACFtn+LRTZ2cccdd0yvvf7660OW\ndU9/+ctfTtdfddVVIctG4s6aNStkX/nKV9J77rnnniHLuvrXrl2brs+uXbZsWciyUap143znz5/f\n0LUjR45M12d7zbppjRMeGLKu6mxM7lNPPZWuz36GG307DPSEpUuXhuzFL35xw+uzEdTf/e53Q5Z9\nZ954443pPb3Fgsz48ePTvO5NWU+3bt26ntzOgOYJMgAAFBTIAABQUCADAEBBgQwAAIW2b9LLRoRe\neuml6bX77LNPQ/e88MIL0zz7I/hG/zC+O8aOHRuy7bbbLr02a3TL9pQ1QtU1ye2///4h+9znPhey\n73znO+n6q6++OmRZM6QmvYFhr732Cll2hqdOnZquz8a9Z42mmWy8KrTajBkzQjZ9+vSG1mbNfFDn\n+OOPb/ja7Ptxw4YNPbmdAc0TZAAAKCiQAQCgoEAGAICCAhkAAApt36S3efPmkGUNQnVa1XjXHY02\n3lVVVQ0b9uz/E2bTzqoqn0R46KGHhqxuIk82Gcr0nvZXdwYPOeSQkI0bNy5k++23X7r+6KOPDtmV\nV14ZspUrV4ZMkx6tVPedmU1Kza5dvXp1yBYuXNj8xhiQsub697///Q2vz74fN23a1NSeBhNPkAEA\noKBABgCAggIZAAAKCmQAACgokAEAoND2b7FYs2ZNyOq67RsdbVx3XdZRmlm1alXIzj333PTan/zk\nJyE7+OCDQ3bOOeek67O3BdR1Wj9d3T9n1vmaXTt+/PiG13vbQHvJzvr222+fXpu94WT48OEhy96O\nUlVV9Y53vCNkN998c8iefPLJdD20ypQpU9L8hS98Yciy77zTTz+9oeugqvLzNnXq1IbXZ7+3++N5\nq6vZ+nqvniADAEBBgQwAAAUFMgAAFBTIAABQaPsmvWzUdJ1sxGI2Avn3v/99un7kyJEh27BhQ8g+\n8IEPhKxunGj2x+mTJ08O2ZYtW9L1WZNitqdsxGnWSFVV+UjfrPHvhhtuSNdnn9XXf2xPvexc77DD\nDiHbe++90/XPf/7zQ5adrbpGjOyzjJWmlbKzmTWl/vKXv0zXDxsWf5U+9dRTIbvggguexe4YDLLf\nsSeddFLIGn1ZQFX1z+/M7Gdt7Nix6bVZzZbVN73FE2QAACgokAEAoKBABgCAggIZAAAKbd+kt3Xr\n1pB961vfSq9961vfGrJLL700ZJdffnm6fvTo0SEbM2ZMyJYuXRqyuj+szyYwXXHFFQ19dlXl//wr\nVqwI2fr160M2YsSI9J7ZH8xnTYa33357uj5rnMz+ML8/NhAMdNl/h+xsZud6zz33TO+50047NfQ5\nGzduTNd/9rOfDdmSJUvSa6FR2bmua0zOpjx+5CMfCdlee+2Vrn/iiSdC9uIXvzhkjU5zZfDJmvQO\nO+ywpu5Z1xjdl7Kfy7pJwdm/k5NPPjlkvfVz5QkyAAAUFMgAAFBQIAMAQEGBDAAAhbZv0ssavX74\nwx+m1955550hW7BgQciefPLJdH02LemII44I2Tve8Y6QnXDCCek9s2ao7sj+4D3Lsj9ir5vOl/1h\n/6JFi0I2ceLEdH3W/Jf9sX1d0xat1WhT5eGHH56uzxqcsqbQ7Oeiqqrqsssue6Ytwv8q+87LJpLu\ntttu6fpXv/rVITvmmGNCVved9aEPfShkjzzySHotZLLv3Oc+97lN3bOVU+cy2T/TG9/4xpC95S1v\nSdc//vjjIWtlc78nyAAAUFAgAwBAQYEMAAAFBTIAABQUyAAAUGj7t1hk6jo358+fH7K6Nzlktt9+\n+5BdeOGFIdtll10avmezsrcFnHjiiSG79957Q5a9vaCq8s7TTZs2hWzUqFHp+qzTuz+OvByMsg7g\n7O0sZ5xxRshe+9rXpvfMxp2fcsopIZszZ04jW4Ruy87wrrvuGrJjjz02XZ+9sSJ7M8ZPfvKTdP2V\nV14ZslZ229P+svOyZs2apu45b968ptY3qu6NVm9/+9tDdtJJJ4Vs9erV6frs59VbLAAAoI8okAEA\noKBABgCAggIZAAAKA7JJr042bjlrHhs3bly6PhuHOH78+JBlf0Re94fl2R+nv/e97w3ZpZdemq7v\nTpMhZI4//viQ/cM//EPD66+99tqQXXLJJc1sCbolG29/3HHHhazuXGdNRgsXLgzZF77whXT92rVr\nn2mL8L/Kmp3/+Mc/hmzmzJnp+qyWmTp1asiy5tM6O+64Y8juueeekGV1UFXlDfs//vGPQ/bxj388\nXb948eJn2mKv8gQZAAAKCmQAACgokAEAoKBABgCAQkcrp5J0dHT06WihrJFjzz33DFnWtFRVVfWy\nl70sZMOHDw9ZNj3mwQcfbGSLdENXV1fLx/P19Rlu1pAh8f+JFy1aFLIpU6aELGtyraqq6uzsDFnW\ncELkDPeMrBkpa3CaNGlSuj5rcLrttttCduihh6brB3OztDPce2bMmBGyO+64I722P06rPe+880L2\nrne9K2R1v1tape4Me4IMAAAFBTIAABQUyAAAUFAgAwBAQYEMAACFQTVqOuu2z7pE169fn67/p3/6\np5DdfvvtIWvlm0GgO7KzOWfOnJC99a1vDdlznvOc9J7eWEFfGzduXMiyMbnZW1yqKu+iP+WUU0I2\nmN9WQevdeeedIat7k8pNN90Usu6MlW5U9rNy0kknpddedNFFPf75reQJMgAAFBTIAABQUCADAEBB\ngQwAAIVBNWr6gx/8YMhmzpwZso985CPp+mXLlvX4nnj2jDil3TnDPSNrwL722mtDNm3atHR91oD9\ngx/8oNltDQrO8MAwbFh8Z0PWgD0QX0Jg1DQAADRAgQwAAAUFMgAAFBTIAABQGFST9LIJeXPnzg3Z\n8uXLW7EdAHrApk2bQvbSl740ZHWT9EzIY7DzMxB5ggwAAAUFMgAAFBTIAABQUCADAEBhUE3SyybF\nZPyxenswwYl25wzT7pxh2p1JegAA0AAFMgAAFBTIAABQUCADAEBBgQwAAIVBNWra2ykAAHgmniAD\nAEBBgQwAAAUFMgAAFBTIAABQaOmoaQAA6O88QQYAgIICGQAACgpkAAAoKJABAKCgQAYAgIICGQAA\nCgpkAAAoKJABAKCgQAYAgIICGQAACgpkAAAoKJABAKCgQAYAgIICGQAACgpkAAAoKJABAKCgQAYA\ngIICGQAACgpkAAAoKJABAKCgQAYAgIICGQAACgpkAAAoKJABAKCgQAYAgIICGQAACgpkAAAoKJAB\nAKCgQAYAgMKwVn7YkCFDup6edXWFCBrS1dXV0erP7OjocGDpMc4w7a4vzrBagp5Ud4Y9QQYAgIIC\nGQAACgpkAAAotPRvkP2NEADQDLUEreAJMgAAFBTIAABQUCADAEBBgQwAAAUFMgAAFBTIAABQUCAD\nAEBBgQwAAAUFMgAAFBTIAABQaOmoaYBWGDYsfrVt27atoQwAPEEGAICCAhkAAAoKZAAAKCiQAQCg\noEmvzXV2doZszJgxIVu5cmXINCjRToYOHRqy22+/Pb12xowZIbviiitCdtxxx6Xru7q6urk7mjFk\nSHxW4/sJ6EueIAMAQEGBDAAABQUyAAAUFMgAAFDoaGUzSkdHx4DrfOno6AhZd/6dTpkyJWRz584N\n2W677dbw5zcr2/+FF14YstNOOy1dv27duh7fU6arq6vn/+GfwUA8w/3RiBEjQrZ06dKQjRs3Ll2/\nZcuWkI0fPz5ka9eufRa76zkD+QxnjXdVVVVXX311yGbPnh2yDRs2pOs//vGPh+zuu+8O2fvf//6Q\nvepVr0rvmTWAbt68OWR/93d/l67/xS9+keaDwUA+w63UbC3Bs1d3hj1BBgCAggIZAAAKCmQAACgo\nkAEAoKBABgCAgrdYdMOkSZNC9uijj4Zs2LA4wbs33jZRVXmXa2991tNlbxWoqqraZZddQtYbY2N1\nTw8M2Wj0Bx98MGTZz1+dyy67LGRveMMburexFhiMZ/jEE08M2XnnnReyurdgtLMVK1aEbI899kiv\nXbNmTW9vp0cMxjPcrOx39Mknnxyyq666KmRZzUFzvMUCAAAaoEAGAICCAhkAAAoKZAAAKGjS64as\n+W7Tpk0h606TXPbvf/ny5SFbsmRJuv7OO+8M2ahRo0J26KGHhmzChAnpPbN/zmzs6/nnn5+u/9CH\nPhSy3jhnmkP6r+xnIBsfXVVV9fOf/zxkRx11VEOfU3eussanhQsXNnTPVnKG/0c2yv5tb3tbem0z\nzXt15yVrIs4+p9kG6Ozzn/e856XX3nfffU19Vqs4w92XNSYvWLAgZNm48ze96U3pPR9//PGQPfXU\nUyHL6ouqqqqtW7em+WCgSQ8AABqgQAYAgIICGQAACgpkAAAoxG4samV/xJ5NRho/fnzINm7cmN7z\nkUceCdn1118fsr333jtdv/3224ds3rx5Ifu3f/u3kF177bXpPbOGvC1btoSs7o/6W9n4SevUNShl\nzUyjR48O2VlnnZWuf/nLX/6s93TDDTek+aJFi571PWm9d73rXSE76KCD0mt32GGHkH3zm98M2Ve+\n8pWQZd9jPSH72cj+mWbOnBmybGokA9tHP/rRkO28884hW7t2bciGDx+e3vP+++8PWbs03tU13vbG\nBN7u8AQZAAAKCmQAACgokAEAoKBABgCAggIZAAAKRk13w9ChQ0N26qmnhuzAAw9sKKuqqtpnn31C\nlnWeZm+rqKp8vOSll14asn/+538O2bp169J7tgsjTvuH7OciewPBTTfdlK6v68p+uieeeCJku+22\nW3pt9iaW/sgZrld3Ljo7O0OWfZf19dt0ss78/fbbL2QPPfRQur5dvp+d4XojR45M89WrV4ds2LD4\nUrE777wzZHW1RF+f90bttNNOIcu+26uqdW/hMGoaAAAaoEAGAICCAhkAAAoKZAAAKBg1nagbqZv5\n3ve+F7Idd9wxZH/84x/T9RMnTmzoc+r+WD1r5LjyyitD1i5NSwwMX/ziF0PWaDNeVeVNLLNmzQqZ\ncz1wbd68uVt5f5M1r2ZNeuPGjUvXz507N2TtMjqY/3HWWWeledaQl3nlK18ZsnZpxquqqvrUpz4V\nsqyB+3Wve10rttNtniADAEBBgQwAAAUFMgAAFBTIAABQGPRNejvssEPI6hp/1q9fH7Jt27aFbPz4\n8SHbeeed03tmDYHZH+HX7el3v/tdyO67776QZVOdsr1Dd40ZMyZkRx55ZMPrs/P+ta99LWQPPPBA\nt/YFrZI1XU2YMCFk2TTJbBpqVfl+bjfZ7/JTTjml4fUrVqwI2eLFi5vaU6tkLyaoqqp673vfG7Jz\nzz03ZP218dATZAAAKCiQAQCgoEAGAICCAhkAAAqDvklvu+22C9lhhx2WXvunP/0pZNkkvJ///Och\n684UseyP/UeNGpVem02gyf7Y/5prrgnZI488kt4zawzIJvZBVVXVSSedFLJsilidrAE1a9LbsmVL\n9zbGoJY1Jnd2dqbXbty4saF7ZpPwqqqqvv71r4fsne98Z8gefvjhhj6H9pM1K48cObLh9e973/t6\ncju9ZubMmSG75ZZb0muzyY9nnnlmj++pt3iCDAAABQUyAAAUFMgAAFBQIAMAQEGBDAAAhY5Wjvjr\n6Ojod/MEs3GgdW93yN54kcneQtFK2YjSZcuWhez6669P1//jP/5jyBYtWtT8xnpYV1dXy/9F98cz\n3CrZON2qqqqlS5eGLHu7S52f/OQnITv++ONDNhBH7zrDrfWc5zwnzT/zmc+E7K/+6q9CNmXKlHR9\n9v266667hqy/jtRthjP8P2bMmBGy7M1Xdfbff/+Q3XXXXU3tqTuyNw9lnz9t2rSG77lmzZqQjR07\ntnsba4G6M+wJMgAAFBTIAABQUCADAEBBgQwAAIVBP2r6lFNOCVk2MrI7spG4a9euTa/NmjayP2Lv\nzujerEkwa5qaPXt2ur7RZkQGl+nTp6f5+PHjG1q/evXqNH/Xu94VsoHYkEffqxv9e+SRR4Ysa8ir\na7L74he/2PC1DEzLly9vav1FF10Usux3dN337Y477hiyhx56KGR136333XdfyCZPnpxe26jzzz+/\nqfV9zRNkAAAoKJABAKCgQAYAgIICGQAACoOqSW/IkPj/A7vvvnvILrzwwnT9F77whZDdf//9zW/s\nabKGvAMOOCC99swzz2zo2uyP7bMpglVVVRs2bHimLTLAZWfwRz/6UXpt9nOVNSj9/d//fbp+xYoV\n3dwdPDtZA3VVVdXo0aNDlp3huu/Gq6++urmN0faeeuqpkNU1amaN9AcffHDInnzyyYY///e//33I\nXve614Usa0itqqqaNGlSQ5+TNVv/4Q9/SK/9/ve/39A9+ytPkAEAoKBABgCAggIZAAAKCmQAACgo\nkAEAoNDRynGYHR0dfTp7c+eddw7ZsmXLQtbuI0JnzpwZsltuuSVk2dsHqqqqZsyYEbK77rqr+Y31\nsK6urtgK3Mv6+gy3ynOf+9yQLViwIL02O0fZ2NXs56+q2v/nrRnOcO/Zd999Q/ab3/wmvTYbQb1y\n5cqQPfDAA+n6k08+OWSPPfbYM21xQHCG6z3/+c9P80suuSRk++yzT8iGDYsvGlu/fn16z6OPPjpk\n8+bNC1lWC1RVVe29994h27hxY8iyUdebNm1K79nZ2dnQtXXjr1ul7gx7ggwAAAUFMgAAFBTIAABQ\nUCADAEBhQI6aHj58eJovXbq0xTvpG1mDVDY6uM6qVat6cju0oWuvvTZkdU2dmcMPPzxkg7kZj951\n4IEHhuzWW28NWd2o6B/84Achy8bk1jUTZWOGYf78+Wk+ffr0kE2bNi1kJ5xwQsjOOeec9J5PPPFE\nyEaNGhWyPffcM12fecc73hGyup+hTHZtNma7v/IEGQAACgpkAAAoKJABAKCgQAYAgELbT9LLGof6\neipLX7vhhhtC9tKXvjRkdf+exowZE7K66T19yQSnnpGdjewM1ckmjk2cODFkmvQiZ7j7Jk+eHLI5\nc+aEbNKkSSF7zWtek94zm5CXnde6RtXs2sFy3p3h/iubinvHHXek127ZsiVk2223Xcg2b97c/Mb6\nGZP0AACgAQpkAAAoKJABAKCgQAYAgELbT9LLpuZt3LixD3bSNzo7O0M2a9ashtbee++9ad6dSTm0\nl5kzZ4Ysm5rXHa94xStCNlgalOg9P/7xj9P8b//2b0P27W9/O2Qf/ehHQ7Z169am9tSdBvBsYlg2\n0XTEiBENr9+0aVNDGVRVVb31rW8NWd138z333BOyrHFvMPEEGQAACgpkAAAoKJABAKCgQAYAgIIC\nGQAACm3/FousA7iuq7edO+uHDcv/U919990hy7qfs+7trBu8qtr739NglP333n///dNr586dG7Ks\nsz7z1FNPpfltt93W0HrojqOPPjrNs+/8iy++OGTNvrGiO7K3Kb373e8O2emnnx6y++67L73nxz72\nsZDNmzfvWeyOwSAbg56NVq97y9dll10Wsux3y2CqDzxBBgCAggIZAAAKCmQAACgokAEAoND2TXpr\n1qwJ2cSJE9Nr161bF7Jmxyo3+kfsdU12WaPcd77znZBNmjSp4T1ln3/JJZeE7P7772/4nvRf48eP\nD9kvf/nL9NpsNHkmO0Nnnnlmem13xu9Cox577LE033fffUOWNSP1RkPb7Nmz0zz7fs2+s7N/pje+\n8Y3pPVesWNHN3TGYZd/tO+20U8iymqWqqupFL3pRyF74wheG7MEHHwzZE088kd6z3Rv6PEEGAICC\nAhkAAAoKZAAAKCiQAQCg0NHKP6Lu6OhoyYeNHTs2zX/3u9+F7KCDDgpZo5PF+oNsKs5b3vKWkF1x\nxRUha/fmqq6urrzboBe16gx3x5gxY0K2aNGi9Npx48Y1dM9sGuWsWbPSa2+55ZaG7knkDNe75ppr\n0vzlL395yLLfY1mTW9bUXVVVNXLkyJB1p8EpkzWAT548OWR1EyrbhTPcP0yfPj1k8+fPb3h91nx3\n4403huyiiy4K2X//93+n92zlNMtm1J1hT5ABAKCgQAYAgIICGQAACgpkAAAoKJABAKAwIN9iUSd7\nu0XW7b/99tu3Yju11q9fH7Jf/OIX6bVvfvObQ7Zly5Ye31N/pHu63lvf+tY0v+CCC0KWjUHPztCR\nRx6Z3rOug5ln5gzXO/roo9M8eyPPiBEjens7/6uLL744ZCeeeGLIBuJ3szPcP/zsZz8LWTaCfe3a\nten61772tSG79dZbQ7Zq1aqQDdQ3YnmCDAAABQUyAAAUFMgAAFBQIAMAQGFQNek1qjvjRFv574//\nl+aQ7svO9rHHHhuy2bNnh+xLX/pSes+6sdY8M2e4Z2Tnevfddw/Zueeem67Pmuc+8IEPhGzhwoXP\nYncDmzPcWnX1yZ133hmyadOmhexf//Vf0/WnnXZayNq9+a5RmvQAAKABCmQAACgokAEAoKBABgCA\ngiY92pbmENqdM0y7c4Zbq7NkWLp9AAAgAElEQVSzM82/+MUvhmz+/Pkhu+iii3p8T+1Okx4AADRA\ngQwAAAUFMgAAFBTIAABQ0KRH29IcQrtzhml3znBr1U3SGzIkPu/cunVrb29nQNCkBwAADVAgAwBA\nQYEMAAAFBTIAABQUyAAAUBjW1xsAAOCZ1b15zBsrep4nyAAAUFAgAwBAQYEMAAAFBTIAABRaOmoa\nAAD6O0+QAQCgoEAGAICCAhkAAAoKZAAAKCiQAQCgoEAGAICCAhkAAAoKZAAAKCiQAQCgoEAGAICC\nAhkAAAoKZAAAKCiQAQCgoEAGAICCAhkAAAoKZAAAKCiQAQCgoEAGAICCAhkAAAoKZAAAKCiQAQCg\noEAGAICCAhkAAAoKZAAAKCiQAQCgoEAGAICCAhkAAAoKZAAAKCiQAQCgMKyVHzZkyJCup2ddXSGC\nhnR1dXW0+jM7OjocWHqMM0y7c4Zpd3Vn2BNkAAAoKJABAKCgQAYAgIICGQAACi1t0tOQBwBAf+cJ\nMgAAFBTIAABQUCADAEBBgQwAAAUFMgAAFBTIAABQUCADAEBBgQwAAAUFMgAAFBTIAABQaOmoaZ69\njo6ONN9uu+1Clo30Xrt2bY/vCQBgIPIEGQAACgpkAAAoKJABAKCgQAYAgIImvSYNGRL/H+Poo48O\n2fXXX5+uX716dY/vaeLEiSHL9rlx48Z0/ebNm0OWNf4BtJPx48en+VFHHRWyefPmheyBBx5I1/t+\nhIHHE2QAACgokAEAoKBABgCAggIZAAAKHa1sLujo6BhwnQyzZs0K2W9/+9uQXXTRRen6U089NWR9\n3fAxevTokA0bFvs5V61a1Yrt1Orq6srHC/aigXiG6TvOcGvNnTs3zQ855JCG1t98881p/tKXvjRk\nW7dubXxjbcwZbi9Zw35VVdXYsWNDtmbNmpANxHNdd4Y9QQYAgIICGQAACgpkAAAoKJABAKCgQAYA\ngIJR00267LLLQjZixIiQTZgwIV3f12+syKxbty5kkydPDlndmOz++M9E93R2dobs/PPPT6897rjj\nQpZ1Si9evDhd/8IXvjBkjz/++DNtEbrtxz/+cZo3+haLXXbZJc1HjhwZsrVr1za+MWjSlClTQnba\naaeFLDurVZWf13vvvTdkQ4cODdkvfvGL9J5PPPFEmrcLT5ABAKCgQAYAgIICGQAACgpkAAAoGDXd\npGzsYtagtMMOO6TrV6xY0eN7apW6kZXbtm1ryecP5BGnY8aMSfNXvepVIVu0aFHI6kbqZuc1GyP+\nm9/8JmSzZ89O79kbbrrpppBlY93b3UA+w/3RjBkz0nz+/Pkhy343zps3L12f/WxkY3o7OuJ/7rrf\nwe3S7OwM956sIe4973lPeu2nPvWpkGW/G84888x0/R/+8IeQHXjggSG74IILQjZx4sT0no1asmRJ\nmu+9994hW79+fVOflTFqGgAAGqBABgCAggIZAAAKCmQAACiYpNcNWTNT1qiWNam1czNenVY14w0k\nWZPORRddFLI3velN6fqsaSNrrjjiiCPS9VmTXubcc88N2f77759emzUUZo0U2223Xbo++7kaO3Zs\nyGbOnBmyP//5z+k9Gz2bdY1QzvbAlDXOVVVVPfnkkyEbPnx4yG699dZ0faPnJTvX22+/fXrto48+\n+qw/h/Zz1FFHhezSSy8NWd33aHY2rrjiipD9/ve/T9ePHj06ZIcddljINm3aFLLNmzen98wmsma/\nA+smVGYTVadOnRqy3qqvPEEGAICCAhkAAAoKZAAAKCiQAQCgoEAGAICCt1h0w3Oe85yGrsvGO0JV\nVdUee+wRsuOPPz5k2Zsd6ixYsCBkdV3FmS1btoQs637Osp6QdU+ffvrpIfv5z38esnHjxqX3zEYH\nf//73w/Z7373u3R99gaBdhn9S/fdcccdIcveOPHwww+n60eNGhWy7Gdwt912a+hzqqqqli1bFrKN\nGzem19JeXvCCF4TsV7/6VciytxatWrUqvWf2dopsNPqRRx6Zrn/pS18asptuuilk2VskuvN2lREj\nRoTs1FNPTa992cteFrLs91Vv8QQZAAAKCmQAACgokAEAoKBABgCAgia9bpg1a1ZD111++eW9vBPa\nVdZkk41lrmvSW716dcjOPPPM5jfWh7Kmi2ys9JQpUxq+Z9Y0ctlll4Us+3fPwJA1OK1bty69Nmvq\n3HvvvUO24447pusPOOCAkN11110hy8YE142/1hQ6cP3zP/9zyLIRzNkI9OOOOy69Z3ZesnP5kpe8\nJF1/8803h+yHP/xhem0zst+BX//619Nrv/GNb/T453eHJ8gAAFBQIAMAQEGBDAAABQUyAAAUNOl1\nw+tf//qGrnvhC1/YyzvpGaeddlqaX3jhhSGrayShe5YvXx6yt73tbSE77LDD0vVXXnllyLJJelnD\nR3f0RoPQ8573vDTPGjGyRpJsMtnZZ5+d3vPTn/50yDQ9DS5DhsTnP3VnIGsAzc7r9OnT0/VZU+gD\nDzwQsuxnddOmTek963LaX9asuXjx4pC9+93vDlk2Ma+q8kl4WaPpI488kq7/1re+leZ9qa+/sz1B\nBgCAggIZAAAKCmQAACgokAEAoKBJrxvuv//+kGVTYbKGjdGjR6f33LBhQ8iyKWqnnHJKuv6ss84K\n2ahRo0KWNW3V/QH8b37zm5Dde++96bV0TzY17qqrrgrZ1Vdfna7Pzlb23zE7A1VVVWPHjg3Z8OHD\nQ7Zs2bKQZXuvqqraY489QnbdddeFbNy4cen6bLrZTTfdFLJsQuX3vve99J4MLtl3ZtZQd+SRR6br\njzjiiJBlU/O2bt2art9rr71Cln23Z83OdT9XDFxZQ96LX/zikH3kIx8J2Z577pneM2vsfuyxx0L2\n5S9/OV1fd7YHM0+QAQCgoEAGAICCAhkAAAoKZAAAKCiQAQCg0NHKUX4dHR1tMeu1s7MzzbOu6Gxk\n5F133dXwZ2Wdq+eee27IspGR3ZH9d/7zn/+cXnvwwQeHLBvz29e6urqam6f8LLTLGc7G7Nbl2dnK\nRu++5z3vSe957LHHhix7a8rPfvazdH028jzr9l+/fn3I2v0NAM5wz8jeQnHeeeeFbPvtt0/XT5w4\nMWTZmzHqxj9feOGFIfvoRz8asuxcZ2+maSfOcL267+G77747ZNOmTWvqs7Lf8ffcc0/IDjjggHT9\nYH6LRd0Z9gQZAAAKCmQAACgokAEAoKBABgCAwqAfNZ2NgM4ajKqqqubPnx+yRpsc6+75+OOPh2zC\nhAkhyxqUqqqqLr744pB94AMfCFk2ErtOXWMB7aOu8SfLH3jggZAddNBBITvmmGPSe2bNTDfffHPI\n3vjGNza8p5EjRzZ0HYNPdt6+9KUvhSxrNK1rsssalLLPqfPoo4+GrNGx8Axcr3rVq9J8l112CVl2\nXrK6oa6WyPJdd901ZCNGjEjXr1u3Ls0HM5UQAAAUFMgAAFBQIAMAQEGBDAAAhUHVpJdNvTvhhBNC\n9sMf/jBd30yDxeTJk9P817/+dcjGjh0bsqzpqaqq6pRTTnnWe6qjSW/gyho53ve+94Usa3oaOnRo\nw59zxx13hKzuXE2aNClk++yzT8j+8Ic/hGzDhg0N74mBITtHt912W8iyZud77703vef+++8fst13\n372hz677rGzKoya9gWvUqFEh+8Y3vpFeu3DhwpC98pWvDNljjz0WsqyOqaqq+ulPfxqyWbNmheyc\nc85J15966qlpPpiphAAAoKBABgCAggIZAAAKCmQAACgokAEAoND2b7HIuvLruu2//vWvh+wlL3lJ\nyLKR0lVVVbfeemvIss7VrJv08MMPT++ZyTqdr7322obXNyvrvmZg6OzsDNmHP/zhkGXd+nUd+IsW\nLQrZZz7zmZDtvPPO6fprrrkmZNmY3wMPPDBdz+CSjYs+66yzQnbfffeF7IknnkjvuXbt2pBNnTo1\nZHXjp1/wgheEbPPmzem1DEzZm0wuvPDC9NrsvNaNQX+61atXp3n2FowHH3wwZCeddFK6/oILLghZ\n9uag3lBXs2VjsbM3F2VjunuCJ8gAAFBQIAMAQEGBDAAABQUyAAAU2r5JLxu7uMcee6TXZg11jz76\naMg+8YlPpOunT58esmwcadY42B3ZONSvfvWrTd0TqqqqNm7cGLLs56U3xuTuuOOOaT5t2rQe/ywG\nl/Xr14csa9Kra7J75JFHQpY1/tStHzduXEPrGbiyJuKzzz47vbbRhrzuyL4zs5pn+PDh6frf//73\nIVuyZEnIvvnNb4Zszpw56T2zJruxY8eGrK5Jb++99w7Zb37zm4b22RM8QQYAgIICGQAACgpkAAAo\nKJABAKDQ9k16WTPRX/7yl/TaM844I2QHHXRQyN773vem6ydPnhyyRhvy6v4o/9WvfnXIfve734VM\nwwe9pVUTv9atW9fwtdnPVTZBCaqqqkaOHBmyrBmobpLe0qVLQ7Z169aQ1TWPLl68uOFraX8zZ84M\n2cUXXxyyXXbZpRXbqaqqqo4//viQ7bDDDg2vz6anZvv/whe+ELLPf/7z6T2zn7dLL700ZP/93/+d\nrs9+hrKJrJr0AACgBRTIAABQUCADAEBBgQwAAIW2b9LLGnfqGueyRrm99torZAsXLkzXz507N2TZ\nH5f/y7/8S8hWr16d3hMGi7ppSZmsOSNrmoKqqqoJEyaELGtQqvseHj9+fMiypqW6xrv/+q//eqYt\nMoBkDf/ZeatrTK6byNiIrEmtqqrqggsuCFmzU30blf2sVFVVjRkzJmTXXHNNyLLpeFWVN5C3svnV\nE2QAACgokAEAoKBABgCAggIZAAAKCmQAACi0/VssMnVdjtlbLK677rqQ3Xjjjen6jRs3NrexfqY7\nHa7GptKs7ryFYtWqVb24E9pZNlb6Xe96V8jWrl0bsrvuuiu956RJk0KWdebXnWFvsRhc/v7v/z5k\n27ZtC1ndm3uyt28dffTRIcvO+hVXXJHes7Ozs6E91Z3hrB4YPnx4eu3TZW+bqKqqes973hOyK6+8\nMmT9tb7wBBkAAAoKZAAAKCiQAQCgoEAGAIDCgGzS644tW7Y0lA1EdX8YP3r06JDVjcyERo0aNSrN\ns3N433339fZ2aFNZQ9Add9wRspUrV4Zs/fr16T0POeSQkGVNS3Wjqu+55540Z/DIGvKyFwNUVVWN\nGDEiZL/97W+b+vzsezSrZZ566ql0fdbUmjWq3nvvvSH7h3/4h/Sey5cvT/N24QkyAAAUFMgAAFBQ\nIAMAQEGBDAAAhUHfpDeY1U3Smzx5csgWL14csrqGF9pL1oiRTWBq1tlnn53mWXNJdt6gqvJJYD/6\n0Y9Cln2PfeADH0jvOX369JBlPxd1U8x8F5J9j9U1Js+dOzdkM2bMCFnW+Fc3CS87g1nj3dVXX52u\n/+pXvxqyBQsWhGygTRT+33iCDAAABQUyAAAUFMgAAFBQIAMAQEGBDAAAhY66ccO98mEdHa37MJ61\nrPM2G+/a1yO5u7q68tdw9KKBeIY/9KEPheyoo44K2Yc//OF0/apVqxpaf95556Xrs67svfbaK2SL\nFi1K17czZ7j3jB49OmTnn39+eu1xxx0XsgceeCBk2ZsGqqrvvwv7kjPcM8aMGROymTNnhiw711VV\nVcuWLQtZNup5yZIl6fq6t2MMBnVn2BNkAAAoKJABAKCgQAYAgIICGQAACpr0aFuaQ3pGNlL3K1/5\nSshOOeWUdP2IESNClo1IrZONSM0aVnpj/HVfc4Zbq6Mj/9edNT5lY3qJnGHanSY9AABogAIZAAAK\nCmQAACgokAEAoKBJj7alOaS16iY4PfrooyEbP358w/f98pe/HLK6qX0DjTNMu3OGaXea9AAAoAEK\nZAAAKCiQAQCgoEAGAICCJj3aluaQ/iGbxHfMMceEbNGiRen6efPmhWwgTs3LOMO0O2eYdqdJDwAA\nGqBABgCAggIZAAAKCmQAACgokAEAoOAtFrQt3dO0O2eYducM0+68xQIAABqgQAYAgIICGQAACgpk\nAAAotLRJDwAA+jtPkAEAoKBABgCAggIZAAAKCmQAACgokAEAoKBABgCAggIZAAAKCmQAACgokAEA\noKBABgCAggIZAAAKCmQAACgokAEAoKBABgCAggIZAAAKCmQAACgokAEAoKBABgCAggIZAAAKCmQA\nACgokAEAoKBABgCAggIZAAAKCmQAACgokAEAoKBABgCAggIZAAAKCmQAACgMa+WHDRkypOvpWVdX\niKAhXV1dHa3+zI6ODgeWHtMXZ9j3MD3J9zDtru4Me4IMAAAFBTIAABQUyAAAUFAgAwBAoaVNehpB\nAPqW72GAZ+YJMgAAFBTIAABQUCADAEBBgQwAAAUFMgAAFBTIAABQUCADAEBBgQwAAAUFMgAAFBTI\nAABQUCADAEBBgQwAAAUFMgAAFBTIAABQUCADAEBBgQwAAAUFMgAAFBTIAABQUCADAEBBgQwAAIVh\nfb0BmjNhwoSQ/fnPfw7ZnnvuGbKNGzf2yp6gNwwZEv9//k1velN67Ze+9KWQPfDAAyE78cQT0/X3\n339/N3cHwEDiCTIAABQUyAAAUFAgAwBAQYEMAAAFBTIAABQ6urq6WvdhHR2t+7ABpq6rfq+99mpo\n/eTJk0O2dOnSpvbU17q6ujpa/ZmD5Qx3dMR/tb31XTF06NCQTZo0KWTZGyc+97nPNXzPTN2bXLK3\nY1x77bUhW716dci2bNnS0GdXlTNM+3OGaXd1Z9gTZAAAKCiQAQCgoEAGAICCAhkAAAqa9PpYZ2dn\nyDZs2BCyrGmqzkMPPRSyrJmvlf/te4PmkJ6Rna3dd989ZMuWLUvXr1+/vsf3lDXZPe95zwvZLbfc\nkq4fMWJEyLL9z5o1K13/4IMPPtMWq6pq/mfIGabdOcPt5TOf+Uyaf+xjHwtZ9t3+wQ9+MGQ/+MEP\nmt5XX9KkBwAADVAgAwBAQYEMAAAFBTIAABSG9fUGBosTTjghzS+88MKG1tdN/Nppp51CtmrVqob3\nBVlD26mnnhqyukaMe++9t6e3VG3dujVk2c/AjTfemK7/5Cc/GbKbb765+Y3R57IGzq997Wshe8tb\n3hKy7KxXVd6omk1EPPbYY9P12ZTFdm+CHmwanb5ZVVU1ZEh8tph9Z9WdgWbOxt57753md911V8iG\nDx/+rD+nbv1//ud/NnXPduIJMgAAFBTIAABQUCADAEBBgQwAAAUFMgAAFLzFohf89Kc/DVld93Nm\nzJgxIVu7dm1Te4I6o0ePDtmMGTNCtnTp0lZsp6qqvEt8xx13DNnnP//5dL03VrS/qVOnpvmf/vSn\nkGXfmevWrQvZD3/4w/SekyZNCtnRRx8dsquuuipdn70FIxtX/qIXvShkvtv7h1GjRoXs5JNPTq/9\n3Oc+F7LsezT7Hutr2dtZqqqqHn/88ZC95jWvCdmjjz7a43vqr/rffz0AAOhDCmQAACgokAEAoKBA\nBgCAgia9JmVjdqdNmxayutGSnZ2dIav7I/pmjB8/PmR1I6m3bdvW459P/zVx4sSQPfnkkyFbuXJl\nK7ZTVVVV/fVf/3XI3vOe94TszW9+cyu2Qy979atfHbKf//znDa/PRk2ffvrpTe0pUzeO+PLLLw9Z\n1uCUNUJNmDAhveeGDRu6ubtnljUTZs2vK1asSNdn45QHiqxZ8oILLkivXbhwYci+/OUvh2yPPfZI\n12f/HRp1ww03pPkRRxzxrO/ZW7Kfl6wW2W+//dL1hx56aMjmz58fst4af+0JMgAAFBTIAABQUCAD\nAEBBgQwAAIWOuuaxXvmwjo7WfVgvuOWWW0L2ghe8oKG1dRN1euPff9Z0lU0EOuecc9L1vdEk2Bu6\nurqefafDs9TuZziz0047hSxrHl20aFGvfH7W1Dp37tyQXXPNNSF7wxvekN6zld9rzRiMZ3iHHXYI\n2fLly0NW18i0Zs2akI0dO7b5jfWw//iP/wjZ6173upCtX78+Xb/vvvuGrDtTzLKfjR/84Achy37W\nZs+end4z+7kajGe4UcOHD0/z7bffPmTZud64cWOP76lZWZPdI488kl7bGz+XmzdvDlnW6NqdCZV1\nZ9gTZAAAKCiQAQCgoEAGAICCAhkAAAom6SUOOeSQNG+0Ie+AAw4IWW81DWXNf6eddlrITjzxxJBl\nzXxVVVVnnHFGyAbyBKXBbtmyZSFrZtJTVeVNfrfeemt6bd0Upae77777QtYuzXj8/26//faQdee8\nHXXUUT25nV7zd3/3dyHLmhGzSXZVlTc+ZdNP65qRpkyZ8kxbrKoqbxz0c9UzsoayqqqqJ554osU7\neXay+iKbstjs74vFixen+f333x+yT33qUyHrTkNed3iCDAAABQUyAAAUFMgAAFBQIAMAQEGBDAAA\nBaOmE3XjHbPO/HXr1oVsu+226/E9dcfOO+8csmyc6NSpU9P12TjSd77znSHbtm1b9zfXg4w47RlZ\np/KBBx4Ysne84x3p+lNPPTVkw4b1/AtylixZErK6M2xcer1WneG99torzRcsWBCy7nTBDx06NGR9\n/V3UqOx3w+OPP55eO2LEiJBl53rDhg3p+jFjxoQsGxe/++67p+sbNZDP8GB3+eWXhyx760md7Lw9\n5znPCVlfvyXLqGkAAGiAAhkAAAoKZAAAKCiQAQCgYNR0Yvjw4Q1fe+ONN/biTp6dJ598MmRZg2Fd\nY8wRRxwRMqNHB4bsHGSjpseNG9eK7VRVlZ+tbETrpEmTQnbllVem93zVq17V/MZoyne+8500b7Qh\nr65Zul0a8jJZM9Kvf/3r9NrsZ/Wyyy4L2SGHHJKuf/vb3x6yunHvkP1c/s3f/E3Isu/mV7ziFek9\nr7vuuuY31oc8QQYAgIICGQAACgpkAAAoKJABAKCgSS9RN9Ulmw7WHxtGDjrooJCNGjUqZHX/nHPm\nzAmZJr2B4bvf/W7Imm3Iy87GHXfcEbLXvva16fpskt83v/nNkGUNI6985SvTe2bnff369em1NC+b\nbveSl7ykD3bSv2VTJ0ePHp1e+3/+z/8J2UMPPRSyusmt2e+rl7/85SHLmrN83w9c2fdtVVXVt7/9\n7ZBlNcL06dND9vDDDze9r/7IE2QAACgokAEAoKBABgCAggIZAAAKCmQAACh4i0Vi/vz5af6CF7wg\nZLNnzw7ZHnvsEbJHHnkkvWez3cJf+MIXQvaxj32soc9ZsWJFes9zzjmnqT3Rf2Wd8d/73vdCdttt\nt4UsGzHaE7Iu+n//938P2eGHHx6yBQsWpPfcsGFD8xujYdl3XvYmke6oG0mdjRxfvnx5U5/VrGyv\nn/jEJ0J2xhlnhKzu7SrZ24iyc/2yl72s4T2NHDkyZNnbLnrrZ53uyd44kf33qqr8v+2sWbNCln3f\nV1VVTZgwIWS/+tWvQlZXywxEniADAEBBgQwAAAUFMgAAFBTIAABQ0KSXOOqoo9I8a2obMWJEyLIm\nv7p73n777SHLGuo+97nPpes//vGPp/nTZU0XM2fOTK994oknGron7eexxx5rKGulbNTumDFjQnbA\nAQeEbKCOOG03++23X8jqGpCz76LFixeHbO7cuen67LxkTYLDhw8P2ZIlS9J7rlmzJs2frq5x8Mgj\njwzZZz/72Ybu+dRTT6X5fffdF7KFCxeG7Mwzz0zXZ2Op161bFzINef3Xtm3bQrZly5b02hNOOCFk\nZ599dsg6OzvT9X/6059CdsoppzS0p4HKE2QAACgokAEAoKBABgCAggIZAAAKmvQSK1euTPOsQWKX\nXXYJ2dChQ0OWTbSpqnwy0ne+852G1zdq//33D9mjjz7a1D2hO7bbbrs0zyaOff/73w+Zhrz+6/rr\nrw9ZXZPa+eefH7KsSa87Jk6cGLLDDjssZJs2bUrXZ81r2RSzfffdN13/ox/96Jm2WFVVVa1duzZk\nb3rTm9Jrb7jhhpBt3bo1ZE8++WS6ft68eQ3tifZS1yT39re/PWTZSwTqfgZe85rXhKyuFhosPEEG\nAICCAhkAAAoKZAAAKCiQAQCg0FE37ahXPqyjo3Uf1gvGjRsXste//vUhW7BgQciWLVuW3vP0008P\n2cknnxyyrPGvqvIpSNOnTw/Z/fffn65vZ11dXflYq17U7me4L1100UVp/stf/jJkc+bM6e3t9AvO\ncO/JmuyyrC7fbbfdQpZNJquqqjrkkENCNn78+JBdcsklIXv3u9+d3rNdJpY5w61VN80xqwWyuuGb\n3/xmuv60005rbmNtrO4Me4IMAAAFBTIAABQUyAAAUFAgAwBAQYEMAAAFb7FoUl1H6dMdfPDBaX7T\nTTeFrLOzM2Rr1qxJ1++www4hqxslOdDonu6/Zs+eHbJvfOMb6bVHHHFEyOrG5w40zvDAkL0tYI89\n9mgou+6669J7eotFvcF8hrMzVFVV9fDDDze0vu6NWO1y3nqDt1gAAEADFMgAAFBQIAMAQEGBDAAA\nhWF9vYF212iT4+c///k0zxrysj+WnzRpUrp+sDTk0X99/OMfbyhbsWJFun716tU9vidopa1bt4bs\nL3/5S8iOPfbYkN15553pPZcvX978xhhwLr/88oavfeihh0I2mJvxussTZAAAKCiQAQCgoEAGAICC\nAhkAAAqa9HrBqFGjQvbXf/3XDa///ve/H7INGzY0tScGl+HDh6f5yJEjQzZsWPwayK676qqr0nse\neOCBIcuaV3/5y1+m61s5zRNaJTvXz33uc0M2a9asdP1Pf/rTHt8T7WX8+PEhy75v62RTSmmcJ8gA\nAFBQIAMAQEGBDAAABQUyAAAUFMgAAFDwFote8Ktf/SpkHR0dDa9/3/ve15PbYYAbMiT+f+7111+f\nXnvwwQeHrO6NF0/XnTO8cuXKkP3rv/5req3RpwxE2bn+7ne/G7Inn3yyFduhDb3xjW8M2ebNm9Nr\nV61aFbJFixb1+J4GE0+QAQCgoEAGAICCAhkAAAoKZAAAKGjSa1I2Vro7oyDnz58fsi1btjS1JwaX\nrBnoyiuvTK/df//9Q7XFb/sAAAHsSURBVJaNms7G5G7atCm95/nnnx+yT3ziEyFbvXp1uh4Gi4ce\neihk48aNS6/NmmKNZR+4smbr/fbbL2S33HJLuv4jH/lIj+9psPMEGQAACgpkAAAoKJABAKCgQAYA\ngEJHK//ov6OjY8B1GGRNeg888EDIJk6cmK6fNm1ayB555JHmNzYIdHV1NT7arYcMxDNM33GGB5es\n8W7q1KnptcuXLw/Z+vXre3xPzXKGe0Y25fTHP/5xyP7yl7+k69/+9reH7LHHHmt+Y4NA3Rn2BBkA\nAAoKZAAAKCiQAQCgoEAGAICCJr0mZU0XU6ZMCdlb3vKWdP23vvWtkK1bt675jQ0CmkNod84w2e+Q\nOv1xkp4z3H3Z9NKskX+XXXYJ2cqVK9N7Zg15ddNP+X9p0gMAgAYokAEAoKBABgCAggIZAAAKCmQA\nACh4iwVtS/c07c4Zpt05w7Q7b7EAAIAGKJABAKCgQAYAgIICGQAACi1t0gMAgP7OE2QAACgokAEA\noKBABgCAggIZAAAKCmQAACgokAEAoKBABgCAggIZAAAKCmQAACgokAEAoKBABgCAggIZAAAKCmQA\nACgokAEAoKBABgCAggIZAAAKCmQAACgokAEAoKBABgCAggIZAAAKCmQAACgokAEAoKBABgCAwv8F\nc/IZX4x+MvsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}